{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce678eb1",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18263a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda 11.1\n",
    "#!pip install -r requirements-torch-cu111.txt --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dc478",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '1cycle'\n",
    "url = 'https://drive.google.com/uc?id=1j46elyFZtkmnmCehlntMi0eX0Tp5nnav'\n",
    "#prefix = 'helper'\n",
    "#url = 'https://drive.google.com/uc?id=1iSP_YKFs56d5cRRTEMzfedwRxrx-nXWO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061a428",
   "metadata": {},
   "source": [
    "## 스토리헬퍼 샘플 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "scripts_file = f'data/scripts_{prefix}.json'\n",
    "gdown.download(url, scripts_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(scripts_file) as f:\n",
    "    data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터 출력\n",
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618293c6",
   "metadata": {},
   "source": [
    "**후처리**\n",
    "1. `\\n`을 제거한다. \"부엌에서 일하게 된 마리오\\n인부들 사이에서 인기만점인 베아트리체\"  \n",
    "   ==> 필요없는 것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정상적 white character가 없는지 확인\n",
    "for idx, data in enumerate(data_dict):\n",
    "    #data['storyline'] = data['storyline'].replace('\\n', ' ')\n",
    "    for i, context in enumerate(data['script']):\n",
    "        #if '\\n' in context:\n",
    "        if '부엌에서 일하게' in context:\n",
    "            print(idx, i, context)\n",
    "            print('\"%s%s\"'%(context[9],context[10]))\n",
    "            print(context[10] == ' ')\n",
    "        #data['script'][i] = context.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58e2ce",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b08482",
   "metadata": {},
   "source": [
    "### kobigbird pretrained model을 이용한 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('monologg/kobigbird-bert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab61f27",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EOS_ID = tokenizer.sep_token_id\n",
    "\n",
    "positive_sessions = []\n",
    "positive_str = []\n",
    "for unit_data in data_dict:\n",
    "    unit_contexts = [tokenizer.tokenize(text) for text in unit_data['script'] + ['[SEP]'] ]\n",
    "    while [] in unit_contexts:\n",
    "        print('empty string in the script. removing...')\n",
    "        index = unit_contexts.index([])\n",
    "        #print(\"'{%s}'\"%unit_data['script'][index])\n",
    "        del unit_contexts[index]\n",
    "        #unit_contexts.remove([])\n",
    "        del unit_data['script'][index]\n",
    "    if len(unit_contexts) <= 1:\n",
    "        print('empty scripts. skipping...')\n",
    "        continue\n",
    "    unit_narrative = tokenizer.tokenize(unit_data['storyline'])\n",
    "    positive_sessions.append([unit_contexts, unit_narrative, 1])\n",
    "    positive_str.append(unit_data)\n",
    "print(\"all suitable sessions: \", len(positive_sessions))\n",
    "\n",
    "# reproducibility를 위한 random seed 설정\n",
    "np.random.seed(42)\n",
    "# random shuffle data\n",
    "np.random.shuffle(positive_sessions)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(positive_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = int(len(positive_sessions) * 0.9)\n",
    "dev_test_num = int(len(positive_sessions) * 0.05)\n",
    "train_sessions, dev_sessions, test_sessions = positive_sessions[:train_num], positive_sessions[train_num: train_num + dev_test_num], positive_sessions[train_num + dev_test_num:]\n",
    "print('number of train =', len(train_sessions), ', val =', len(dev_sessions), ', test =', len(test_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132fe371",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "for train_session in train_sessions:\n",
    "    train_texts += train_session[0]\n",
    "    train_texts.append(train_session[1])\n",
    "print('number of word2vec training sentences =', len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7540667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# word2vec 학습\n",
    "model = Word2Vec(sentences = train_texts, vector_size = 200, window = 7, min_count = 5, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e2e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total num of words =', len(model.wv.key_to_index))\n",
    "print('first word = \"%s\"'%model.wv.index_to_key[0])\n",
    "print('last word = \"%s\"'%model.wv.index_to_key[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec이 잘 학습되었는지 여러가지 테스트를 수행하자.\n",
    "print(model.wv.most_similar(\"가족\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ad5d7",
   "metadata": {},
   "source": [
    "## 데이터 저장\n",
    "\n",
    "`embeddings.pkl`과 `vocab.txt`를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8453ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/vocab_{prefix}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, key in enumerate(model.wv.index_to_key):\n",
    "        file.write('%s\\t%i\\n'%(key, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "new_embeddings = np.array([[0.]*200],dtype='float32') \n",
    "for i in range(len(model.wv.index_to_key)):\n",
    "    new_embeddings = np.append(new_embeddings, [model.wv.get_vector(i)], axis=0)\n",
    "\n",
    "with open(f'data/embeddings_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(new_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"data/word2vec_{prefix}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4ec99",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fc318",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = model.wv.key_to_index['[SEP]']+1\n",
    "UNK_ID = model.wv.key_to_index['[UNK]']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "with open(f\"data/vocab_{prefix}.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "    for idx, line in enumerate(fr):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        vocab[line[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0501f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample id 출력 확인\n",
    "vocab['가족']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611b33b",
   "metadata": {},
   "source": [
    "**positive data 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = []\n",
    "positive_str2 = []\n",
    "\n",
    "for unit, unit_str in zip(positive_sessions, positive_str):\n",
    "    narrative = unit[1]\n",
    "    #print(narrative)\n",
    "    context = unit[0]\n",
    "    narrative_id = [vocab.get(word, UNK_ID) for word in narrative]\n",
    "    context_id = [[vocab.get(word, UNK_ID) for word in sent] for sent in context]\n",
    "    if len(narrative_id) == 0 or len(context_id) == 0:\n",
    "        print('empty narrative found. skipping...')\n",
    "        #print(unit)\n",
    "        continue\n",
    "    data = [context_id, narrative_id, 1]\n",
    "    positive_data.append(data)\n",
    "    positive_str2.append(unit_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_num = int(len(positive_data) * 0.05)\n",
    "train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22242ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_all, dev_all, test_all = [], [], []\n",
    "for context_id, narrative_id, _ in train:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        train_all.append([context, response, narrative_id, response, 1])\n",
    "        flag = True\n",
    "        while flag:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if len(response) != len(random_response):\n",
    "                flag = False\n",
    "                train_all.append([context, random_response, narrative_id, response, 0])\n",
    "            else:\n",
    "                for idx, wid in enumerate(response):\n",
    "                    if wid != random_response[idx]:\n",
    "                        flag = False\n",
    "                        train_all.append([context, random_response, narrative_id, response, 0])\n",
    "                        break\n",
    "print(train_all[0]) \n",
    "print(train_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for context_id, narrative_id, _ in dev:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        dev_all.append([context, response, narrative_id, response, 1])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        # fix count 버그\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    dev_all.append([context, random_response, narrative_id, response, 0])\n",
    "                    count += 1\n",
    "                    negative_samples.append(random_response)\n",
    "                else:\n",
    "                    for idx, wid in enumerate(response):\n",
    "                        if wid != random_response[idx]:\n",
    "                            dev_all.append([context, random_response, narrative_id, response, 0])\n",
    "                            count += 1\n",
    "                            negative_samples.append(random_response)\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 1])\n",
    "        else:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 0])\n",
    "print(dev_all[0], dev_all[1], dev_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for context_id, narrative_id, _ in test:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        test_all.append([context, response, narrative_id, response, 1])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        # fix count 버그\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    test_all.append([context, random_response, narrative_id, response, 0])\n",
    "                    negative_samples.append(random_response)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for idx, id in enumerate(response):\n",
    "                        if id != random_response[idx]:\n",
    "                            test_all.append([context, random_response, narrative_id, response, 0])\n",
    "                            negative_samples.append(random_response)\n",
    "                            count += 1\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 1])\n",
    "        else:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 0])\n",
    "print(test_all[0], test_all[1], test_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total train count =', len(train_all))\n",
    "print('total val count =', len(dev_all))\n",
    "print('total test count =', len(test_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_from_nonfixed_2d_array(aa, max_sentence_len=50, max_num_utterance=10, padding_value=0):\n",
    "    PAD_SEQUENCE = np.array([0] * max_sentence_len)\n",
    "    rows = np.empty([0, max_sentence_len], dtype='int')\n",
    "    aa = aa[-max_num_utterance:]\n",
    "    for a in aa:\n",
    "        sentence_len = len(a)\n",
    "        if sentence_len < max_sentence_len:\n",
    "            rows  = np.append(rows, [np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)[:max_sentence_len]], axis=0)\n",
    "        else:\n",
    "            rows = np.append(rows, [a[:max_sentence_len]], axis=0)\n",
    "    num_utterance = len(aa)\n",
    "    if num_utterance < max_num_utterance:\n",
    "        rows = np.append(rows, [PAD_SEQUENCE]*(max_num_utterance-num_utterance), axis=0)\n",
    "    # add empty +1 sentence\n",
    "    rows = np.append(rows, [PAD_SEQUENCE], axis=0)\n",
    "    #return np.concatenate(rows, axis=0).reshape(-1, max_sentence_len)\n",
    "    return rows\n",
    "\n",
    "def get_numpy_from_nonfixed_1d_array(a, max_sentence_len=50, padding_value=0):\n",
    "    sentence_len = len(a)\n",
    "    if sentence_len < max_sentence_len:\n",
    "        return np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)\n",
    "    else:\n",
    "        return np.array(a[:max_sentence_len])\n",
    "\n",
    "cc_test_data = [\n",
    "        [1,2],\n",
    "        [4,5,6],\n",
    "        [7]\n",
    "     ]\n",
    "#get_numpy_from_nonfixed_2d_array(cc_test_data, max_sentence_len=5, max_num_utterance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ebe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    __IPYTHON__\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "def pad_process(data, max_sentence_len=50, max_num_utterance=10):\n",
    "    utterance = []\n",
    "    response = []\n",
    "    narrative = []\n",
    "    gt_response = []\n",
    "    y_true = []\n",
    "    for unit in tqdm(data):\n",
    "        utterance.append(get_numpy_from_nonfixed_2d_array(unit[0]))\n",
    "        response.append(get_numpy_from_nonfixed_1d_array(unit[1]))\n",
    "        narrative.append(get_numpy_from_nonfixed_1d_array(unit[2]))\n",
    "        gt_response.append(get_numpy_from_nonfixed_1d_array(unit[3]))\n",
    "        y_true.append(unit[4])\n",
    "        \n",
    "    utterance = np.stack(utterance)\n",
    "    response = np.stack(response)\n",
    "    narrative = np.stack(narrative)\n",
    "    gt_response = np.stack(gt_response)\n",
    "    y_true = np.stack(y_true)\n",
    "    return (utterance, response, narrative, gt_response, y_true)\n",
    "\n",
    "train_pad = pad_process(train_all)\n",
    "dev_pad = pad_process(dev_all)\n",
    "test_pad = pad_process(test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003946db",
   "metadata": {},
   "source": [
    "**학습데이터셋 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/train_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(train_pad, f)\n",
    "with open(f'data/dev_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_pad, f)\n",
    "with open(f'data/test_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/positive_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_data, f)\n",
    "with open(f'data/positive_str_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_str2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fbe23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449089fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit, unit_str in zip (positive_data, positive_str2):\n",
    "    len_unit = len(unit[0])\n",
    "    len_unit_str = len(unit_str['script'])\n",
    "    if len_unit != len_unit_str+1:\n",
    "        print(len_unit, len_unit_str)\n",
    "    #print(unit[0])\n",
    "    #print(unit_str['script'])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f477e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_dat(index, data_pad):\n",
    "    utterances = data_pad[0][index]\n",
    "    response = data_pad[1][index]\n",
    "    narrative = data_pad[2][index]\n",
    "    gt_response = data_pad[3][index]\n",
    "    y_true = data_pad[4][index]\n",
    "    narrative = narrative[narrative!=0]\n",
    "    response = response[response!=0]\n",
    "    gt_response = gt_response[gt_response!=0]\n",
    "    #print([model.wv.index_to_key[k-1] for k in narrative])\n",
    "    print('N:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in narrative]))\n",
    "    print('R:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in response]))\n",
    "    print('T:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in gt_response]))\n",
    "    print(y_true)\n",
    "    for i in range(10):\n",
    "        utterance = utterances[i]\n",
    "        utterance = utterance[utterance!=0]\n",
    "        if len(utterance) == 0:\n",
    "            break\n",
    "        print('U:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in utterance]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "#browse_dat(0, train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30): \n",
    "    browse_dat(i, train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82471e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
