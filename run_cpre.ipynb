{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e31e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import Utils\n",
    "import Evaluate\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6e4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# for train\n",
    "#embedding_file = \"./data/embeddings.pkl\"\n",
    "#train_file = \"./data/train.gr.pkl\"\n",
    "#val_file = \"./data/dev.gr.pkl\"\n",
    "#evaluate_file = \"./data/test.gr.pkl\"\n",
    "embedding_file = \"./data/embeddings_ko.pkl\"\n",
    "train_file = \"./data/train_ko.pkl\"\n",
    "val_file = \"./data/dev_ko.pkl\"\n",
    "evaluate_file = \"./data/test_ko.pkl\"\n",
    "\n",
    "save_path = \"./model/cpre/\"\n",
    "result_path = \"./output/cpre/\"\n",
    "log_path = \"./model/cpre/\"\n",
    "\n",
    "max_sentence_len = 50\n",
    "max_num_utterance = 11\n",
    "batch_size = 50\n",
    "eval_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311cb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptWriter_cpre():\n",
    "    def __init__(self, eta=0.5):\n",
    "        self.max_num_utterance = max_num_utterance\n",
    "        self.negative_samples = 1\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        self.word_embedding_size = 200\n",
    "        self.hidden_units = 200\n",
    "        #self.total_words = 43514\n",
    "        self.total_words = 11883\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.learning_rate_ph = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "        self.dropout_rate = 0\n",
    "        self.num_heads = 1\n",
    "        self.num_blocks = 3\n",
    "        self.eta = eta\n",
    "        self.gamma = tf.compat.v1.get_variable('gamma', shape=1, dtype=tf.float32, trainable=True, initializer=tf.constant_initializer(0.5))\n",
    "\n",
    "        self.embedding_ph = tf.compat.v1.placeholder(tf.float32, shape=(self.total_words, self.word_embedding_size))\n",
    "        self.utterance_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_num_utterance, max_sentence_len))\n",
    "        self.response_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_sentence_len))\n",
    "        self.gt_response_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_sentence_len))\n",
    "        self.y_true_ph = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
    "        self.narrative_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_sentence_len))\n",
    "\n",
    "        self.word_embeddings = tf.compat.v1.get_variable('word_embeddings_v', shape=(self.total_words, self.word_embedding_size), dtype=tf.float32, trainable=False)\n",
    "        self.embedding_init = self.word_embeddings.assign(self.embedding_ph)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.is_training = True\n",
    "        print(\"current eta: \", self.eta)\n",
    "\n",
    "    def load(self, previous_modelpath):\n",
    "        #sess = tf.Session()\n",
    "        sess = tf.compat.v1.Session()\n",
    "        latest_ckpt = tf.compat.v1.train.latest_checkpoint(previous_modelpath)\n",
    "        # print(\"recover from checkpoint: \" + latest_ckpt)\n",
    "        #variables = tf.contrib.framework.get_variables_to_restore()\n",
    "        #variables = tf.compat.v1.get_variables_to_restore()\n",
    "        #saver = tf.train.Saver(variables)\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "        return sess\n",
    "\n",
    "    def build(self):\n",
    "        all_utterances = tf.unstack(self.utterance_ph, num=self.max_num_utterance, axis=1)\n",
    "        reuse = None\n",
    "        alpha_1, alpha_2 = None, None\n",
    "\n",
    "        response_embeddings = embedding(self.response_ph, initializer=self.word_embeddings)\n",
    "        Hr_stack = [response_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            with tf.compat.v1.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                response_embeddings, _ = multihead_attention(queries=response_embeddings, keys=response_embeddings, num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                response_embeddings = feedforward(response_embeddings, num_units=[self.hidden_units, self.hidden_units])\n",
    "                Hr_stack.append(response_embeddings)\n",
    "\n",
    "        gt_response_embeddings = embedding(self.gt_response_ph, initializer=self.word_embeddings)\n",
    "        Hgtr_stack = [gt_response_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            with tf.compat.v1.variable_scope(\"num_blocks_{}\".format(i), reuse=True):\n",
    "                gt_response_embeddings, _ = multihead_attention(queries=gt_response_embeddings, keys=gt_response_embeddings, num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                gt_response_embeddings = feedforward(gt_response_embeddings, num_units=[self.hidden_units, self.hidden_units])\n",
    "                Hgtr_stack.append(gt_response_embeddings)\n",
    "\n",
    "        narrative_embeddings = embedding(self.narrative_ph, initializer=self.word_embeddings)\n",
    "        Hn_stack = [narrative_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            with tf.compat.v1.variable_scope(\"num_blocks_{}\".format(i), reuse=True):\n",
    "                narrative_embeddings, _ = multihead_attention(queries=narrative_embeddings, keys=narrative_embeddings, num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                narrative_embeddings = feedforward(narrative_embeddings, num_units=[self.hidden_units, self.hidden_units])\n",
    "                Hn_stack.append(narrative_embeddings)\n",
    "\n",
    "        Mur, Mun = [], []\n",
    "        self.decay_factor = []\n",
    "        last_u_reps = []\n",
    "        turn_id = 0\n",
    "        for utterance in all_utterances:\n",
    "            utterance_embeddings = embedding(utterance, initializer=self.word_embeddings)\n",
    "            Hu_stack = [utterance_embeddings]\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.compat.v1.variable_scope(\"num_blocks_{}\".format(i), reuse=True):\n",
    "                    utterance_embeddings, _ = multihead_attention(queries=utterance_embeddings, keys=utterance_embeddings, num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                    utterance_embeddings = feedforward(utterance_embeddings, num_units=[self.hidden_units, self.hidden_units])\n",
    "                    Hu_stack.append(utterance_embeddings)\n",
    "\n",
    "            if turn_id == self.max_num_utterance - 1:\n",
    "                last_u_reps = Hu_stack\n",
    "\n",
    "            r_a_u_stack = []\n",
    "            u_a_r_stack = []\n",
    "\n",
    "            for i in range(self.num_blocks + 1):\n",
    "                with tf.compat.v1.variable_scope(\"utterance_attention_response_{}\".format(i), reuse=reuse):\n",
    "                    u_a_r, _ = multihead_attention(queries=Hu_stack[i], keys=Hr_stack[i], num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                    u_a_r = feedforward(u_a_r, num_units=[self.hidden_units, self.hidden_units])\n",
    "                    u_a_r_stack.append(u_a_r)\n",
    "                with tf.compat.v1.variable_scope(\"response_attention_utterance_{}\".format(i), reuse=reuse):\n",
    "                    r_a_u, _ = multihead_attention(queries=Hr_stack[i], keys=Hu_stack[i], num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                    r_a_u = feedforward(r_a_u, num_units=[self.hidden_units, self.hidden_units])\n",
    "                    r_a_u_stack.append(r_a_u)\n",
    "            u_a_r_stack.extend(Hu_stack)\n",
    "            r_a_u_stack.extend(Hr_stack)\n",
    "\n",
    "            n_a_u_stack = []\n",
    "            u_a_n_stack = []\n",
    "            for i in range(self.num_blocks + 1):\n",
    "                with tf.compat.v1.variable_scope(\"narrative_attention_response_{}\".format(i), reuse=reuse):\n",
    "                    n_a_u, _ = multihead_attention(queries=Hn_stack[i], keys=Hu_stack[i], num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                    n_a_u = feedforward(n_a_u, num_units=[self.hidden_units, self.hidden_units])\n",
    "                    n_a_u_stack.append(n_a_u)\n",
    "                with tf.compat.v1.variable_scope(\"response_attention_narrative_{}\".format(i), reuse=reuse):\n",
    "                    u_a_n, alpha_1 = multihead_attention(queries=Hu_stack[i], keys=Hn_stack[i], num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                    u_a_n = feedforward(u_a_n, num_units=[self.hidden_units, self.hidden_units])\n",
    "                    u_a_n_stack.append(u_a_n)\n",
    "            n_a_u_stack.extend(Hn_stack)\n",
    "            u_a_n_stack.extend(Hu_stack)\n",
    "\n",
    "            u_a_r = tf.stack(u_a_r_stack, axis=-1)\n",
    "            r_a_u = tf.stack(r_a_u_stack, axis=-1)\n",
    "            u_a_n = tf.stack(u_a_n_stack, axis=-1)\n",
    "            n_a_u = tf.stack(n_a_u_stack, axis=-1)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('similarity'):\n",
    "                # sim shape [batch, max_sent_len, max_sent_len, 2 * (stack_num + 1)]\n",
    "                sim_ur = tf.einsum('biks,bjks->bijs', u_a_r, r_a_u) / tf.sqrt(200.0)  # for no rp and normal\n",
    "                sim_un = tf.einsum('biks,bjks->bijs', u_a_n, n_a_u) / tf.sqrt(200.0)  # for no rp and normal\n",
    "\n",
    "            self_n = tf.nn.l2_normalize(tf.stack(Hn_stack, axis=-1))  # #for no rp\n",
    "            self_u = tf.nn.l2_normalize(tf.stack(Hu_stack, axis=-1))  # #for no rp\n",
    "            Hn_stack_tensor = tf.stack(Hn_stack, axis=-1)  # [batch, o_len, embedding_size, stack]\n",
    "            with tf.compat.v1.variable_scope('similarity'):\n",
    "                self_sim = tf.einsum('biks,bjks->bijs', self_u, self_n)  # [batch, u_len, o_len, stack]\n",
    "                self_sim = 1 - self.gamma * tf.reduce_sum(self_sim, axis=1)  # [batch, (1), o_len, stack]\n",
    "                Hn_stack = tf.einsum('bjkl,bjl->bjkl', Hn_stack_tensor, self_sim)\n",
    "                Hn_stack = tf.unstack(Hn_stack, axis=-1, num=self.num_blocks + 1)\n",
    "\n",
    "            Mur.append(sim_ur)\n",
    "            Mun.append(sim_un)\n",
    "            turn_id += 1\n",
    "            if not reuse:\n",
    "                reuse = True\n",
    "\n",
    "        Hn_stack_for_tracking = tf.compat.v1.layers.dense(tf.stack(Hn_stack, axis=2), self.hidden_units)  # [batch, o_len, stack, embedding_size]\n",
    "        Hn_stack_for_tracking = tf.transpose(Hn_stack_for_tracking, perm=[0, 1, 3, 2])  # [batch, o_len, embedding_size, stack]\n",
    "        Hlastu_stack_for_tracking = tf.stack(last_u_reps, axis=-1)  # [batch, u_len, embedding_size, stack]\n",
    "        Hr_stack_for_tracking = tf.stack(Hgtr_stack, axis=-1)  # [batch, r_len, embedding_size, stack]\n",
    "        Hlastu = tf.transpose(Hlastu_stack_for_tracking, perm=[0, 2, 3, 1])\n",
    "        Hlastu = tf.squeeze(tf.compat.v1.layers.dense(Hlastu, 1), axis=-1)  # [batch, embedding_size, stack]\n",
    "        p1_tensor = tf.nn.softmax(tf.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastu), axis=1)  # [batch, o_len, stack]\n",
    "        Hlastur = tf.transpose(Hr_stack_for_tracking, perm=[0, 2, 3, 1])\n",
    "        Hlastur = tf.squeeze(tf.compat.v1.layers.dense(Hlastur, 1), axis=-1)  # [batch, embedding_size, stack]\n",
    "        p2_tensor = tf.nn.softmax(tf.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastur), axis=1)  # [batch, o_len, stack]\n",
    "        p1 = tf.unstack(p1_tensor, num=self.num_blocks + 1, axis=-1)\n",
    "        p2 = tf.unstack(p2_tensor, num=self.num_blocks + 1, axis=-1)\n",
    "        KL_loss = 0.0\n",
    "        for i in range(self.num_blocks + 1):\n",
    "            KL_loss += tf.reduce_mean(tf.keras.losses.kullback_leibler_divergence(p1[i], p2[i]))\n",
    "        KL_loss /= (self.num_blocks + 1)\n",
    "\n",
    "        r_a_n_stack = []\n",
    "        n_a_r_stack = []\n",
    "        for i in range(self.num_blocks + 1):\n",
    "            with tf.compat.v1.variable_scope(\"narrative_attention_response_{}\".format(i), reuse=True):\n",
    "                n_a_r, _ = multihead_attention(queries=Hn_stack[i], keys=Hr_stack[i], num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                n_a_r = feedforward(n_a_r, num_units=[self.hidden_units, self.hidden_units])\n",
    "                n_a_r_stack.append(n_a_r)\n",
    "            with tf.compat.v1.variable_scope(\"response_attention_narrative_{}\".format(i), reuse=True):\n",
    "                r_a_n, _ = multihead_attention(queries=Hr_stack[i], keys=Hn_stack[i], num_units=self.hidden_units, num_heads=self.num_heads, is_training=self.is_training, causality=False, dropout_rate=self.dropout_rate)\n",
    "                r_a_n = feedforward(r_a_n, num_units=[self.hidden_units, self.hidden_units])\n",
    "                r_a_n_stack.append(r_a_n)\n",
    "\n",
    "        n_a_r_stack.extend(Hn_stack)\n",
    "        r_a_n_stack.extend(Hr_stack)\n",
    "        n_a_r = tf.stack(n_a_r_stack, axis=-1)\n",
    "        r_a_n = tf.stack(r_a_n_stack, axis=-1)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('similarity'):\n",
    "            Mrn = tf.einsum('biks,bjks->bijs', n_a_r, r_a_n) / tf.sqrt(200.0)\n",
    "        self.rosim = Mrn\n",
    "        Mur = tf.stack(Mur, axis=1)\n",
    "        Mun = tf.stack(Mun, axis=1) \n",
    "        with tf.compat.v1.variable_scope('cnn_aggregation'):\n",
    "            conv3d = tf.compat.v1.layers.conv3d(Mur, filters=32, kernel_size=[3, 3, 3], padding=\"SAME\", activation=tf.nn.elu, kernel_initializer=tf.random_uniform_initializer(-0.01, 0.01), name=\"conv1\")\n",
    "            pool3d = tf.compat.v1.layers.max_pooling3d(conv3d, pool_size=[3, 3, 3], strides=[3, 3, 3], padding=\"SAME\")\n",
    "            conv3d2 = tf.compat.v1.layers.conv3d(pool3d, filters=32, kernel_size=[3, 3, 3], padding=\"SAME\", activation=tf.nn.elu, kernel_initializer=tf.random_uniform_initializer(-0.01, 0.01), name=\"conv2\")\n",
    "            pool3d2 = tf.compat.v1.layers.max_pooling3d(conv3d2, pool_size=[3, 3, 3], strides=[3, 3, 3], padding=\"SAME\")\n",
    "            mur = tf.compat.v1.layers.flatten(pool3d2)\n",
    "        with tf.compat.v1.variable_scope('cnn_aggregation', reuse=True):\n",
    "            conv3d = tf.compat.v1.layers.conv3d(Mun, filters=32, kernel_size=[3, 3, 3], padding=\"SAME\", activation=tf.nn.elu, kernel_initializer=tf.random_uniform_initializer(-0.01, 0.01), name=\"conv1\")\n",
    "            pool3d = tf.compat.v1.layers.max_pooling3d(conv3d, pool_size=[3, 3, 3], strides=[3, 3, 3], padding=\"SAME\")\n",
    "            conv3d2 = tf.compat.v1.layers.conv3d(pool3d, filters=32, kernel_size=[3, 3, 3], padding=\"SAME\", activation=tf.nn.elu, kernel_initializer=tf.random_uniform_initializer(-0.01, 0.01), name=\"conv2\")\n",
    "            pool3d2 = tf.compat.v1.layers.max_pooling3d(conv3d2, pool_size=[3, 3, 3], strides=[3, 3, 3], padding=\"SAME\")\n",
    "            mun = tf.compat.v1.layers.flatten(pool3d2)\n",
    "        with tf.compat.v1.variable_scope('cnn_aggregation'):\n",
    "            conv2d = tf.compat.v1.layers.conv2d(Mrn, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.elu, kernel_initializer=tf.random_uniform_initializer(-0.01, 0.01), name=\"conv2d\")\n",
    "            pool2d = tf.compat.v1.layers.max_pooling2d(conv2d, pool_size=[3, 3], strides=[3, 3], padding=\"SAME\")\n",
    "            conv2d2 = tf.compat.v1.layers.conv2d(pool2d, filters=32, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.elu, kernel_initializer=tf.random_uniform_initializer(-0.01, 0.01), name=\"conv2d2\")\n",
    "            pool2d2 = tf.compat.v1.layers.max_pooling2d(conv2d2, pool_size=[3, 3], strides=[3, 3], padding=\"SAME\")\n",
    "            mrn = tf.compat.v1.layers.flatten(pool2d2)\n",
    "\n",
    "        all_vector = tf.concat([mur, mun, mrn], axis=-1)\n",
    "        logits = tf.reshape(tf.compat.v1.layers.dense(all_vector, 1, kernel_initializer=tf.compat.v1.orthogonal_initializer()), [-1])\n",
    "\n",
    "        self.y_pred = tf.sigmoid(logits)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate_ph, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "        RS_loss = tf.reduce_mean(tf.clip_by_value(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(self.y_true_ph, tf.float32), logits=logits), -10, 10))\n",
    "        self.loss = self.eta * RS_loss + (1 - self.eta) * KL_loss\n",
    "        self.all_variables = tf.compat.v1.global_variables()\n",
    "        self.grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "\n",
    "        for grad, var in self.grads_and_vars:\n",
    "            if grad is None:\n",
    "                print(var)\n",
    "\n",
    "        self.capped_gvs = [(tf.clip_by_value(grad, -5, 5), var) for grad, var in self.grads_and_vars]\n",
    "        self.train_op = optimizer.apply_gradients(self.capped_gvs, global_step=self.global_step)\n",
    "        self.saver = tf.compat.v1.train.Saver(max_to_keep=10)\n",
    "        self.alpha_1 = alpha_1\n",
    "        # self.alpha_2 = alpha_2\n",
    "        # self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc60d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_evaluate(sess, model, eval_file):\n",
    "    with open(eval_file, 'rb') as f:\n",
    "        utterance, response, narrative, y_true = pickle.load(f)\n",
    "    utterance, utterance_len = Utils.multi_sequences_padding(utterance, max_sentence_len, max_num_utterance=max_num_utterance)\n",
    "    utterance = np.array(utterance)\n",
    "    narrative = np.array(pad_sequences(narrative, padding='post', maxlen=max_sentence_len))\n",
    "    response = np.array(pad_sequences(response, padding='post', maxlen=max_sentence_len))\n",
    "    y_true = np.array(y_true)\n",
    "    all_candidate_scores = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((utterance, narrative, response, y_true)).batch(eval_batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    data_iterator = iterator.get_next()\n",
    "    sess.run(iterator.initializer)\n",
    "    current_lr = 1e-3\n",
    "    test_loss = 0.0\n",
    "    step = 0\n",
    "    try:\n",
    "        with tqdm(total=len(y_true), ncols=100) as pbar:\n",
    "            while True:\n",
    "                bu, bn, br, by = data_iterator\n",
    "                bu, bn, br, by = sess.run([bu, bn, br, by])\n",
    "                candidate_scores, loss = sess.run([model.y_pred, model.loss], feed_dict={\n",
    "                    model.utterance_ph: bu,\n",
    "                    model.narrative_ph: bn,\n",
    "                    model.response_ph: br,\n",
    "                    model.y_true_ph: by,\n",
    "                    model.gt_response_ph: br,\n",
    "                    model.learning_rate_ph: current_lr\n",
    "                })\n",
    "                all_candidate_scores.append(candidate_scores)\n",
    "                test_loss += loss\n",
    "                pbar.update(eval_batch_size)\n",
    "                step += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    all_candidate_scores = np.concatenate(all_candidate_scores, axis=0)\n",
    "    return Evaluate.evaluate_all(all_candidate_scores, y_true), test_loss / step, all_candidate_scores.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c643c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multi_turns(test_file, model_path, output_path):\n",
    "    vocab = {}\n",
    "    vocab_id2word = {}\n",
    "\n",
    "    with open(embedding_file, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    model = ScriptWriter_cpre()\n",
    "    model.build()\n",
    "    sess = model.load(model_path)\n",
    "    sess.run(model.embedding_init, feed_dict={model.embedding_ph: embeddings})\n",
    "\n",
    "    with open(\"./data/vocab.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            vocab[line[0]] = idx + 1\n",
    "            vocab_id2word[idx + 1] = line[0]\n",
    "    vocab[\"_PAD_\"] = 0\n",
    "    vocab_id2word[0] = \"_PAD_\"\n",
    "\n",
    "    def initialize(test_file):\n",
    "        initial_file = output_path + \"test.multi.0.pkl\"\n",
    "        max_turn = 0\n",
    "        narrative_dict = {}\n",
    "        narrative_dict_score = {}\n",
    "\n",
    "        with open(test_file, 'rb') as f:\n",
    "            utterance, response, outline, labels = pickle.load(f)\n",
    "        new_utterance, new_response, new_narrative, new_labels = [], [], [], []\n",
    "        for i in range(len(response)):\n",
    "            ut = utterance[i]\n",
    "            if len(ut) == 1:\n",
    "                o = outline[i]\n",
    "                r = response[i]\n",
    "                l = labels[i]\n",
    "                new_utterance.append(ut)\n",
    "                new_response.append(r)\n",
    "                new_narrative.append(o)\n",
    "                new_labels.append(l)\n",
    "            if len(ut) > max_turn:\n",
    "                max_turn = len(ut)\n",
    "            o = \"\".join([vocab_id2word[x] for x in outline[i]])\n",
    "            if o not in narrative_dict:\n",
    "                narrative_dict[o] = {0: outline[i]}\n",
    "                narrative_dict_score[o] = {0: [-1]}\n",
    "            r = response[i]\n",
    "            l = labels[i]\n",
    "            if len(ut) in narrative_dict[o]:\n",
    "                narrative_dict[o][len(ut)].append(r)\n",
    "                narrative_dict_score[o][len(ut)].append(l)\n",
    "            else:\n",
    "                narrative_dict[o][len(ut)] = [r]\n",
    "                narrative_dict_score[o][len(ut)] = [l]\n",
    "\n",
    "        pickle.dump(narrative_dict, open(output_path + \"response_candidate.pkl\", \"wb\"))\n",
    "\n",
    "        new_data = [new_utterance, new_response, new_narrative, new_labels]\n",
    "        pickle.dump(new_data, open(initial_file, \"wb\"))\n",
    "\n",
    "        (r2_1, r10_1, r10_2, r10_5, mrr), eva_loss, result = simple_evaluate(sess, model, initial_file)\n",
    "        with open(output_path + \"test.result.multi.0.txt\", \"w\") as fw:\n",
    "            fw.write(\"R2@1: %f, R10@1: %f, R10@2: %f, R10@5: %f, MRR: %f\\n\" % (r2_1, r10_1, r10_2, r10_5, mrr))\n",
    "            for r in result:\n",
    "                fw.write(str(r) + \"\\n\")\n",
    "\n",
    "        return max_turn, narrative_dict, narrative_dict_score\n",
    "\n",
    "    max_turn, narrative_dict, narrative_dict_score = initialize(test_file)\n",
    "    for turn in range(1, max_turn):\n",
    "        score = []\n",
    "        with open(output_path + \"test.result.multi.\" + str(turn - 1) + \".txt\", \"r\") as fr:\n",
    "            for idx, line in enumerate(fr):\n",
    "                if idx == 0:\n",
    "                    continue\n",
    "                score.append(float(line.strip()))\n",
    "        with open(output_path + \"test.multi.\" + str(turn - 1) + \".pkl\", \"rb\") as fr:\n",
    "            utterance, response, narrative, y_true = pickle.load(fr)\n",
    "\n",
    "        new_utterance = []\n",
    "        new_response = []\n",
    "        new_narrative = []\n",
    "        new_labels = []\n",
    "\n",
    "        for i, o in enumerate(narrative):\n",
    "            if i % 10 == 0:\n",
    "                sent_o = \"\".join([vocab_id2word[x] for x in o])\n",
    "                if turn + 1 in narrative_dict[sent_o]:\n",
    "                    new_response.extend(narrative_dict[sent_o][turn + 1])\n",
    "                    score_sub_list = score[i:i + 10]\n",
    "                    response_sub_list = response[i:i + 10]\n",
    "                    max_idx = score_sub_list.index(max(score_sub_list))\n",
    "                    selected_response = response_sub_list[max_idx]\n",
    "                    for ut in utterance[i:i + 10]:\n",
    "                        tmp = ut + [selected_response]\n",
    "                        new_utterance.append(tmp)\n",
    "                    new_narrative.extend([o] * 10)\n",
    "                    new_labels.extend(narrative_dict_score[sent_o][turn + 1])\n",
    "\n",
    "        new_data = [new_utterance, new_response, new_narrative, new_labels]\n",
    "        new_file = output_path + \"test.multi.\" + str(turn) + \".pkl\"\n",
    "        pickle.dump(new_data, open(new_file, \"wb\"))\n",
    "\n",
    "        (r2_1, r10_1, r10_2, r10_5, mrr), eva_loss, result = simple_evaluate(sess, model, new_file)\n",
    "        with open(output_path + \"test.result.multi.\" + str(turn) + \".txt\", \"w\") as fw:\n",
    "            fw.write(\"R2@1: %f, R10@1: %f, R10@2: %f, R10@5: %f, MRR: %f\\n\" % (r2_1, r10_1, r10_2, r10_5, mrr))\n",
    "            for r in result:\n",
    "                fw.write(str(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09db048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(eta=0.5, load=False, model_path=None, logger=None):\n",
    "    config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    epoch = 0\n",
    "    best_result = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        with open(embedding_file, 'rb') as f:\n",
    "            embeddings = pickle.load(f, encoding=\"bytes\")\n",
    "        with open(train_file, 'rb') as f:\n",
    "            utterance_train, response_train, narrative_train, gt_response_train, y_true_train = pickle.load(f)\n",
    "        with open(val_file, \"rb\") as f:\n",
    "            utterance_val, response_val, narrative_val, gt_response_val, y_true_val = pickle.load(f)\n",
    "\n",
    "        train_dataset = tf.compat.v1.data.Dataset.from_tensor_slices((utterance_train, narrative_train, response_train, gt_response_train, y_true_train)).shuffle(1024).batch(batch_size)\n",
    "        train_iterator = train_dataset.make_initializable_iterator()\n",
    "        train_data_iterator = train_iterator.get_next()\n",
    "\n",
    "        val_dataset = tf.compat.v1.data.Dataset.from_tensor_slices((utterance_val, narrative_val, response_val, gt_response_val, y_true_val)).batch(batch_size)\n",
    "        val_iterator = val_dataset.make_initializable_iterator()\n",
    "        val_data_iterator = val_iterator.get_next()\n",
    "\n",
    "        model = ScriptWriter_cpre(eta=eta)\n",
    "        model.build()\n",
    "\n",
    "        if load:\n",
    "            sess = model.load(model_path)\n",
    "\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(model.embedding_init, feed_dict={model.embedding_ph: embeddings})\n",
    "        current_lr = 1e-3\n",
    "\n",
    "        while epoch < 4:\n",
    "            print(\"\\nEpoch \", epoch + 1, \"/ 4\")\n",
    "            train_loss = 0.0\n",
    "            sess.run(train_iterator.initializer)\n",
    "            step = 0\n",
    "            try:\n",
    "                with tqdm(total=len(y_true_train), ncols=100) as pbar:\n",
    "                    while True:\n",
    "                        bu, bn, br, bgtr, by = train_data_iterator\n",
    "                        bu, bn, br, bgtr, by = sess.run([bu, bn, br, bgtr, by])\n",
    "                        _, loss = sess.run([model.train_op, model.loss], feed_dict={\n",
    "                            model.utterance_ph: bu, \n",
    "                            model.narrative_ph: bn,\n",
    "                            model.response_ph: br,\n",
    "                            model.gt_response_ph: bgtr,\n",
    "                            model.y_true_ph: by,\n",
    "                            model.learning_rate_ph: current_lr\n",
    "                        })\n",
    "                        train_loss += loss\n",
    "                        pbar.set_postfix(learning_rate=current_lr, loss=loss)\n",
    "                        pbar.update(model.batch_size)\n",
    "                        step += 1\n",
    "                        if step % 500 == 0:\n",
    "                            val_loss = 0.0\n",
    "                            val_step = 0\n",
    "                            sess.run(val_iterator.initializer)\n",
    "                            all_candidate_scores = []\n",
    "                            try:\n",
    "                                while True:\n",
    "                                    bu, bn, br, bgtr, by = val_data_iterator\n",
    "                                    bu, bn, br, bgtr, by = sess.run([bu, bn, br, bgtr, by])\n",
    "                                    candidate_scores, loss = sess.run([model.y_pred, model.loss], feed_dict={\n",
    "                                        model.utterance_ph: bu, \n",
    "                                        model.narrative_ph: bn,\n",
    "                                        model.response_ph: br,\n",
    "                                        model.gt_response_ph: bgtr,\n",
    "                                        model.y_true_ph: by,\n",
    "                                    })\n",
    "                                    all_candidate_scores.append(candidate_scores)\n",
    "                                    val_loss += loss\n",
    "                                    val_step += 1\n",
    "                            except tf.errors.OutOfRangeError:\n",
    "                                pass\n",
    "                            all_candidate_scores = np.concatenate(all_candidate_scores, axis=0)\n",
    "                            result = Evaluate.evaluate_all(all_candidate_scores, y_true_val)\n",
    "                            if result[0] + result[1] + result[2] + result[3] + result[4] > best_result[0] + best_result[1] + best_result[2] + best_result[3] + best_result[4]:\n",
    "                                best_result = result\n",
    "                                tqdm.write(\"Current best result on validation set: r2@1 %.3f, r10@1 %.3f, r10@2 %.3f, r10@5 %.3f, mrr %.3f\" % (best_result[0], best_result[1], best_result[2], best_result[3], best_result[4]))\n",
    "                                logger.info(\"Current best result on validation set: r2@1 %.3f, r10@1 %.3f, r10@2 %.3f, r10@5 %.3f, mrr %.3f\" % (best_result[0], best_result[1], best_result[2], best_result[3], best_result[4]))\n",
    "                                model.saver.save(sess, save_path + \"model\")\n",
    "                                patience = 0\n",
    "                            else:\n",
    "                                patience += 1\n",
    "                                if patience >= 3:\n",
    "                                    current_lr *= 0.5\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            \n",
    "            model.saver.save(sess, save_path + \"model\")\n",
    "            print(len(all_candidate_scores))\n",
    "            print(all_candidate_scores)\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_step = 0\n",
    "            sess.run(val_iterator.initializer)\n",
    "            all_candidate_scores = []\n",
    "            try:\n",
    "                while True:\n",
    "                    bu, bn, br, bgtr, by = val_data_iterator\n",
    "                    bu, bn, br, bgtr, by = sess.run([bu, bn, br, bgtr, by])\n",
    "                    candidate_scores, loss = sess.run([model.y_pred, model.loss], feed_dict={\n",
    "                        model.utterance_ph: bu, \n",
    "                        model.narrative_ph: bn,\n",
    "                        model.response_ph: br,\n",
    "                        model.gt_response_ph: bgtr,\n",
    "                        model.y_true_ph: by\n",
    "                    })\n",
    "                    all_candidate_scores.append(candidate_scores)\n",
    "                    val_loss += loss\n",
    "                    val_step += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            all_candidate_scores = np.concatenate(all_candidate_scores, axis=0)\n",
    "            result = Evaluate.evaluate_all(all_candidate_scores, y_true_val)\n",
    "            if result[0] + result[1] + result[2] + result[3] + result[4] > best_result[0] + best_result[1] + best_result[2] + best_result[3] + best_result[4]:\n",
    "                best_result = result\n",
    "                tqdm.write(\"Current best result on validation set: r2@1 %.3f, r10@1 %.3f, r10@2 %.3f, r10@5 %.3f, mrr %.3f\" % (best_result[0], best_result[1], best_result[2], best_result[3], best_result[4]))\n",
    "                logger.info(\"Current best result on validation set: r2@1 %.3f, r10@1 %.3f, r10@2 %.3f, r10@5 %.3f, mrr %.3f\" % (best_result[0], best_result[1], best_result[2], best_result[3], best_result[4]))\n",
    "                model.saver.save(sess, save_path + \"model\")\n",
    "            tqdm.write('Epoch No: %d, the train loss is %f, the dev loss is %f' % (epoch + 1, train_loss / step, val_loss / val_step))\n",
    "            epoch += 1\n",
    "        sess.close()\n",
    "    tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb691094",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 16:44:52.974466: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-12 16:44:53.520605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21469 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_16256/3534897697.py:15: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_initializable_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "current eta:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/core.py:393: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
      "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/convolutional.py:263: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  warnings.warn('`tf.layers.conv1d` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/convolutional.py:811: UserWarning: `tf.layers.conv3d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv3D` instead.\n",
      "  warnings.warn('`tf.layers.conv3d` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/pooling.py:830: UserWarning: `tf.layers.max_pooling3d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling3D` instead.\n",
      "  warnings.warn('`tf.layers.max_pooling3d` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/core.py:513: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/convolutional.py:536: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  warnings.warn('`tf.layers.conv2d` is deprecated and '\n",
      "/home/kotech/venv-tensor2/lib/python3.8/site-packages/keras/legacy_tf_layers/pooling.py:554: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  warnings.warn('`tf.layers.max_pooling2d` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                    | 0/246578 [00:00<?, ?it/s]2022-08-12 16:47:54.177301: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-08-12 16:47:54.648735: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      " 10%|██▏                   | 25000/246578 [13:59<2:04:02, 29.77it/s, learning_rate=0.001, loss=0.27]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16256/250485085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Eta: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_16256/3534897697.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(eta, load, model_path, logger)\u001b[0m\n\u001b[1;32m     74\u001b[0m                                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                             \u001b[0mall_candidate_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_candidate_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_candidate_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                 \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ScriptWriter/Evaluate.py\u001b[0m in \u001b[0;36mevaluate_all\u001b[0;34m(scores, labels)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_r_n_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_r_n_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_r_n_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m            \u001b[0mcompute_r_n_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_mrr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_all_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ScriptWriter/Evaluate.py\u001b[0m in \u001b[0;36mcompute_r_n_m\u001b[0;34m(scores, labels, count, at)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mpos_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msublist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpos_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "log_path = \"./model/cpre/all_log\"\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "eta = 0.7\n",
    "save_path = \"./model/cpre/\"\n",
    "result_path = \"./output/cpre/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "if not os.path.exists(result_path):\n",
    "    os.mkdir(result_path)\n",
    "logger.info(\"Current Eta: %.2f\" % eta)\n",
    "train(eta=eta, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bedb63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, eval_file, output_path, eta):\n",
    "    with open(eval_file, 'rb') as f:\n",
    "        utterance, response, narrative, gt_response, y_true = pickle.load(f)\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "        \n",
    "    current_lr = 1e-3\n",
    "    all_candidate_scores = []\n",
    "    \n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        dataset = tf.compat.v1.data.Dataset.from_tensor_slices((utterance, narrative, response, gt_response, y_true)).batch(eval_batch_size)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        data_iterator = iterator.get_next()\n",
    "\n",
    "        with open(embedding_file, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "\n",
    "        model = ScriptWriter_cpre(eta)\n",
    "        model.build()\n",
    "        sess = model.load(model_path)\n",
    "        sess.run(iterator.initializer)\n",
    "        sess.run(model.embedding_init, feed_dict={model.embedding_ph: embeddings})\n",
    "\n",
    "        test_loss = 0.0\n",
    "        step = 0\n",
    "        try:\n",
    "            with tqdm(total=len(y_true), ncols=100) as pbar:\n",
    "                while True:\n",
    "                    bu, bn, br, bgtr, by = data_iterator\n",
    "                    bu, bn, br, bgtr, by = sess.run([bu, bn, br, bgtr, by])\n",
    "                    candidate_scores, loss = sess.run([model.y_pred, model.loss], feed_dict={\n",
    "                        model.utterance_ph: bu, \n",
    "                        model.narrative_ph: bn,\n",
    "                        model.response_ph: br,\n",
    "                        model.gt_response_ph: bgtr,\n",
    "                        model.y_true_ph: by,\n",
    "                        model.learning_rate_ph: current_lr\n",
    "                    })\n",
    "                    all_candidate_scores.append(candidate_scores)\n",
    "                    test_loss += loss\n",
    "                    pbar.update(model.eval_batch_size)\n",
    "                    step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        #sess.close()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    all_candidate_scores = np.concatenate(all_candidate_scores, axis=0)\n",
    "    with open(output_path + \"test.result.micro_session.txt\", \"w\") as fw:\n",
    "        for sc in all_candidate_scores.tolist():\n",
    "            fw.write(str(sc) + \"\\n\")\n",
    "    return Evaluate.evaluate_all(all_candidate_scores, y_true), test_loss / step, all_candidate_scores.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4d33c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 17:35:07.917683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21469 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current eta:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 17:36:05.857107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21469 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/cpre/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38400it [02:09, 297.55it/s]                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test set: 0.458008, R2@1: 0.736691, R10@1: 0.376566, R10@2: 0.539144, R10@5: 0.817067, MRR: 0.494602\n"
     ]
    }
   ],
   "source": [
    "(acc, r2_1, r10_1, r10_2, r10_5, mrr), eva_loss, _ = evaluate(save_path, evaluate_file, output_path=result_path, eta=eta)\n",
    "print(\"Loss on test set: %f, R2@1: %f, R10@1: %f, R10@2: %f, R10@5: %f, MRR: %f\" % (eva_loss, r2_1, r10_1, r10_2, r10_5, mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba18e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_scope1/Variable:0\n",
      "some_scope1/Variable_1:0\n",
      "some_scope1/Variable_2:0\n",
      "some_scope1_1/Variable:0\n",
      "some_scope1_1/Variable_1:0\n",
      "some_scope1_1/Variable_2:0\n",
      "some_scope1_2/Variable:0\n",
      "some_scope1_2/Variable_1:0\n",
      "some_scope1_2/Variable_2:0\n",
      "some_scope1_3/a:0\n",
      "some_scope1_3/b:0\n",
      "some_scope1_3/c:0\n",
      "some_scope1_4/a:0\n",
      "some_scope1_4/b:0\n",
      "some_scope1_4/c:0\n",
      "some_scope1_5/a:0\n",
      "some_scope1_5/b:0\n",
      "some_scope1_5/c:0\n",
      "some_scope1_6/a:0\n",
      "some_scope1_6/b:0\n",
      "some_scope1_6/c:0\n",
      "\n",
      "some_scope1/Variable:0\n",
      "some_scope1/Variable_1:0\n",
      "some_scope1/Variable_2:0\n",
      "some_scope2/Variable:0\n",
      "some_scope2/Variable_1:0\n",
      "some_scope2/Variable_2:0\n",
      "Variable:0\n",
      "some_scope1_1/Variable:0\n",
      "some_scope1_1/Variable_1:0\n",
      "some_scope1_1/Variable_2:0\n",
      "some_scope2_1/Variable:0\n",
      "some_scope2_1/Variable_1:0\n",
      "some_scope2_1/Variable_2:0\n",
      "Variable_1:0\n",
      "some_scope1_2/Variable:0\n",
      "some_scope1_2/Variable_1:0\n",
      "some_scope1_2/Variable_2:0\n",
      "some_scope2_2/Variable:0\n",
      "some_scope2_2/Variable_1:0\n",
      "some_scope2_2/Variable_2:0\n",
      "Variable_2:0\n",
      "some_scope1_3/a:0\n",
      "some_scope1_3/b:0\n",
      "some_scope1_3/c:0\n",
      "some_scope2_3/Variable:0\n",
      "some_scope2_3/Variable_1:0\n",
      "some_scope2_3/Variable_2:0\n",
      "Variable_3:0\n",
      "some_scope1_4/a:0\n",
      "some_scope1_4/b:0\n",
      "some_scope1_4/c:0\n",
      "some_scope2_4/d:0\n",
      "some_scope2_4/e:0\n",
      "some_scope2_4/f:0\n",
      "h:0\n",
      "some_scope1_5/a:0\n",
      "some_scope1_5/b:0\n",
      "some_scope1_5/c:0\n",
      "some_scope2_5/d:0\n",
      "some_scope2_5/e:0\n",
      "some_scope2_5/f:0\n",
      "h_1:0\n",
      "some_scope1_6/a:0\n",
      "some_scope1_6/b:0\n",
      "some_scope1_6/c:0\n",
      "some_scope2_6/d:0\n",
      "some_scope2_6/e:0\n",
      "some_scope2_6/f:0\n",
      "h_2:0\n"
     ]
    }
   ],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "with tf.compat.v1.variable_scope('some_scope1'):\n",
    "    a = tf.Variable(1, name='a')\n",
    "    b = tf.Variable(2, name='b')\n",
    "    c = tf.Variable(3, name='c')\n",
    "\n",
    "with tf.compat.v1.variable_scope('some_scope2'):\n",
    "    d = tf.Variable(4, name='d')\n",
    "    e = tf.Variable(5, name='e')\n",
    "    f = tf.Variable(6, name='f')\n",
    "\n",
    "h = tf.Variable(8, name='h')\n",
    "\n",
    "for i in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='some_scope1'):\n",
    "#for i in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope='some_scope1'):\n",
    "    #print(i)   # i.name if you want just a name\n",
    "    print(i.name)   # i.name if you want just a name\n",
    "print()\n",
    "for v in tf.compat.v1.global_variables():\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fea0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1)\n",
    "y = 2 * x\n",
    "z = y + 1\n",
    "for v in tf.compat.v1.get_default_graph().as_graph_def().node:\n",
    "  print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdbc4935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.global_variables(\n",
    "    scope='some_scope1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87e2426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_scope1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.compat.v1.variable_scope(\"some_scope1\"):\n",
    "  print(tf.compat.v1.get_variable_scope().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9884602a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2ffbfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 13:51:52.458015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20583 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    learning_rate_ph = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "    print(tf.compat.v1.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b0eca49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39d04d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.variable_scope(\"foo\"):\n",
    "    with tf.compat.v1.variable_scope(\"bar\"):\n",
    "        v = tf.compat.v1.get_variable(\"v\", [1])\n",
    "        assert v.name == \"foo/bar/v:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bb71461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'foo/bar/v:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c9af454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'foo/bar/v:0' shape=(1,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(tf.compat.v1.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb50e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
