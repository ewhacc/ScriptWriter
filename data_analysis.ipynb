{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48a2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "    \n",
    "with open('data/dev.gr.pkl', 'rb') as f:\n",
    "    dev_gr = pickle.load(f)\n",
    "    \n",
    "with open('data/test.gr.pkl', 'rb') as f:\n",
    "    test_gr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc7ef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43514, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0288a750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37480, 11, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_gr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57770671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38320, 11, 50), (38320, 50), (38320, 50), (38320, 50), (38320,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gr[0].shape, test_gr[1].shape, test_gr[2].shape, test_gr[3].shape, test_gr[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c604a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합계 1288100\r\n",
      "-rw-rw-r-- 1 kotech kotech  50021962  7월 26 22:05 data.zip\r\n",
      "-rw-r--r-- 1 kotech kotech 210188173 10월 20  2021 dev.gr.pkl\r\n",
      "-rwxr-xr-x 1 kotech kotech   1353831  2월 19  2019 dev.pkl\r\n",
      "-rwxr-xr-x 1 kotech kotech  69622561  2월 14  2019 embeddings.pkl\r\n",
      "-rw-rw-r-- 1 kotech kotech        34  7월 26 22:06 readme.md\r\n",
      "-rw-rw-r-- 1 kotech kotech      1394  7월 26 22:06 sample_data.txt\r\n",
      "-rw-rw-r-- 1 kotech kotech       586  7월 26 22:06 sample_data_english.txt\r\n",
      "-rw-rw-r-- 1 kotech kotech       100  7월 26 22:06 sample_vocab.txt\r\n",
      "-rw-rw-r-- 1 kotech kotech        90  7월 26 22:06 sample_vocab_english.txt\r\n",
      "-rw-r--r-- 1 kotech kotech 214898893 10월 20  2021 test.gr.pkl\r\n",
      "-rwxr-xr-x 1 kotech kotech   1384520  2월 19  2019 test.pkl\r\n",
      "-rw-r--r-- 1 kotech kotech 765626935 10월 20  2021 train.gr.pkl\r\n",
      "-rwxr-xr-x 1 kotech kotech   5871713  2월 15  2019 train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0e9250d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7559/2105313877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def sample_fn():\n",
    "    return 0, 1, 2, 3, 4, 5, 6\n",
    "\n",
    "(acc, result) = sample_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b27fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "def multi_sequences_padding(all_sequences, max_sentence_len=50, max_num_utterance=10):\n",
    "    PAD_SEQUENCE = [0] * max_sentence_len\n",
    "    padded_sequences = []\n",
    "    sequences_length = []\n",
    "    for sequences in all_sequences:\n",
    "        sequences_len = len(sequences)\n",
    "        sequences_length.append(get_sequences_length(sequences, maxlen=max_sentence_len))\n",
    "        if sequences_len < max_num_utterance:\n",
    "            sequences += [PAD_SEQUENCE] * (max_num_utterance - sequences_len)\n",
    "            sequences_length[-1] += [0] * (max_num_utterance - sequences_len)\n",
    "        else:\n",
    "            sequences = sequences[-max_num_utterance:]\n",
    "            sequences_length[-1] = sequences_length[-1][-max_num_utterance:]\n",
    "        sequences = pad_sequences(sequences, padding='post', maxlen=max_sentence_len)\n",
    "        padded_sequences.append(sequences)\n",
    "    return padded_sequences, sequences_length\n",
    "\n",
    "\n",
    "def get_sequences_length(sequences, maxlen):\n",
    "    sequences_length = [min(len(sequence), maxlen) for sequence in sequences]\n",
    "    return sequences_length\n",
    "\n",
    "\n",
    "def generate_data_with_random_samples():\n",
    "    # generate negative samples randomly\n",
    "    # In training set, for each sample, we randomly sample a response as a negative candidate\n",
    "    # In development and test set, for each sample, we randomly sample 9 responses as negative candidates and we add a \"EOS\" response as a candidate to let model select when to stop\n",
    "    import random\n",
    "    import pickle\n",
    "    vocab = {}\n",
    "    positive_data = []\n",
    "    EOS_ID = 7\n",
    "    with open(\"./data/sample_vocab.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            vocab[line[0]] = idx + 1\n",
    "    with open(\"./data/sample_data.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "        tmp = []\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                line = line.split(\"\\t\")\n",
    "                if line[0] == \"narrative\":\n",
    "                    tmp.append(line[1])\n",
    "                elif line[0] == \"script\":\n",
    "                    tmp.append(line[1])\n",
    "            else:\n",
    "                narrative = tmp[0]\n",
    "                context = tmp[1:]\n",
    "                narrative_id = [vocab.get(word, 0) for word in narrative.split()]\n",
    "                context_id = [[vocab.get(word, 0) for word in sent.split()] for sent in context]\n",
    "                if len(narrative_id) == 0 or len(context_id) == 0:\n",
    "                    continue\n",
    "                data = [context_id, narrative_id, 1]\n",
    "                positive_data.append(data)\n",
    "                tmp = []\n",
    "        random.shuffle(positive_data)\n",
    "        print(\"all suitable sessions: \", len(positive_data))\n",
    "        train_num = int(len(positive_data) * 0.9)\n",
    "        dev_test_num = int(len(positive_data) * 0.05)\n",
    "        train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]\n",
    "        train_all, dev_all, test_all = [], [], []\n",
    "        for context_id, narrative_id, _ in train:\n",
    "            num_context = len(context_id)\n",
    "            for i in range(1, num_context):\n",
    "                context = context_id[:i]\n",
    "                response = context_id[i]\n",
    "                train_all.append([context, response, narrative_id, 1])\n",
    "                flag = True\n",
    "                while flag:\n",
    "                    random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                    random_context = positive_data[random_idx][0]\n",
    "                    random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                    random_response = random_context[random_idx_2]\n",
    "                    if len(response) != len(random_response):\n",
    "                        flag = False\n",
    "                        train_all.append([context, random_response, narrative_id, 0])\n",
    "                    else:\n",
    "                        for idx, wid in enumerate(response):\n",
    "                            if wid != random_response[idx]:\n",
    "                                flag = False\n",
    "                                train_all.append([context, random_response, narrative_id, 0])\n",
    "                                break\n",
    "        print(train_all[0], train_all[1])\n",
    "        for context_id, narrative_id, _ in dev:\n",
    "            num_context = len(context_id)\n",
    "            for i in range(1, num_context):\n",
    "                context = context_id[:i]\n",
    "                response = context_id[i]\n",
    "                dev_all.append([context, response, narrative_id, 1])\n",
    "                count = 0\n",
    "                negative_samples = []\n",
    "                while count < 9:\n",
    "                    random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                    random_context = positive_data[random_idx][0]\n",
    "                    random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                    random_response = random_context[random_idx_2]\n",
    "                    if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                        if len(response) != len(random_response):\n",
    "                            dev_all.append([context, random_response, narrative_id, 0])\n",
    "                            count += 1\n",
    "                            negative_samples.append(random_response)\n",
    "                        else:\n",
    "                            for idx, wid in enumerate(response):\n",
    "                                if wid != random_response[idx]:\n",
    "                                    dev_all.append([context, random_response, narrative_id, 0])\n",
    "                                    count += 1\n",
    "                                    negative_samples.append(random_response)\n",
    "                                    break\n",
    "                if response == [EOS_ID]:\n",
    "                    dev_all.append([context, [EOS_ID], narrative_id, 1])\n",
    "                else:\n",
    "                    dev_all.append([context, [EOS_ID], narrative_id, 0])\n",
    "        print(dev_all[0], dev_all[1], dev_all[2])\n",
    "        for context_id, narrative_id, _ in test:\n",
    "            num_context = len(context_id)\n",
    "            for i in range(1, num_context):\n",
    "                context = context_id[:i]\n",
    "                response = context_id[i]\n",
    "                test_all.append([context, response, narrative_id, 1])\n",
    "                count = 0\n",
    "                negative_samples = []\n",
    "                while count < 9:\n",
    "                    random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                    random_context = positive_data[random_idx][0]\n",
    "                    random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                    random_response = random_context[random_idx_2]\n",
    "                    if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                        if len(response) != len(random_response):\n",
    "                            test_all.append([context, random_response, narrative_id, 0])\n",
    "                            negative_samples.append(random_response)\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            for idx, id in enumerate(response):\n",
    "                                if id != random_response[idx]:\n",
    "                                    test_all.append([context, random_response, narrative_id, 0])\n",
    "                                    negative_samples.append(random_response)\n",
    "                                    count += 1\n",
    "                                    break\n",
    "                if response == [EOS_ID]:\n",
    "                    test_all.append([context, [EOS_ID], narrative_id, 1])\n",
    "                else:\n",
    "                    test_all.append([context, [EOS_ID], narrative_id, 0])\n",
    "        print(test_all[0], test_all[1], test_all[2])\n",
    "    context, response, narrative, label = [], [], [], []\n",
    "    print(\"train size: \", len(train_all))\n",
    "    for data in train_all:\n",
    "        context.append(data[0])\n",
    "        response.append(data[1])\n",
    "        narrative.append(data[2])\n",
    "        label.append(data[3])\n",
    "    train = [context, response, narrative, label]\n",
    "    pickle.dump(train, open(\"./data/train.multi.pkl\", \"wb\"))\n",
    "    context, response, narrative, label = [], [], [], []\n",
    "    print(\"dev size: \", len(dev_all))\n",
    "    for data in dev_all:\n",
    "        context.append(data[0])\n",
    "        response.append(data[1])\n",
    "        narrative.append(data[2])\n",
    "        label.append(data[3])\n",
    "    dev = [context, response, narrative, label]\n",
    "    pickle.dump(dev, open(\"./data/dev.multi.pkl\", \"wb\"))\n",
    "    context, response, narrative, label = [], [], [], []\n",
    "    print(\"test size: \", len(test_all))\n",
    "    for data in test_all:\n",
    "        context.append(data[0])\n",
    "        response.append(data[1])\n",
    "        narrative.append(data[2])\n",
    "        label.append(data[3])\n",
    "    test = [context, response, narrative, label]\n",
    "    pickle.dump(test, open(\"./data/test.multi.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc67f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['珍妮不喜欢回家。为了陪珍妮，甘决定晚点回家。珍妮是甘最好的朋友']\n",
      "[[[0], [0], [0], [0], [0], [7]], [0], 1]\n",
      "['珍妮不喜欢回家。为了陪珍妮，甘决定晚点回家。珍妮是甘最好的朋友']\n",
      "[[[0], [0], [0], [0], [0], [7]], [0], 1]\n",
      "['珍妮不喜欢回家。为了陪珍妮，甘决定晚点回家。珍妮是甘最好的朋友']\n",
      "[[[0], [0], [0], [0], [0], [7]], [0], 1]\n",
      "['珍妮不喜欢回家。为了陪珍妮，甘决定晚点回家。珍妮是甘最好的朋友']\n",
      "[[[0], [0], [0], [0], [0], [7]], [0], 1]\n",
      "['珍妮不喜欢回家。为了陪珍妮，甘决定晚点回家。珍妮是甘最好的朋友']\n",
      "[[[0], [0], [0], [0], [0], [7]], [0], 1]\n",
      "all suitable sessions:  5\n",
      "[[[0]], [0], [0], 1] [[[0]], [7], [0], 0]\n",
      "[[[0], [0]], [0], [0], 1] [[[0], [0]], [7], [0], 0]\n",
      "[[[0], [0], [0]], [0], [0], 1] [[[0], [0], [0]], [7], [0], 0]\n",
      "[[[0], [0], [0], [0]], [0], [0], 1] [[[0], [0], [0], [0]], [7], [0], 0]\n",
      "[[[0], [0], [0], [0], [0]], [7], [0], 1] [[[0], [0], [0], [0], [0]], [0], [0], 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_all[\u001b[38;5;241m6\u001b[39m], train_all[\u001b[38;5;241m7\u001b[39m])\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_all[\u001b[38;5;241m8\u001b[39m], train_all[\u001b[38;5;241m9\u001b[39m])\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msdf\u001b[49m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context_id, narrative_id, _ \u001b[38;5;129;01min\u001b[39;00m dev:\n\u001b[1;32m     71\u001b[0m     num_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(context_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdf' is not defined"
     ]
    }
   ],
   "source": [
    "# generate negative samples randomly\n",
    "# In training set, for each sample, we randomly sample a response as a negative candidate\n",
    "# In development and test set, for each sample, we randomly sample 9 responses as negative candidates and we add a \"EOS\" response as a candidate to let model select when to stop\n",
    "import random\n",
    "import pickle\n",
    "vocab = {}\n",
    "positive_data = []\n",
    "EOS_ID = 7\n",
    "with open(\"./data/sample_vocab.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "    for idx, line in enumerate(fr):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        vocab[line[0]] = idx + 1\n",
    "with open(\"./data/sample_data.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "    tmp = []\n",
    "    for line in fr:\n",
    "        line = line.strip()\n",
    "        if len(line) > 0:\n",
    "            line = line.split(\"\\t\")\n",
    "            if line[0] == \"narrative\":\n",
    "                tmp.append(line[1])\n",
    "            elif line[0] == \"script\":\n",
    "                tmp.append(line[1])\n",
    "        else: # empty line\n",
    "            narrative = tmp[0]\n",
    "            context = tmp[1:]\n",
    "            narrative_id = [vocab.get(word, 0) for word in narrative.split()]\n",
    "            context_id = [[vocab.get(word, 0) for word in sent.split()] for sent in context]\n",
    "            if len(narrative_id) == 0 or len(context_id) == 0:\n",
    "                continue\n",
    "            data = [context_id, narrative_id, 1]\n",
    "            positive_data.append(data)\n",
    "            tmp = []\n",
    "            print(narrative.split())\n",
    "            print(data)\n",
    "    random.shuffle(positive_data)\n",
    "    print(\"all suitable sessions: \", len(positive_data))\n",
    "    #train_num = int(len(positive_data) * 0.9)\n",
    "    #dev_test_num = int(len(positive_data) * 0.05)\n",
    "    train_num = 3\n",
    "    dev_test_num = 1\n",
    "    train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]\n",
    "    train_all, dev_all, test_all = [], [], []\n",
    "    for context_id, narrative_id, _ in train:\n",
    "        num_context = len(context_id)\n",
    "        for i in range(1, num_context):\n",
    "            context = context_id[:i]\n",
    "            response = context_id[i]\n",
    "            train_all.append([context, response, narrative_id, 1])\n",
    "            flag = True\n",
    "            while flag:\n",
    "                random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                random_context = positive_data[random_idx][0]\n",
    "                random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                random_response = random_context[random_idx_2]\n",
    "                if len(response) != len(random_response):\n",
    "                    flag = False\n",
    "                    train_all.append([context, random_response, narrative_id, 0])\n",
    "                else:\n",
    "                    for idx, wid in enumerate(response):\n",
    "                        if wid != random_response[idx]:\n",
    "                            flag = False\n",
    "                            train_all.append([context, random_response, narrative_id, 0])\n",
    "                            break\n",
    "    print(train_all[0], train_all[1])\n",
    "    print(train_all[2], train_all[3])\n",
    "    print(train_all[4], train_all[5])\n",
    "    print(train_all[6], train_all[7])\n",
    "    print(train_all[8], train_all[9])\n",
    "    print(sdf)\n",
    "    for context_id, narrative_id, _ in dev:\n",
    "        num_context = len(context_id)\n",
    "        for i in range(1, num_context):\n",
    "            context = context_id[:i]\n",
    "            response = context_id[i]\n",
    "            dev_all.append([context, response, narrative_id, 1])\n",
    "            count = 0\n",
    "            negative_samples = []\n",
    "            while count < 9:\n",
    "                random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                random_context = positive_data[random_idx][0]\n",
    "                random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                random_response = random_context[random_idx_2]\n",
    "                if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                    if len(response) != len(random_response):\n",
    "                        dev_all.append([context, random_response, narrative_id, 0])\n",
    "                        count += 1\n",
    "                        negative_samples.append(random_response)\n",
    "                    else:\n",
    "                        for idx, wid in enumerate(response):\n",
    "                            if wid != random_response[idx]:\n",
    "                                dev_all.append([context, random_response, narrative_id, 0])\n",
    "                                count += 1\n",
    "                                negative_samples.append(random_response)\n",
    "                                break\n",
    "            if response == [EOS_ID]:\n",
    "                dev_all.append([context, [EOS_ID], narrative_id, 1])\n",
    "            else:\n",
    "                dev_all.append([context, [EOS_ID], narrative_id, 0])\n",
    "    print(dev_all[0], dev_all[1], dev_all[2])\n",
    "    for context_id, narrative_id, _ in test:\n",
    "        num_context = len(context_id)\n",
    "        for i in range(1, num_context):\n",
    "            context = context_id[:i]\n",
    "            response = context_id[i]\n",
    "            test_all.append([context, response, narrative_id, 1])\n",
    "            count = 0\n",
    "            negative_samples = []\n",
    "            while count < 9:\n",
    "                random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                random_context = positive_data[random_idx][0]\n",
    "                random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                random_response = random_context[random_idx_2]\n",
    "                if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                    if len(response) != len(random_response):\n",
    "                        test_all.append([context, random_response, narrative_id, 0])\n",
    "                        negative_samples.append(random_response)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        for idx, id in enumerate(response):\n",
    "                            if id != random_response[idx]:\n",
    "                                test_all.append([context, random_response, narrative_id, 0])\n",
    "                                negative_samples.append(random_response)\n",
    "                                count += 1\n",
    "                                break\n",
    "            if response == [EOS_ID]:\n",
    "                test_all.append([context, [EOS_ID], narrative_id, 1])\n",
    "            else:\n",
    "                test_all.append([context, [EOS_ID], narrative_id, 0])\n",
    "    print(test_all[0], test_all[1], test_all[2])\n",
    "context, response, narrative, label = [], [], [], []\n",
    "print(\"train size: \", len(train_all))\n",
    "for data in train_all:\n",
    "    context.append(data[0])\n",
    "    response.append(data[1])\n",
    "    narrative.append(data[2])\n",
    "    label.append(data[3])\n",
    "train = [context, response, narrative, label]\n",
    "pickle.dump(train, open(\"./data/train.multi.pkl\", \"wb\"))\n",
    "context, response, narrative, label = [], [], [], []\n",
    "print(\"dev size: \", len(dev_all))\n",
    "for data in dev_all:\n",
    "    context.append(data[0])\n",
    "    response.append(data[1])\n",
    "    narrative.append(data[2])\n",
    "    label.append(data[3])\n",
    "dev = [context, response, narrative, label]\n",
    "pickle.dump(dev, open(\"./data/dev.multi.pkl\", \"wb\"))\n",
    "context, response, narrative, label = [], [], [], []\n",
    "print(\"test size: \", len(test_all))\n",
    "for data in test_all:\n",
    "    context.append(data[0])\n",
    "    response.append(data[1])\n",
    "    narrative.append(data[2])\n",
    "    label.append(data[3])\n",
    "test = [context, response, narrative, label]\n",
    "pickle.dump(test, open(\"./data/test.multi.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2450eb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c21c1cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[0], [0], [0], [0], [0], [7]], [0], 1],\n",
       " [[[0], [0], [0], [0], [0], [7]], [0], 1],\n",
       " [[[0], [0], [0], [0], [0], [7]], [0], 1],\n",
       " [[[0], [0], [0], [0], [0], [7]], [0], 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "304f4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = int(len(positive_data) * 0.9)\n",
    "dev_test_num = int(len(positive_data) * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53b6d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b036ed67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6d51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
