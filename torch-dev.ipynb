{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751fb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f189fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b1b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScriptWriter_cpre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "023f562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0511,  0.5404, -0.8509,  0.4603, -1.9017,  1.5315, -0.0478, -1.3589,\n",
       "          0.5049,  0.8584, -0.4572, -0.0399, -0.8955, -0.9557, -0.9209, -1.0791,\n",
       "          2.2756, -0.3729,  0.5335,  1.4092, -0.1845, -0.8261, -0.9601,  1.4598,\n",
       "         -0.4093,  0.6887,  0.1956,  0.3117, -0.0178, -0.6985, -1.1898,  0.1680,\n",
       "         -0.4340, -1.6522,  1.0798, -0.3336,  0.4201,  1.4249, -0.3786,  1.5399,\n",
       "          0.2179,  0.0048,  0.8186,  1.5922, -0.6720,  0.2043, -0.0201,  0.5689,\n",
       "         -1.6773, -1.1306,  0.2200,  0.4936,  1.1892, -0.9849, -0.3791, -1.4667,\n",
       "         -0.3478, -1.9947,  0.5119, -0.3157,  0.5153, -0.3999, -1.5718,  0.7975,\n",
       "         -1.3458,  0.8821,  0.4253,  1.1042, -0.7413,  0.9077, -0.0536, -0.9522,\n",
       "          0.7220, -0.4399, -0.0276,  0.7771, -0.7920,  0.8184,  0.6510, -1.0325,\n",
       "         -0.7193, -0.7454,  0.3566, -0.0286, -1.0471, -0.9070,  0.7654, -0.2098,\n",
       "         -0.9704, -0.1319,  0.3694,  0.0906,  0.7484, -0.0094,  0.8626, -0.0944,\n",
       "          0.8216,  0.1229,  0.9069,  0.8423, -1.2777, -0.9049,  1.8120, -0.1554,\n",
       "         -2.1961,  0.0644,  0.2928, -0.2877, -2.0144, -1.2234,  0.0613, -0.5652,\n",
       "         -0.6891,  0.4098,  0.2451, -1.7109, -1.7825,  0.6250,  0.3644,  0.2283,\n",
       "         -1.1163, -0.6302,  1.3100,  0.1462, -0.7234, -0.0471,  0.0443, -0.8500,\n",
       "         -0.1488,  0.0137,  0.5324, -1.1618,  1.1304, -1.5275, -0.5577, -0.0618,\n",
       "          1.1680,  0.1812,  0.4972, -1.1083,  0.1380, -1.1409, -0.6451,  0.2810,\n",
       "          0.6931, -0.3198, -1.0485,  0.2355,  0.8182, -0.4066,  0.4713,  0.9527,\n",
       "          0.7825,  0.3126,  0.4541,  0.2537,  1.5099, -1.1368, -0.4891, -0.5611,\n",
       "         -0.3726,  0.6173, -0.6700, -0.4966, -0.6927, -1.1267,  0.0403,  0.8528,\n",
       "         -0.9557,  0.2097,  0.3390,  0.3291,  1.1931,  0.6987, -0.4986,  1.3186,\n",
       "         -0.7244,  0.8644, -0.4144,  0.7104,  0.4731, -0.4118,  0.8976,  0.2393,\n",
       "         -1.0552,  0.2232, -0.4224,  1.0398, -0.1263,  1.5898,  0.1813,  0.8207,\n",
       "         -0.4181, -0.9813,  1.1851,  1.7201, -0.2436, -0.0551, -0.2800, -1.2752]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding(torch.tensor([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1)) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "   \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "         \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f27586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, keys, num_units=None, num_heads=8, dropout_rate=0, is_training=True, causality=False, scope=\"multihead_attention\", reuse=None):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        Q = tf.compat.v1.layers.dense(queries, num_units, activation=tf.compat.v1.nn.relu)  # (N, T_q, C)\n",
    "        K = tf.compat.v1.layers.dense(keys, num_units, activation=tf.compat.v1.nn.relu)  # (N, T_k, C)\n",
    "        V = tf.compat.v1.layers.dense(keys, num_units, activation=tf.compat.v1.nn.relu)  # (N, T_k, C)\n",
    "\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, C/h)\n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)\n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)\n",
    "\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n",
    "        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n",
    "            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n",
    "        outputs = tf.compat.v1.nn.softmax(outputs)  # (h*N, T_q, T_k)\n",
    "        attn_weights = outputs\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks  # broadcasting. (N, T_q, C)\n",
    "        outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)\n",
    "        outputs += queries\n",
    "        outputs = normalize(outputs, scope=scope)  # (N, T_q, C)\n",
    "\n",
    "    return outputs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0170c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import *\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class layer_normalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features, epsilon=1e-8):\n",
    "        '''Applies layer normalization.\n",
    "        Args:\n",
    "          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "        '''\n",
    "        super(layer_normalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
    "    \n",
    "class multihead_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, num_heads=8, dropout_rate=0, causality=False):\n",
    "        '''Applies multihead attention.\n",
    "        Args:\n",
    "            num_units: A scalar. Attention size.\n",
    "            dropout_rate: A floating point number.\n",
    "            causality: Boolean. If true, units that reference the future are masked.\n",
    "            num_heads: An int. Number of heads.\n",
    "        '''\n",
    "        super(multihead_attention, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.causality = causality\n",
    "        self.Q_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.K_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.V_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "\n",
    "        self.output_dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        self.normalization = layer_normalization(self.num_units)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # keys, values: same shape of [N, T_k, C_k]\n",
    "        # queries: A 3d Variable with shape of [N, T_q, C_q]\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.Q_proj(queries)  # (N, T_q, C)\n",
    "        K = self.K_proj(keys)  # (N, T_q, C)\n",
    "        V = self.V_proj(values)  # (N, T_q, C)\n",
    "\n",
    "        # Split and concat\n",
    "        Q_ = torch.cat(torch.chunk(Q, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        K_ = torch.cat(torch.chunk(K, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        V_ = torch.cat(torch.chunk(V, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "\n",
    "        # Multiplication\n",
    "        outputs = torch.bmm(Q_, K_.permute(0, 2, 1))  # (h*N, T_q, T_k)\n",
    "\n",
    "        # Scale\n",
    "        outputs = outputs / (K_.size()[-1] ** 0.5)\n",
    "\n",
    "        # Key Masking\n",
    "        key_masks = torch.sign(torch.abs(torch.sum(keys, dim=-1)))  # (N, T_k)\n",
    "        key_masks = key_masks.repeat(self.num_heads, 1)  # (h*N, T_k)\n",
    "        key_masks = torch.unsqueeze(key_masks, 1).repeat(1, queries.size()[1], 1)  # (h*N, T_q, T_k)\n",
    "\n",
    "        padding = Variable(torch.ones(*outputs.size()).cuda() * (-2 ** 32 + 1))\n",
    "        condition = key_masks.eq(0.).float()\n",
    "        outputs = padding * condition + outputs * (1. - condition)\n",
    "\n",
    "        # Causality = Future blinding\n",
    "        if self.causality:\n",
    "            diag_vals = torch.ones(*outputs[0, :, :].size()).cuda()  # (T_q, T_k)\n",
    "            tril = torch.tril(diag_vals, diagonal=0)  # (T_q, T_k)\n",
    "            # print(tril)\n",
    "            masks = Variable(torch.unsqueeze(tril, 0).repeat(outputs.size()[0], 1, 1))  # (h*N, T_q, T_k)\n",
    "\n",
    "            padding = Variable(torch.ones(*masks.size()).cuda() * (-2 ** 32 + 1))\n",
    "            condition = masks.eq(0.).float()\n",
    "            outputs = padding * condition + outputs * (1. - condition)\n",
    "\n",
    "        # Activation\n",
    "        outputs = F.softmax(outputs, dim=-1)  # (h*N, T_q, T_k)\n",
    "\n",
    "        # Query Masking\n",
    "        query_masks = torch.sign(torch.abs(torch.sum(queries, dim=-1)))  # (N, T_q)\n",
    "        query_masks = query_masks.repeat(self.num_heads, 1)  # (h*N, T_q)\n",
    "        query_masks = torch.unsqueeze(query_masks, 2).repeat(1, 1, keys.size()[1])  # (h*N, T_q, T_k)\n",
    "        outputs = outputs * query_masks\n",
    "\n",
    "        # Dropouts\n",
    "        outputs = self.output_dropout(outputs)  # (h*N, T_q, T_k)\n",
    "\n",
    "        # Weighted sum\n",
    "        outputs = torch.bmm(outputs, V_)  # (h*N, T_q, C/h)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = torch.cat(torch.chunk(outputs, self.num_heads, dim=0), dim=2)  # (N, T_q, C)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "\n",
    "        # Normalize\n",
    "        outputs = self.normalization(outputs)  # (N, T_q, C)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class feedforward(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_units=[2048, 512]):\n",
    "        '''Point-wise feed forward net.\n",
    "        Args:\n",
    "          in_channels: a number of channels of inputs\n",
    "          num_units: A list of two integers.\n",
    "        '''\n",
    "        super(feedforward, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_units = num_units\n",
    "\n",
    "        # nn.Linear is faster than nn.Conv1d\n",
    "        self.conv = False\n",
    "        if self.conv:\n",
    "            params = {'in_channels': self.in_channels, 'out_channels': self.num_units[0],\n",
    "                      'kernel_size': 1, 'stride': 1, 'bias': True}\n",
    "            self.conv1 = nn.Sequential(nn.Conv1d(**params), nn.ReLU())\n",
    "            params = {'in_channels': self.num_units[0], 'out_channels': self.num_units[1],\n",
    "                      'kernel_size': 1, 'stride': 1, 'bias': True}\n",
    "            self.conv2 = nn.Conv1d(**params)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Linear(self.in_channels, self.num_units[0]), nn.ReLU())\n",
    "            self.conv2 = nn.Linear(self.num_units[0], self.num_units[1])\n",
    "        self.normalization = layer_normalization(self.in_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.conv:\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "\n",
    "        # Layer normalization\n",
    "        if self.conv:\n",
    "            outputs = self.normalization(outputs.permute(0, 2, 1))\n",
    "        else:\n",
    "            outputs = self.normalization(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttModel(nn.Module):\n",
    "    def __init__(self, hp_, enc_voc, dec_voc):\n",
    "        '''Attention is all you nedd. https://arxiv.org/abs/1706.03762\n",
    "        Args:\n",
    "            hp: Hyper Parameters\n",
    "            enc_voc: vocabulary size of encoder language\n",
    "            dec_voc: vacabulary size of decoder language\n",
    "        '''\n",
    "        super(AttModel, self).__init__()\n",
    "        self.hp = hp_\n",
    "\n",
    "        self.enc_voc = enc_voc\n",
    "        self.dec_voc = dec_voc\n",
    "\n",
    "        #encoder\n",
    "        self.enc_emb = embedding(self.enc_voc, self.hp.hidden_units, scale=True)\n",
    "\n",
    "        if self.hp.sinusoid:\n",
    "            self.enc_positional_encoding = positional_encoding(num_units=self.hp.hidden_units,\n",
    "                                                               zeros_pad=False,\n",
    "                                                               scale=False)\n",
    "        else:\n",
    "            self.enc_positional_encoding = embedding(self.hp.maxlen, self.hp.hidden_units, zeros_pad=False, scale=False)\n",
    "        self.enc_dropout = nn.Dropout(self.hp.dropout_rate)\n",
    "        for i in range(self.hp.num_blocks):\n",
    "            self.__setattr__('enc_self_attention_%d' % i, multihead_attention(num_units=self.hp.hidden_units,\n",
    "                                                                              num_heads=self.hp.num_heads,\n",
    "                                                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                                                              causality=False))\n",
    "            self.__setattr__('enc_feed_forward_%d' % i, feedforward(self.hp.hidden_units,\n",
    "                                                                    [4 * self.hp.hidden_units,\n",
    "                                                                     self.hp.hidden_units]))\n",
    "\n",
    "        #decoder\n",
    "        self.dec_emb = embedding(self.dec_voc, self.hp.hidden_units, scale=True)\n",
    "        if self.hp.sinusoid:\n",
    "            self.dec_positional_encoding = positional_encoding(num_units=self.hp.hidden_units,\n",
    "                                                               zeros_pad=False,\n",
    "                                                               scale=False)\n",
    "        else:\n",
    "            self.dec_positional_encoding = embedding(self.hp.maxlen, self.hp.hidden_units, zeros_pad=False, scale=False)\n",
    "\n",
    "        self.dec_dropout = nn.Dropout(self.hp.dropout_rate)\n",
    "        for i in range(self.hp.num_blocks):\n",
    "            self.__setattr__('dec_self_attention_%d' % i,\n",
    "                             multihead_attention(num_units=self.hp.hidden_units,\n",
    "                                                 num_heads=self.hp.num_heads,\n",
    "                                                 dropout_rate=self.hp.dropout_rate,\n",
    "                                                 causality=True))\n",
    "            self.__setattr__('dec_vanilla_attention_%d' % i,\n",
    "                             multihead_attention(num_units=self.hp.hidden_units,\n",
    "                                                 num_heads=self.hp.num_heads,\n",
    "                                                 dropout_rate=self.hp.dropout_rate,\n",
    "                                                 causality=False))\n",
    "            self.__setattr__('dec_feed_forward_%d' % i, feedforward(self.hp.hidden_units,\n",
    "                                                                    [4 * self.hp.hidden_units,\n",
    "                                                                     self.hp.hidden_units]))\n",
    "        self.logits_layer = nn.Linear(self.hp.hidden_units, self.dec_voc)\n",
    "        self.label_smoothing = label_smoothing()\n",
    "        #self.losslayer = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        #define decoder inputs\n",
    "        self.decoder_inputs = torch.cat([Variable(torch.ones(y[:, :1].size()).cuda() * 2).long(), y[:, :-1]], dim=-1)  # 2:<S>\n",
    "\n",
    "        #Encoder\n",
    "        self.enc = self.enc_emb(x)\n",
    "        #Positional Encoding\n",
    "        if self.hp.sinusoid:\n",
    "            self.enc += self.enc_positional_encoding(x)\n",
    "        else:\n",
    "            self.enc += self.enc_positional_encoding(\n",
    "                Variable(torch.unsqueeze(torch.arange(0, x.size()[1]), 0).repeat(x.size(0), 1).long().cuda()))\n",
    "        self.enc = self.enc_dropout(self.enc)\n",
    "        #Blocks\n",
    "        for i in range(self.hp.num_blocks):\n",
    "            self.enc = self.__getattr__('enc_self_attention_%d' % i)(self.enc, self.enc, self.enc)\n",
    "            #Feed Forward\n",
    "            self.enc = self.__getattr__('enc_feed_forward_%d' % i)(self.enc)\n",
    "        #Decoder\n",
    "        self.dec = self.dec_emb(self.decoder_inputs)\n",
    "        #Positional Encoding\n",
    "        if self.hp.sinusoid:\n",
    "            self.dec += self.dec_positional_encoding(self.decoder_inputs)\n",
    "        else:\n",
    "            self.dec += self.dec_positional_encoding(\n",
    "                Variable(torch.unsqueeze(torch.arange(0, self.decoder_inputs.size()[1]), 0).repeat(self.decoder_inputs.size(0), 1).long().cuda()))\n",
    "\n",
    "        #Dropout\n",
    "        self.dec = self.dec_dropout(self.dec)\n",
    "        #Blocks\n",
    "        for i in range(self.hp.num_blocks):\n",
    "            #self-attention\n",
    "            self.dec = self.__getattr__('dec_self_attention_%d' % i)(self.dec, self.dec, self.dec)\n",
    "            #vanilla attention\n",
    "            self.dec = self.__getattr__('dec_vanilla_attention_%d' % i)(self.dec, self.enc, self.enc)\n",
    "            #feed forward\n",
    "            self.dec = self.__getattr__('dec_feed_forward_%d' % i)(self.dec)\n",
    "\n",
    "        #Final linear projection\n",
    "        self.logits = self.logits_layer(self.dec)\n",
    "        self.probs = F.softmax(self.logits, dim=-1).view(-1, self.dec_voc)\n",
    "        _, self.preds = torch.max(self.logits, -1)\n",
    "        self.istarget = (1. - y.eq(0.).float()).view(-1)\n",
    "        self.acc = torch.sum(self.preds.eq(y).float().view(-1) * self.istarget) / torch.sum(self.istarget)\n",
    "\n",
    "        #Loss\n",
    "        self.y_onehot = torch.zeros(self.logits.size()[0] * self.logits.size()[1], self.dec_voc).cuda()\n",
    "        self.y_onehot = Variable(self.y_onehot.scatter_(1, y.view(-1, 1).data, 1))\n",
    "\n",
    "        self.y_smoothed = self.label_smoothing(self.y_onehot)\n",
    "\n",
    "        #self.loss = self.losslayer(self.probs, self.y_smoothed)\n",
    "        self.loss = - torch.sum(self.y_smoothed * torch.log(self.probs), dim=-1)\n",
    "        #print(self.loss)\n",
    "\n",
    "        self.mean_loss = torch.sum(self.loss * self.istarget) / torch.sum(self.istarget)\n",
    "\n",
    "        return self.mean_loss, self.preds, self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deeab974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class ScriptWriter_cpre(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta=0.5,\n",
    "        max_sentence_len = 50,\n",
    "        max_num_utterance = 11,\n",
    "        embedding_file = 'data/embeddings_ko.pkl',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_num_utterance = max_num_utterance\n",
    "        self.negative_samples = 1\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        self.emb_size = 200 # word_embedding_size \n",
    "        self.hidden_units = 200\n",
    "        #self.total_words = 43514\n",
    "        self.total_words = 11883\n",
    "        #self.batch_size = batch_size\n",
    "        #self.eval_batch_size = eval_batch_size\n",
    "        #self.learning_rate_ph = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "        self.dropout_rate = 0\n",
    "        self.num_heads = 1\n",
    "        self.num_blocks = 3 \n",
    "        self.eta = eta\n",
    "        #self.gamma = tf.compat.v1.get_variable('gamma', shape=1, dtype=tf.float32, trainable=True, initializer=tf.constant_initializer(0.5))\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
    "        word_emb = pickle.load(open(embedding_file, 'rb'), encoding=\"bytes\")\n",
    "        word_emb = torch.FloatTensor(word_emb)\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_emb, freeze=True)\n",
    "        \n",
    "        for i in range(self.num_blocks):\n",
    "            self.__setattr__('self_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('self_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            \n",
    "        for i in range(self.num_blocks+1):\n",
    "            self.__setattr__('ru_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('ru_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('ur_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('ur_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('nu_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('nu_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('un_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('un_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('nr_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('nr_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('rn_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('rn_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "                                       \n",
    "                                       \n",
    "        self.n_dense = nn.Linear(self.hidden_units, self.hidden_units)\n",
    "        self.lastu_dense = nn.Linear(self.max_sentence_len, 1) # check Y\n",
    "        self.lastur_dense = nn.Linear(self.max_sentence_len, 1) # check Y\n",
    "        \n",
    "        conv3d_1_layer = nn.Conv3d((self.num_blocks+1)*2, 32, 3, padding='same') # check input channel Y\n",
    "        nn.init.uniform_(conv3d_1_layer.weight, -0.01, 0.01) \n",
    "        self.conv3d_1 = torch.nn.Sequential(conv3d_1_layer, torch.nn.ELU())\n",
    "        \n",
    "        conv3d_2_layer = nn.Conv3d(32, 32, 3, padding='same') \n",
    "        nn.init.uniform_(conv3d_2_layer.weight, -0.01, 0.01)\n",
    "        self.conv3d_2 = torch.nn.Sequential(conv3d_2_layer, torch.nn.ELU())\n",
    "        \n",
    "        self.maxpooling3d = torch.nn.Maxpooling3D\n",
    "        \n",
    "        conv2d_1_layer = nn.Conv2d((self.num_blocks+1)*2, 32, 3, padding='same') # check input channel Y\n",
    "        nn.init.uniform_(conv2d_1_layer.weight, -0.01, 0.01)\n",
    "        self.conv2d_1 = torch.nn.Sequential(conv2d_1_layer, torch.nn.ELU())\n",
    "        \n",
    "        conv2d_2_layer = nn.Conv2d(32, 32, 3, padding='same') \n",
    "        nn.init.uniform_(conv2d_2_layer.weight, -0.01, 0.01)\n",
    "        self.conv2d_2 = torch.nn.Sequential(conv2d_2_layer, torch.nn.ELU())\n",
    "        \n",
    "        logits_dense = nn.Linear(self.hidden_units, 1)  # check input channel\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.embedding_ph = tf.compat.v1.placeholder(tf.float32, shape=(self.total_words, self.word_embedding_size))\n",
    "        self.utterance_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_num_utterance, max_sentence_len))\n",
    "        self.response_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_sentence_len))\n",
    "        self.gt_response_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_sentence_len))\n",
    "        self.y_true_ph = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
    "        self.narrative_ph = tf.compat.v1.placeholder(tf.int32, shape=(None, max_sentence_len))\n",
    "\n",
    "        self.word_embeddings = tf.compat.v1.get_variable('word_embeddings_v', shape=(self.total_words, self.word_embedding_size), dtype=tf.float32, trainable=False)\n",
    "        self.embedding_init = self.word_embeddings.assign(self.embedding_ph)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.is_training = True\n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        response: Optional[torch.Tensor] = None,\n",
    "        gt_response: Optional[torch.Tensor] = None,\n",
    "        narrative: Optional[torch.Tensor] = None,\n",
    "        utterance: Optional[torch.Tensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        all_utterances = torch.unbind(utterance, dim=1)\n",
    "        print(all_utterances[0].shape)\n",
    "        \n",
    "        response_embeddings = self.embedding(response)\n",
    "        Hr_stack = [response_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            response_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                response_embeddings, response_embeddings, response_embeddings)\n",
    "            response_embeddings = self.__getattr__('self_feedforward_%d' % i)(response_embeddings)\n",
    "            Hr_stack.append(response_embeddings)\n",
    "        \n",
    "        gt_response_embeddings = self.embedding(gt_response)\n",
    "        Hgtr_stack = [gt_response_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            gt_response_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                gt_response_embeddings, gt_response_embeddings, gt_response_embeddings)\n",
    "            gt_response_embeddings = self.__getattr__('self_feedforward_%d' % i)(gt_response_embeddings)\n",
    "            Hgtr_stack.append(response_embeddings)\n",
    "            \n",
    "        narrative_embeddings = self.embedding(narrative)\n",
    "        Hn_stack = [narrative_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            narrative_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                narrative_embeddings, narrative_embeddings, narrative_embeddings)\n",
    "            narrative_embeddings = self.__getattr__('self_feedforward_%d' % i)(narrative_embeddings)\n",
    "            Hn_stack.append(response_embeddings)\n",
    "        \n",
    "        Mur, Mun = [], []\n",
    "        self.decay_factor = []\n",
    "        last_u_reps = []\n",
    "        turn_id = 0\n",
    "        \n",
    "        for utterance in all_utterances:\n",
    "            utterance_embeddings = self.embedding(utterance)\n",
    "            Hu_stack = [utterance_embeddings]\n",
    "            for i in range(self.num_blocks):\n",
    "                utterance_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                    utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
    "                utterance_embeddings = self.__getattr__('self_feedforward_%d' % i)(utterance_embeddings)\n",
    "                Hu_stack.append(utterance_embeddings)\n",
    "                \n",
    "            if turn_id == self.max_num_utterance - 1:\n",
    "                last_u_reps = Hu_stack\n",
    "            \n",
    "            r_a_u_stack = []\n",
    "            u_a_r_stack = []\n",
    "            for i in range(self.num_blocks + 1):\n",
    "                r_a_u = self.__getattr__('ru_multihead_attention_%d' % i)(\n",
    "                    Hr_stack[i], Hu_stack[i], Hu_stack[i])\n",
    "                r_a_u = self.__getattr__('ru_feedforward_%d' % i)(r_a_u)\n",
    "                r_a_u_stack.append(r_a_u)\n",
    "                u_a_r = self.__getattr__('ur_multihead_attention_%d' % i)(\n",
    "                    Hu_stack[i], Hr_stack[i], Hr_stack[i])\n",
    "                u_a_r = self.__getattr__('ur_feedforward_%d' % i)(u_a_r)\n",
    "                u_a_r_stack.append(u_a_r)\n",
    "            r_a_u_stack.extend(Hr_stack)\n",
    "            u_a_r_stack.extend(Hu_stack)\n",
    "            \n",
    "            n_a_u_stack = []\n",
    "            u_a_n_stack = []\n",
    "            for i in range(self.num_blocks + 1):\n",
    "                n_a_u = self.__getattr__('nu_multihead_attention_%d' % i)(\n",
    "                    Hn_stack[i], Hu_stack[i], Hu_stack[i])\n",
    "                n_a_u = self.__getattr__('nu_feedforward_%d' % i)(n_a_u)\n",
    "                n_a_u_stack.append(n_a_u)\n",
    "                u_a_n = self.__getattr__('un_multihead_attention_%d' % i)(\n",
    "                    Hu_stack[i], Hn_stack[i], Hn_stack[i])\n",
    "                u_a_n = self.__getattr__('un_feedforward_%d' % i)(u_a_n)\n",
    "                u_a_n_stack.append(u_a_n)\n",
    "            n_a_u_stack.extend(Hn_stack)\n",
    "            u_a_n_stack.extend(Hu_stack)\n",
    "            \n",
    "            r_a_u = torch.stack(r_a_u_stack, dim=-1)\n",
    "            u_a_r = torch.stack(u_a_r_stack, dim=-1)\n",
    "            n_a_u = torch.stack(n_a_u_stack, dim=-1)\n",
    "            u_a_n = torch.stack(u_a_n_stack, dim=-1)\n",
    "            \n",
    "            # sim shape [batch, max_sent_len, max_sent_len, 2 * (stack_num + 1)]\n",
    "            # divide sqrt(200) to prevent gradient explosion\n",
    "            # (-1, 50, 50, 8)\n",
    "            sim_ur = torch.einsum('biks,bjks->bijs', u_a_r, r_a_u) / torch.sqrt(torch.tensor(200.0))  # for no rp and normal\n",
    "            sim_un = torch.einsum('biks,bjks->bijs', u_a_n, n_a_u) / torch.sqrt(torch.tensor(200.0))  # for no rp and normal\n",
    "            \n",
    "            self_n = torch.nn.functional.normalize(torch.stack(Hn_stack, dim=-1))  # for no rp\n",
    "            self_u = torch.nn.functional.normalize(torch.stack(Hu_stack, dim=-1))  # for no rp\n",
    "            Hn_stack_tensor = torch.stack(Hn_stack, dim=-1)  # [batch, o_len, embedding_size, stack]\n",
    "            \n",
    "            self_sim = torch.einsum('biks,bjks->bijs', self_u, self_n)  # [batch, u_len, o_len, stack]\n",
    "            self_sim = 1 - self.gamma * torch.sum(self_sim, dim=1)  # [batch, (1), o_len, stack]\n",
    "            Hn_stack = torch.einsum('bjkl,bjl->bjkl', Hn_stack_tensor, self_sim)\n",
    "            Hn_stack = torch.unbind(Hn_stack, dim=-1)\n",
    "            \n",
    "            Mur.append(sim_ur)\n",
    "            Mun.append(sim_un)\n",
    "            turn_id += 1\n",
    "            \n",
    "        # Hn_stack ( (-1,50,200), ... ) len = block_num\n",
    "        print('narrative updated final len(Hn_stack) =', len(Hn_stack), ', Hn_stack[0].shape =', Hn_stack[0].shape)\n",
    "        \n",
    "        print('stack shape = ', torch.stack(Hn_stack, dim=2).shape)\n",
    "        Hn_stack_for_tracking = self.n_dense(torch.stack(Hn_stack, dim=2))  # [batch, o_len, stack, embedding_size]\n",
    "        print('Hn_stack_for_tracking.shape after dense =', Hn_stack_for_tracking.shape)\n",
    "        Hn_stack_for_tracking = Hn_stack_for_tracking.permute((0, 1, 3, 2))  # [batch, o_len, embedding_size, stack]\n",
    "        print('Hn_stack_for_tracking.shape after permute =', Hn_stack_for_tracking.shape)\n",
    "        Hlastu_stack_for_tracking = torch.stack(last_u_reps, dim=-1)  # [batch, u_len, embedding_size, stack]\n",
    "        Hr_stack_for_tracking = torch.stack(Hgtr_stack, dim=-1)  # [batch, r_len, embedding_size, stack]\n",
    "        Hlastu = Hlastu_stack_for_tracking.permute((0, 2, 3, 1)) # [batch, embedding_size, stack, u_len]\n",
    "        Hlastu = torch.squeeze(self.lastu_dense(Hlastu), dim=-1)  # [batch, embedding_size, stack]\n",
    "        p1_tensor = nn.functional.softmax(torch.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastu), dim=1)  # [batch, o_len, stack]\n",
    "        Hlastur = Hr_stack_for_tracking.permute((0, 2, 3, 1))\n",
    "        Hlastur = torch.squeeze(self.lastur_dense(Hlastur), dim=-1)  # [batch, embedding_size, stack]\n",
    "        p2_tensor = nn.functional.softmax(torch.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastur), dim=1)  # [batch, o_len, stack]\n",
    "        p1 = torch.unbind(p1_tensor, dim=-1)\n",
    "        p2 = torch.unbind(p2_tensor, dim=-1)\n",
    "        print('len(p1) =', len(p1), ', p1[0].shape =', p1[0].shape)\n",
    "        \n",
    "        n_a_r_stack = []\n",
    "        r_a_n_stack = []\n",
    "        for i in range(self.num_blocks + 1):\n",
    "            n_a_r = self.__getattr__('nr_multihead_attention_%d' % i)(\n",
    "                Hn_stack[i], Hr_stack[i], Hr_stack[i])\n",
    "            n_a_r = self.__getattr__('nr_feedforward_%d' % i)(n_a_r)\n",
    "            n_a_r_stack.append(n_a_r)\n",
    "            r_a_n = self.__getattr__('rn_multihead_attention_%d' % i)(\n",
    "                Hr_stack[i], Hn_stack[i], Hn_stack[i])\n",
    "            r_a_n = self.__getattr__('rn_feedforward_%d' % i)(r_a_n)\n",
    "            r_a_n_stack.append(r_a_n)\n",
    "        n_a_r_stack.extend(Hn_stack)\n",
    "        r_a_n_stack.extend(Hr_stack)\n",
    "\n",
    "        n_a_r = torch.stack(n_a_r_stack, dim=-1)\n",
    "        r_a_n = torch.stack(r_a_n_stack, dim=-1)\n",
    "\n",
    "        Mrn = torch.einsum('biks,bjks->bijs', n_a_r, r_a_n) / torch.sqrt(torch.tensor(200.0))\n",
    "        self.rosim = Mrn\n",
    "        Mur = torch.stack(Mur, dim=1)\n",
    "        Mun = torch.stack(Mun, dim=1)\n",
    "        \n",
    "        print('Mur.shape =', Mur.shape)\n",
    "        print('Mun.shape =', Mun.shape)\n",
    "        \n",
    "        conv3d = self.conv3d_1(Mur)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00109cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotech/venv-cpre/lib/python3.9/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Maxpooling3D'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m narrative \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11400\u001b[39m, size\u001b[38;5;241m=\u001b[39m(N, \u001b[38;5;241m50\u001b[39m)))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m utterance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11400\u001b[39m, size\u001b[38;5;241m=\u001b[39m(N, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m50\u001b[39m)))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mScriptWriter_cpre\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn [3], line 105\u001b[0m, in \u001b[0;36mScriptWriter_cpre.__init__\u001b[0;34m(self, eta, max_sentence_len, max_num_utterance, embedding_file)\u001b[0m\n\u001b[1;32m    102\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39muniform_(conv3d_2_layer\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3d_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(conv3d_2_layer, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mELU())\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpooling3d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxpooling3D\u001b[49m\n\u001b[1;32m    107\u001b[0m conv2d_1_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_blocks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# check input channel Y\u001b[39;00m\n\u001b[1;32m    108\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39muniform_(conv2d_1_layer\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Maxpooling3D'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N = 1\n",
    "device = torch.device(\"cuda\")\n",
    "response = torch.from_numpy(np.random.randint(1, 11400, size=(N, 50))).to(device)\n",
    "gt_response = torch.from_numpy(np.random.randint(1, 11400, size=(N, 50))).to(device)\n",
    "narrative = torch.from_numpy(np.random.randint(1, 11400, size=(N, 50))).to(device)\n",
    "utterance = torch.from_numpy(np.random.randint(1, 11400, size=(N, 11, 50))).to(device)\n",
    "\n",
    "model = ScriptWriter_cpre().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7de4cf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m(response\u001b[38;5;241m=\u001b[39mresponse, gt_response\u001b[38;5;241m=\u001b[39mgt_response, narrative\u001b[38;5;241m=\u001b[39mnarrative, utterance\u001b[38;5;241m=\u001b[39mutterance)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model(response=response, gt_response=gt_response, narrative=narrative, utterance=utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b4321c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                   1.8.2+cu111\r\n",
      "torchaudio              0.8.2\r\n",
      "torchvision             0.9.2+cu111\r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1e58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
