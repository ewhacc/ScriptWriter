{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3524f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af67c2e",
   "metadata": {},
   "source": [
    "**GPU 사용 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44160198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "#%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13247f",
   "metadata": {},
   "source": [
    "**Dataset 지정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254e3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_name = \"original\"  # originial dataset in the paper\n",
    "#dataset_name = \"ko\"        # helper dataset\n",
    "dataset_name = \"1cycle\"     # 1cycle dataset\n",
    "\n",
    "if dataset_name == \"original\":\n",
    "    EMBEDDING_FILE = \"data/embeddings.pkl\"\n",
    "else:\n",
    "    EMBEDDING_FILE = f\"data/embeddings_{dataset_name}.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d8b42",
   "metadata": {},
   "source": [
    "**Random seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be19227c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f40a43429d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "#torch.use_deterministic_algorithms(True)\n",
    "#torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24afd15",
   "metadata": {},
   "source": [
    "**Model 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0170c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import *\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import pickle\n",
    "from typing import Optional\n",
    "\n",
    "class layer_normalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features, epsilon=1e-8):\n",
    "    #def __init__(self, features, epsilon=1e-12):\n",
    "        '''Applies layer normalization.\n",
    "        Args:\n",
    "          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "        '''\n",
    "        super(layer_normalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        #self.gamma = nn.Parameter(torch.ones(features))\n",
    "        #self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.layernorm = nn.LayerNorm(features, eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #mean = x.mean(-1, keepdim=True)\n",
    "        #std = x.std(-1, keepdim=True)\n",
    "        #return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
    "        #var = x.var(-1, keepdim=True)\n",
    "        #return self.gamma * (x - mean) / torch.sqrt(var + self.epsilon) + self.beta\n",
    "        return self.layernorm(x)\n",
    "    \n",
    "class multihead_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, num_heads=8, dropout_rate=0, causality=False):\n",
    "        '''Applies multihead attention.\n",
    "        Args:\n",
    "            num_units: A scalar. Attention size.\n",
    "            dropout_rate: A floating point number.\n",
    "            causality: Boolean. If true, units that reference the future are masked.\n",
    "            num_heads: An int. Number of heads.\n",
    "        '''\n",
    "        super(multihead_attention, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.causality = causality\n",
    "        self.Q_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.K_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        self.V_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n",
    "        \n",
    "        # tensorflow compatible initializer\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        self.Q_proj.apply(init_weights)\n",
    "        self.K_proj.apply(init_weights)\n",
    "        self.V_proj.apply(init_weights)\n",
    "\n",
    "        self.output_dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        self.normalization = layer_normalization(self.num_units)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # keys, values: same shape of [N, T_k, C_k]\n",
    "        # queries: A 3d Variable with shape of [N, T_q, C_q]\n",
    "\n",
    "        #print('q shape:', queries.shape)\n",
    "        # Linear projections\n",
    "        Q = self.Q_proj(queries)  # (N, T_q, C)\n",
    "        #print('Q shape:', Q.shape)\n",
    "        K = self.K_proj(keys)  # (N, T_q, C)\n",
    "        V = self.V_proj(values)  # (N, T_q, C)\n",
    "\n",
    "        # Split and concat\n",
    "        Q_ = torch.cat(torch.chunk(Q, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        K_ = torch.cat(torch.chunk(K, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "        V_ = torch.cat(torch.chunk(V, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n",
    "\n",
    "        # Multiplication\n",
    "        outputs = torch.bmm(Q_, K_.permute(0, 2, 1))  # (h*N, T_q, T_k)\n",
    "\n",
    "        # Scale\n",
    "        outputs = outputs / (K_.size()[-1] ** 0.5)\n",
    "\n",
    "        # Key Masking\n",
    "        key_masks = torch.sign(torch.abs(torch.sum(keys, dim=-1)))  # (N, T_k)\n",
    "        key_masks = key_masks.repeat(self.num_heads, 1)  # (h*N, T_k)\n",
    "        key_masks = torch.unsqueeze(key_masks, 1).repeat(1, queries.size()[1], 1)  # (h*N, T_q, T_k)\n",
    "\n",
    "        padding = Variable(torch.ones(*outputs.size()).cuda() * (-2 ** 32 + 1))\n",
    "        condition = key_masks.eq(0.).float()\n",
    "        outputs = padding * condition + outputs * (1. - condition)\n",
    "\n",
    "        # Causality = Future blinding\n",
    "        if self.causality:\n",
    "            diag_vals = torch.ones(*outputs[0, :, :].size()).cuda()  # (T_q, T_k)\n",
    "            tril = torch.tril(diag_vals, diagonal=0)  # (T_q, T_k)\n",
    "            # print(tril)\n",
    "            masks = Variable(torch.unsqueeze(tril, 0).repeat(outputs.size()[0], 1, 1))  # (h*N, T_q, T_k)\n",
    "\n",
    "            padding = Variable(torch.ones(*masks.size()).cuda() * (-2 ** 32 + 1))\n",
    "            condition = masks.eq(0.).float()\n",
    "            outputs = padding * condition + outputs * (1. - condition)\n",
    "\n",
    "        # Activation\n",
    "        outputs = F.softmax(outputs, dim=-1)  # (h*N, T_q, T_k)\n",
    "\n",
    "        # Query Masking\n",
    "        query_masks = torch.sign(torch.abs(torch.sum(queries, dim=-1)))  # (N, T_q)\n",
    "        query_masks = query_masks.repeat(self.num_heads, 1)  # (h*N, T_q)\n",
    "        query_masks = torch.unsqueeze(query_masks, 2).repeat(1, 1, keys.size()[1])  # (h*N, T_q, T_k)\n",
    "        outputs = outputs * query_masks\n",
    "\n",
    "        # Dropouts\n",
    "        outputs = self.output_dropout(outputs)  # (h*N, T_q, T_k)\n",
    "\n",
    "        # Weighted sum\n",
    "        outputs = torch.bmm(outputs, V_)  # (h*N, T_q, C/h)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = torch.cat(torch.chunk(outputs, self.num_heads, dim=0), dim=2)  # (N, T_q, C)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "\n",
    "        # Normalize\n",
    "        #outputs = self.normalization(outputs)  # (N, T_q, C)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class feedforward(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_units=[2048, 512]):\n",
    "        '''Point-wise feed forward net.\n",
    "        Args:\n",
    "          in_channels: a number of channels of inputs\n",
    "          num_units: A list of two integers.\n",
    "        '''\n",
    "        super(feedforward, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_units = num_units\n",
    "        \n",
    "        # nn.Linear is faster than nn.Conv1d\n",
    "        self.conv = False\n",
    "        if self.conv:\n",
    "            params = {'in_channels': self.in_channels, 'out_channels': self.num_units[0],\n",
    "                      'kernel_size': 1, 'stride': 1, 'bias': True}\n",
    "            self.conv1 = nn.Sequential(nn.Conv1d(**params), nn.ReLU())\n",
    "            params = {'in_channels': self.num_units[0], 'out_channels': self.num_units[1],\n",
    "                      'kernel_size': 1, 'stride': 1, 'bias': True}\n",
    "            self.conv2 = nn.Conv1d(**params)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Linear(self.in_channels, self.num_units[0]), nn.ReLU())\n",
    "            self.conv2 = nn.Linear(self.num_units[0], self.num_units[1])\n",
    "            \n",
    "        # tensorflow compatible initializer\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        self.conv1.apply(init_weights)\n",
    "        self.conv2.apply(init_weights)\n",
    "        \n",
    "        self.normalization = layer_normalization(self.in_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.conv:\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "\n",
    "        # Layer normalization\n",
    "        if self.conv:\n",
    "            outputs = self.normalization(outputs.permute(0, 2, 1))\n",
    "        else:\n",
    "            outputs = self.normalization(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class ScriptWriter_cpre(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        eta=0.7,\n",
    "        max_sentence_len = 50,\n",
    "        max_num_utterance = 11,\n",
    "        embedding_file = EMBEDDING_FILE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_num_utterance = max_num_utterance\n",
    "        self.negative_samples = 1\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        self.hidden_units = 200 #word embedding size\n",
    "        #self.total_words = 43514\n",
    "        #self.total_words = 11883\n",
    "        self.dropout_rate = 0\n",
    "        self.num_heads = 1\n",
    "        self.num_blocks = 3 \n",
    "        self.eta = eta\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
    "        word_emb = pickle.load(open(embedding_file, 'rb'), encoding=\"bytes\")\n",
    "        word_emb = torch.FloatTensor(word_emb)\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_emb, freeze=True)\n",
    "        \n",
    "        for i in range(self.num_blocks):\n",
    "            self.__setattr__('self_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('self_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            \n",
    "        for i in range(self.num_blocks+1):\n",
    "            self.__setattr__('ru_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('ru_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('ur_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('ur_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('nu_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('nu_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('un_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('un_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('nr_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('nr_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "            self.__setattr__('rn_multihead_attention_%d' % i, multihead_attention(\n",
    "                     num_units=self.hidden_units,\n",
    "                     num_heads=self.num_heads,\n",
    "                     dropout_rate=self.dropout_rate,\n",
    "                     causality=False))\n",
    "            self.__setattr__('rn_feedforward_%d' % i, feedforward(\n",
    "                     self.hidden_units,\n",
    "                     [self.hidden_units, self.hidden_units]))\n",
    "                                       \n",
    "                                       \n",
    "        self.n_dense = nn.Linear(self.hidden_units, self.hidden_units)\n",
    "        torch.nn.init.xavier_uniform_(self.n_dense.weight)\n",
    "        torch.nn.init.zeros_(self.n_dense.bias)\n",
    "        self.lastu_dense = nn.Linear(self.max_sentence_len, 1) \n",
    "        torch.nn.init.xavier_uniform_(self.lastu_dense.weight)\n",
    "        torch.nn.init.zeros_(self.lastu_dense.bias)\n",
    "        self.lastur_dense = nn.Linear(self.max_sentence_len, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.lastur_dense.weight)\n",
    "        torch.nn.init.zeros_(self.lastur_dense.bias)\n",
    "        \n",
    "        depth = self.max_num_utterance # 11\n",
    "        height = self.max_sentence_len # 50\n",
    "        width = self.max_sentence_len # 50\n",
    "        padding = ((depth%3 + 1)//2, (height%3 + 1)//2, (width%3 + 1)//2,)\n",
    "        conv3d_1_layer = nn.Conv3d((self.num_blocks+1)*2, 32, 3, padding='same') \n",
    "        nn.init.uniform_(conv3d_1_layer.weight, -0.01, 0.01) \n",
    "        nn.init.zeros_(conv3d_1_layer.bias)\n",
    "        self.conv3d_1 = torch.nn.Sequential(conv3d_1_layer, torch.nn.ELU())\n",
    "        self.maxpool3d_1 = torch.nn.MaxPool3d(3, padding=padding)\n",
    "        \n",
    "        depth = (self.max_num_utterance+2)//3 # 11\n",
    "        height = (self.max_sentence_len+2)//3 # 50\n",
    "        width = (self.max_sentence_len+2)//3 # 50\n",
    "        padding = ((depth%3 + 1)//2, (height%3 + 1)//2, (width%3 + 1)//2,)\n",
    "        conv3d_2_layer = nn.Conv3d(32, 32, 3, padding='same') \n",
    "        nn.init.uniform_(conv3d_2_layer.weight, -0.01, 0.01)\n",
    "        nn.init.zeros_(conv3d_2_layer.bias)\n",
    "        self.conv3d_2 = torch.nn.Sequential(conv3d_2_layer, torch.nn.ELU())\n",
    "        self.maxpool3d_2 = torch.nn.MaxPool3d(3, padding=padding)\n",
    "        mur_flatten_size = ((depth+2)//3)*((height+2)//3)*((width+2)//3)*32\n",
    "        #print('mur_flatten_size =', mur_flatten_size)\n",
    "        \n",
    "        height = self.max_sentence_len # 50\n",
    "        width = self.max_sentence_len # 50\n",
    "        padding = ((height%3 + 1)//2, (width%3 + 1)//2)\n",
    "        conv2d_1_layer = nn.Conv2d((self.num_blocks+1)*2, 32, 3, padding='same')\n",
    "        nn.init.uniform_(conv2d_1_layer.weight, -0.01, 0.01)\n",
    "        nn.init.zeros_(conv2d_1_layer.bias)\n",
    "        self.conv2d_1 = torch.nn.Sequential(conv2d_1_layer, torch.nn.ELU())\n",
    "        self.maxpool2d_1 = torch.nn.MaxPool2d(3, padding=padding)\n",
    "        \n",
    "        height = (self.max_sentence_len+2)//3 # 50\n",
    "        width = (self.max_sentence_len+2)//3 # 50\n",
    "        padding = ((height%3 + 1)//2, (width%3 + 1)//2)\n",
    "        conv2d_2_layer = nn.Conv2d(32, 32, 3, padding='same') \n",
    "        nn.init.uniform_(conv2d_2_layer.weight, -0.01, 0.01)\n",
    "        nn.init.zeros_(conv2d_2_layer.bias)\n",
    "        self.conv2d_2 = torch.nn.Sequential(conv2d_2_layer, torch.nn.ELU())\n",
    "        self.maxpool2d_2 = torch.nn.MaxPool2d(3, padding=padding)\n",
    "        \n",
    "        total_flatten_size = mur_flatten_size*2 + ((height+2)//3)*((width+2)//3)*32\n",
    "        #print('total_flatten_size =', total_flatten_size)\n",
    "        \n",
    "        self.logits_dense = nn.Linear(total_flatten_size, 1)  \n",
    "        nn.init.orthogonal_(self.logits_dense.weight)\n",
    "        nn.init.zeros_(self.logits_dense.bias)\n",
    "        \n",
    "        self.bcewithlogitsloss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        idx: Optional[torch.Tensor] = None,\n",
    "        response: Optional[torch.Tensor] = None,\n",
    "        gt_response: Optional[torch.Tensor] = None,\n",
    "        narrative: Optional[torch.Tensor] = None,\n",
    "        utterance: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        #print('response.shape =', response.shape)\n",
    "        #print('response type =', type(response))\n",
    "        #print('utterance.shape =', utterance.shape)\n",
    "        all_utterances = torch.unbind(utterance, dim=1)\n",
    "        #print(all_utterances[0].shape)\n",
    "        \n",
    "        response_embeddings = self.embedding(response)\n",
    "        Hr_stack = [response_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            response_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                response_embeddings, response_embeddings, response_embeddings)\n",
    "            response_embeddings = self.__getattr__('self_feedforward_%d' % i)(response_embeddings)\n",
    "            Hr_stack.append(response_embeddings)\n",
    "            \n",
    "        \n",
    "        #for i in range(self.num_blocks+1):\n",
    "        #    print('Hr_stack[%d] ='%i, Hr_stack[i])\n",
    "        \n",
    "        gt_response_embeddings = self.embedding(gt_response)\n",
    "        Hgtr_stack = [gt_response_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            gt_response_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                gt_response_embeddings, gt_response_embeddings, gt_response_embeddings)\n",
    "            gt_response_embeddings = self.__getattr__('self_feedforward_%d' % i)(gt_response_embeddings)\n",
    "            Hgtr_stack.append(gt_response_embeddings)\n",
    "            \n",
    "        narrative_embeddings = self.embedding(narrative)\n",
    "        Hn_stack = [narrative_embeddings]\n",
    "        for i in range(self.num_blocks):\n",
    "            narrative_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                narrative_embeddings, narrative_embeddings, narrative_embeddings)\n",
    "            narrative_embeddings = self.__getattr__('self_feedforward_%d' % i)(narrative_embeddings)\n",
    "            Hn_stack.append(narrative_embeddings)\n",
    "            \n",
    "        #for i in range(self.num_blocks+1):\n",
    "        #    print('Hn_stack[%d] ='%i, Hn_stack[i])\n",
    "        \n",
    "        Mur, Mun = [], []\n",
    "        self.decay_factor = []\n",
    "        last_u_reps = []\n",
    "        turn_id = 0\n",
    "        \n",
    "        for utterance in all_utterances:\n",
    "            utterance_embeddings = self.embedding(utterance)\n",
    "            Hu_stack = [utterance_embeddings]\n",
    "            for i in range(self.num_blocks):\n",
    "                utterance_embeddings = self.__getattr__('self_multihead_attention_%d' % i)(\n",
    "                    utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
    "                utterance_embeddings = self.__getattr__('self_feedforward_%d' % i)(utterance_embeddings)\n",
    "                Hu_stack.append(utterance_embeddings)\n",
    "                \n",
    "            if turn_id == self.max_num_utterance - 1:\n",
    "                last_u_reps = Hu_stack\n",
    "            \n",
    "            r_a_u_stack = []\n",
    "            u_a_r_stack = []\n",
    "            for i in range(self.num_blocks + 1):\n",
    "                r_a_u = self.__getattr__('ru_multihead_attention_%d' % i)(\n",
    "                    Hr_stack[i], Hu_stack[i], Hu_stack[i])\n",
    "                r_a_u = self.__getattr__('ru_feedforward_%d' % i)(r_a_u)\n",
    "                r_a_u_stack.append(r_a_u)\n",
    "                u_a_r = self.__getattr__('ur_multihead_attention_%d' % i)(\n",
    "                    Hu_stack[i], Hr_stack[i], Hr_stack[i])\n",
    "                u_a_r = self.__getattr__('ur_feedforward_%d' % i)(u_a_r)\n",
    "                u_a_r_stack.append(u_a_r)\n",
    "            r_a_u_stack.extend(Hr_stack)\n",
    "            u_a_r_stack.extend(Hu_stack)\n",
    "            \n",
    "            n_a_u_stack = []\n",
    "            u_a_n_stack = []\n",
    "            for i in range(self.num_blocks + 1):\n",
    "                n_a_u = self.__getattr__('nu_multihead_attention_%d' % i)(\n",
    "                    Hn_stack[i], Hu_stack[i], Hu_stack[i])\n",
    "                n_a_u = self.__getattr__('nu_feedforward_%d' % i)(n_a_u)\n",
    "                n_a_u_stack.append(n_a_u)\n",
    "                u_a_n = self.__getattr__('un_multihead_attention_%d' % i)(\n",
    "                    Hu_stack[i], Hn_stack[i], Hn_stack[i])\n",
    "                u_a_n = self.__getattr__('un_feedforward_%d' % i)(u_a_n)\n",
    "                u_a_n_stack.append(u_a_n)\n",
    "            n_a_u_stack.extend(Hn_stack)\n",
    "            u_a_n_stack.extend(Hu_stack)\n",
    "            \n",
    "            r_a_u = torch.stack(r_a_u_stack, dim=-1)\n",
    "            u_a_r = torch.stack(u_a_r_stack, dim=-1)\n",
    "            n_a_u = torch.stack(n_a_u_stack, dim=-1)\n",
    "            u_a_n = torch.stack(u_a_n_stack, dim=-1)\n",
    "            \n",
    "            # sim shape [batch, max_sent_len, max_sent_len, 2 * (stack_num + 1)]\n",
    "            # divide sqrt(200) to prevent gradient explosion\n",
    "            # (-1, 50, 50, 8)\n",
    "            sim_ur = torch.einsum('biks,bjks->bijs', u_a_r, r_a_u) / torch.sqrt(torch.tensor(200.0))  # for no rp and normal\n",
    "            sim_un = torch.einsum('biks,bjks->bijs', u_a_n, n_a_u) / torch.sqrt(torch.tensor(200.0))  # for no rp and normal\n",
    "            \n",
    "            self_n = torch.nn.functional.normalize(torch.stack(Hn_stack, dim=-1), p=2, dim=None)  # for no rp\n",
    "            self_u = torch.nn.functional.normalize(torch.stack(Hu_stack, dim=-1), p=2, dim=None)  # for no rp\n",
    "            Hn_stack_tensor = torch.stack(Hn_stack, dim=-1)  # [batch, o_len, embedding_size, stack]\n",
    "            #print('Hn_stack_tensor =', Hn_stack_tensor)\n",
    "            #print('self_n =', self_n)\n",
    "            #print('self_u =', self_u)\n",
    "            \n",
    "            self_sim = torch.einsum('biks,bjks->bijs', self_u, self_n)  # [batch, u_len, o_len, stack]\n",
    "            #print('self_sim0 = ', self_sim)\n",
    "            self_sim = 1 - self.gamma * torch.sum(self_sim, dim=1)  # [batch, (1), o_len, stack]\n",
    "            #print('self_sim = ', self_sim)\n",
    "            Hn_stack = torch.einsum('bjkl,bjl->bjkl', Hn_stack_tensor, self_sim)\n",
    "            Hn_stack = torch.unbind(Hn_stack, dim=-1)\n",
    "            #for i in range(4):\n",
    "            #    print('Hn_stack[%d] ='%i, Hn_stack[i])\n",
    "            \n",
    "            Mur.append(sim_ur)\n",
    "            Mun.append(sim_un)\n",
    "            turn_id += 1\n",
    "            \n",
    "        # Hn_stack ( (-1,50,200), ... ) len = block_num\n",
    "        #print('narrative updated final len(Hn_stack) =', len(Hn_stack), ', Hn_stack[0].shape =', Hn_stack[0].shape)\n",
    "        \n",
    "        #print('stack shape = ', torch.stack(Hn_stack, dim=2).shape)\n",
    "        Hn_stack_for_tracking = self.n_dense(torch.stack(Hn_stack, dim=2))  # [batch, o_len, stack, embedding_size]\n",
    "        #print('torch.stack(Hn_stack, dim=2) =', torch.stack(Hn_stack, dim=2))\n",
    "        #print('Hn_stack_for_tracking.shape after dense =', Hn_stack_for_tracking.shape)\n",
    "        Hn_stack_for_tracking = Hn_stack_for_tracking.permute((0, 1, 3, 2))  # [batch, o_len, embedding_size, stack]\n",
    "        #print('Hn_stack_for_tracking.shape after permute =', Hn_stack_for_tracking.shape)\n",
    "        Hlastu_stack_for_tracking = torch.stack(last_u_reps, dim=-1)  # [batch, u_len, embedding_size, stack]\n",
    "        Hr_stack_for_tracking = torch.stack(Hgtr_stack, dim=-1)  # [batch, r_len, embedding_size, stack]\n",
    "        Hlastu = Hlastu_stack_for_tracking.permute((0, 2, 3, 1)) # [batch, embedding_size, stack, u_len]\n",
    "        Hlastu = torch.squeeze(self.lastu_dense(Hlastu), dim=-1)  # [batch, embedding_size, stack]\n",
    "        p1_tensor = nn.functional.softmax(torch.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastu), dim=1)  # [batch, o_len, stack]\n",
    "        #print('Hlastu =', Hlastu)\n",
    "        Hlastur = Hr_stack_for_tracking.permute((0, 2, 3, 1))\n",
    "        Hlastur = torch.squeeze(self.lastur_dense(Hlastur), dim=-1)  # [batch, embedding_size, stack]\n",
    "        p2_tensor = nn.functional.softmax(torch.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastur), dim=1)  # [batch, o_len, stack]\n",
    "        #print('Hn_stack_for_tracking =', Hn_stack_for_tracking)\n",
    "        #print('Hlastur =', Hlastur)\n",
    "        #print('einsum for p2 =', torch.einsum('bnds,bds->bns', Hn_stack_for_tracking, Hlastur))\n",
    "        p1 = torch.unbind(p1_tensor, dim=-1)\n",
    "        p2 = torch.unbind(p2_tensor, dim=-1)\n",
    "        #print('len(p1) =', len(p1), ', p1[0].shape =', p1[0].shape)\n",
    "        #print('forward p2 =', p2)\n",
    "        \n",
    "        n_a_r_stack = []\n",
    "        r_a_n_stack = []\n",
    "        for i in range(self.num_blocks + 1):\n",
    "            #print('Hn_stack[%d] ='%i, Hn_stack[i])\n",
    "            n_a_r = self.__getattr__('nr_multihead_attention_%d' % i)(\n",
    "                Hn_stack[i], Hr_stack[i], Hr_stack[i])\n",
    "            n_a_r = self.__getattr__('nr_feedforward_%d' % i)(n_a_r)\n",
    "            n_a_r_stack.append(n_a_r)\n",
    "            r_a_n = self.__getattr__('rn_multihead_attention_%d' % i)(\n",
    "                Hr_stack[i], Hn_stack[i], Hn_stack[i])\n",
    "            r_a_n = self.__getattr__('rn_feedforward_%d' % i)(r_a_n)\n",
    "            r_a_n_stack.append(r_a_n)\n",
    "        n_a_r_stack.extend(Hn_stack)\n",
    "        r_a_n_stack.extend(Hr_stack)\n",
    "\n",
    "        n_a_r = torch.stack(n_a_r_stack, dim=-1)\n",
    "        r_a_n = torch.stack(r_a_n_stack, dim=-1)\n",
    "        \n",
    "        #print('n_a_r =', n_a_r)\n",
    "        #print('r_a_n =', r_a_n)\n",
    "\n",
    "        Mrn = torch.einsum('biks,bjks->bijs', n_a_r, r_a_n) / torch.sqrt(torch.tensor(200.0))\n",
    "        self.rosim = Mrn\n",
    "        Mur = torch.stack(Mur, dim=1)\n",
    "        Mun = torch.stack(Mun, dim=1)\n",
    "        \n",
    "        #print('Mrn.shape =', Mrn.shape)\n",
    "        #print('Mrn =', Mrn)\n",
    "        \n",
    "        conv3d = self.conv3d_1(Mur.permute(0,4,1,2,3)) # (-1, 11, 50, 50, 8) -> (-1, 8, 11, 50, 50)\n",
    "        #print('conv3d.shape =', conv3d.shape)\n",
    "        pool3d = self.maxpool3d_1(conv3d)              # (-1, 32, 4, 17, 17)\n",
    "        #print('pool3d.shape =', pool3d.shape)\n",
    "        conv3d2 = self.conv3d_2(pool3d)\n",
    "        pool3d2 = self.maxpool3d_2(conv3d2)            # (-1, 32, 2, 6, 6)\n",
    "        #print('pool3d2.shape =', pool3d2.shape)\n",
    "        mur = torch.flatten(pool3d2.permute(0,2,3,4,1), start_dim=1)\n",
    "        #print('mur.shape =', mur.shape)\n",
    "        \n",
    "        conv3d = self.conv3d_1(Mun.permute(0,4,1,2,3))\n",
    "        pool3d = self.maxpool3d_1(conv3d)\n",
    "        conv3d2 = self.conv3d_2(pool3d)\n",
    "        pool3d2 = self.maxpool3d_2(conv3d2)\n",
    "        mun = torch.flatten(pool3d2.permute(0,2,3,4,1), start_dim=1)\n",
    "        #print('mun.shape =', mun.shape)\n",
    "        \n",
    "        conv2d = self.conv2d_1(Mrn.permute((0,3,1,2)))\n",
    "        pool2d = self.maxpool2d_1(conv2d)\n",
    "        conv2d2 = self.conv2d_2(pool2d)\n",
    "        pool2d2 = self.maxpool2d_2(conv2d2)\n",
    "        mrn = torch.flatten(pool2d2.permute(0,2,3,1), start_dim=1) \n",
    "        #print('mrn.shape =', mrn.shape)\n",
    "        #print('mrn=', mrn)\n",
    "        \n",
    "        #print('p1[0] =', p1[0])\n",
    "        #print('p2[0] =', p2[0])\n",
    "        #KL_loss = torch.tensor(0.0)\n",
    "        eps = 1e-7\n",
    "        #print(num_blocks)\n",
    "        KL_loss = 0.0\n",
    "        for i in range(self.num_blocks + 1):\n",
    "            #print('p1[%d].shape ='%i, p1[i])\n",
    "            #print('p2[%d].shape ='%i, p2[i])\n",
    "            KL_loss += torch.mean(nn.functional.kl_div((p2[i]+eps).log(), p1[i], reduction='batchmean'))\n",
    "            #KL_loss += torch.mean(self.kl_loss((p2[i]+eps).log(), p1[i]))\n",
    "            #KL_loss += torch.mean(nn.functional.kl_div(p2_log[i], p1[i], reduction='batchmean'))\n",
    "            #print('KL:', i, torch.mean(nn.functional.kl_div(p2[i].log(), p1[i], reduction='batchmean')))\n",
    "        KL_loss /= (self.num_blocks + 1)\n",
    "        #print('KL =', KL_loss)\n",
    "        #print(logits)\n",
    "        #print(y_true)\n",
    "        #print('###')\n",
    "        #print(self.bcewithlogitsloss(logits, y_true))\n",
    "        \n",
    "        all_vector = torch.cat([mur, mun, mrn], dim=-1)\n",
    "        #print('all_vector.shape =', all_vector.shape)\n",
    "        #print('all_vector =', all_vector)\n",
    "        logits = torch.reshape(self.logits_dense(all_vector), shape=(-1,))\n",
    "        y_pred = torch.sigmoid(logits)\n",
    "        #print('y_pred.shape =', y_pred.shape)\n",
    "        \n",
    "        y_true = labels\n",
    "        #RS_loss = torch.mean(self.bcewithlogitsloss(logits, y_true))\n",
    "        RS_loss = torch.mean(torch.clip(self.bcewithlogitsloss(logits, y_true), -10, 10))\n",
    "        #print('RS =', RS_loss)\n",
    "        loss = self.eta * RS_loss + (1 - self.eta) * KL_loss\n",
    "        #print('loss =', loss)\n",
    "        loss = torch.unsqueeze(loss, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_pred': y_pred,\n",
    "            #'logits': logits,\n",
    "            #'KL_loss': KL_loss\n",
    "            #'p1': p1,\n",
    "            #'p2': p2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5526cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScriptWriter_cpre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14888f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "from typing import Dict, Union, Any\n",
    "from transformers.utils import  is_apex_available, is_sagemaker_dp_enabled, is_sagemaker_mp_enabled\n",
    "\n",
    "class ScriptwriterTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.bcewithlogitsloss = nn.BCEWithLogitsLoss()\n",
    "        self.kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        #y_true = inputs.pop('labels')\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \"\"\"\n",
    "        p1 = outputs.pop('p1')\n",
    "        p2 = outputs.pop('p2')\n",
    "        print('len(p1)=',len(p1))\n",
    "        print('p1[0].shape=',p1[0].shape)\n",
    "        \n",
    "        #print('p1[0] =', p1[0])\n",
    "        #print('p2[0] =', p2[0])\n",
    "        self.eta = 0.7\n",
    "        KL_loss = 0.0\n",
    "        num_blocks = len(p1)-1\n",
    "        eps = torch.tensor(1e-7)\n",
    "        #print(num_blocks)\n",
    "        for i in range(num_blocks + 1):\n",
    "            #print('p1[%d].shape ='%i, p1[i])\n",
    "            #print('p2[%d].shape ='%i, p2[i])\n",
    "            #KL_loss += torch.mean(nn.functional.kl_div((p2[i]+eps).log(), p1[i], reduction='batchmean'))\n",
    "            KL_loss += torch.mean(self.kl_loss((p2[i]+eps).log(), p1[i]))\n",
    "            #KL_loss += torch.mean(nn.functional.kl_div(p2_log[i], p1[i], reduction='batchmean'))\n",
    "            #print('KL:', i, torch.mean(nn.functional.kl_div(p2[i].log(), p1[i], reduction='batchmean')))\n",
    "        KL_loss /= (num_blocks + 1)\n",
    "        print('KL =', KL_loss)\n",
    "        #print(logits)\n",
    "        #print(y_true)\n",
    "        #print('###')\n",
    "        #print(self.bcewithlogitsloss(logits, y_true))\n",
    "        print('logits.shpae =', logits.shape)\n",
    "        #print('y_true =',y_true)\n",
    "        #RS_loss = torch.mean(torch.clip(self.bcewithlogitsloss(logits, y_true), -10, 10))\n",
    "        RS_loss = torch.mean(self.bcewithlogitsloss(logits, y_true))\n",
    "        print('RS =', RS_loss)\n",
    "        print('KL =', KL_loss)\n",
    "        self.eta = 0.7\n",
    "        loss = self.eta * RS_loss + (1 - self.eta) * KL_loss\n",
    "        #loss = KL_loss\n",
    "        \"\"\"\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        #print('loss =', loss)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "            return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            #print('multi gpu loss =', loss)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            #print('## backward')\n",
    "            loss.backward()\n",
    "            #for name, param in model.named_parameters():\n",
    "                #print(name, param.grad)\n",
    "\n",
    "        return loss.detach()\n",
    "\n",
    "import Evaluate\n",
    "\n",
    "def compute_metrics(evalpred):\n",
    "    \n",
    "    preds, labels = evalpred\n",
    "    result = Evaluate.evaluate_all(preds, labels)\n",
    "            \n",
    "    return {\n",
    "        \"accuracy\" : result[0], \n",
    "        \"r2@1\"     : result[1],\n",
    "        \"r10@1\"    : result[2],\n",
    "        \"r10@2\"    : result[3],\n",
    "        \"r10@5\"    : result[4],\n",
    "        \"mrr\"      : result[5],\n",
    "        \"AvgScore\" : (result[1]+result[2]+result[3]+result[4]+result[5])/5.0, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c4e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset story_data (/home/kotech/.cache/huggingface/datasets/story_data/1cycle/1.0.0/1be572e09b3460db38e2808fe31e146a230e8bbde52821057ade3c66f8bb56e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff8c1a7e1cc48c58521e20fa69afed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('story_data', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9635f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"checkpoint\"\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-{dataset_name}\",\n",
    "    max_grad_norm = 5.0, # from the original source\n",
    "    #optim=\"adamw_torch\",\n",
    "    #learning_rate=0.001,\n",
    "    #adam_beta1=0.9,\n",
    "    #adam_beta2=0.98,\n",
    "    #adam_epsilon=1e-8,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size = 128,\n",
    "    per_device_eval_batch_size = 128*4,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"AvgScore\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\", # no, steps   # save_stratge should be same as eval stratege for best \n",
    "    eval_steps = 50 if dataset_name == \"original\" else 100,  # 25000/(batch*GPU)=50 for original 100 for 246578 helper\n",
    "    save_steps = 50 if dataset_name == \"original\" else 100,  # save steps should be multiple of eval_steps\n",
    "    save_total_limit = 2, # limit 1 and best_model automatically set to 2\n",
    "    logging_steps = 50, # not working????\n",
    "    logging_first_step = True, # not working???\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    seed=random_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf48594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset story_data (/home/kotech/.cache/huggingface/datasets/story_data/1cycle/1.0.0/1be572e09b3460db38e2808fe31e146a230e8bbde52821057ade3c66f8bb56e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527adc819eb14740b3cf00471d765d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('story_data', dataset_name)\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0568cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class patience_scheduler(torch.optim.lr_scheduler.LambdaLR):\n",
    "    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n",
    "        \n",
    "        self.lr = optimizer.param_groups[0]['lr']\n",
    "        #print('self.lr =', self.lr)\n",
    "        self._last_lr = [self.lr]\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            #print('self.lr =', self.lr)\n",
    "            #print('step =', step)\n",
    "            #lr = self.lr*(0.5**step)\n",
    "            lr = 0.5**(step+1)\n",
    "            #self._last_lr = [lr]\n",
    "            return lr\n",
    "        \n",
    "        super().__init__(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
    "        \n",
    "    def step(self, from_callback=False):\n",
    "        if (from_callback):\n",
    "            super().step()\n",
    "            print('lr changed to:', self.optimizer.param_groups[0]['lr'])\n",
    "            #print(self.lr)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "class LrCallback(TrainerCallback):\n",
    "    \"A callback after evaluation\"\n",
    "    def __init__(self):\n",
    "        self.best_score = -1\n",
    "        self.patience = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # eval_ is appended to the metric name with Trainer\n",
    "        name = 'eval_' + args.metric_for_best_model\n",
    "        score = kwargs['metrics'][name]\n",
    "        if score > self.best_score:\n",
    "            self.best_score = score\n",
    "            self.patience = 0\n",
    "        else:\n",
    "            self.patience += 1\n",
    "            if self.patience >= 3:\n",
    "                lr_scheduler = kwargs['lr_scheduler']\n",
    "                lr_scheduler.step(from_callback=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-8)\n",
    "scheduler = patience_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffeb69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ScriptwriterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[LrCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386769ee",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d3f2ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 277954\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2172\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2172' max='2172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2172/2172 1:30:19, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>R2@1</th>\n",
       "      <th>R10@1</th>\n",
       "      <th>R10@2</th>\n",
       "      <th>R10@5</th>\n",
       "      <th>Mrr</th>\n",
       "      <th>Avgscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.348919</td>\n",
       "      <td>0.778196</td>\n",
       "      <td>0.890847</td>\n",
       "      <td>0.575271</td>\n",
       "      <td>0.735083</td>\n",
       "      <td>0.942551</td>\n",
       "      <td>0.697718</td>\n",
       "      <td>0.768294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261524</td>\n",
       "      <td>0.822953</td>\n",
       "      <td>0.921922</td>\n",
       "      <td>0.687296</td>\n",
       "      <td>0.821909</td>\n",
       "      <td>0.971928</td>\n",
       "      <td>0.781626</td>\n",
       "      <td>0.836936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.302835</td>\n",
       "      <td>0.791918</td>\n",
       "      <td>0.939157</td>\n",
       "      <td>0.702833</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.977282</td>\n",
       "      <td>0.784094</td>\n",
       "      <td>0.848789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.264505</td>\n",
       "      <td>0.822797</td>\n",
       "      <td>0.947382</td>\n",
       "      <td>0.737564</td>\n",
       "      <td>0.863690</td>\n",
       "      <td>0.981460</td>\n",
       "      <td>0.809265</td>\n",
       "      <td>0.867872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.178733</td>\n",
       "      <td>0.884632</td>\n",
       "      <td>0.949732</td>\n",
       "      <td>0.743700</td>\n",
       "      <td>0.867476</td>\n",
       "      <td>0.983810</td>\n",
       "      <td>0.811826</td>\n",
       "      <td>0.871309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.152586</td>\n",
       "      <td>0.904452</td>\n",
       "      <td>0.956391</td>\n",
       "      <td>0.757540</td>\n",
       "      <td>0.882752</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.821227</td>\n",
       "      <td>0.880684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>0.171954</td>\n",
       "      <td>0.884541</td>\n",
       "      <td>0.960569</td>\n",
       "      <td>0.772033</td>\n",
       "      <td>0.885886</td>\n",
       "      <td>0.986943</td>\n",
       "      <td>0.829792</td>\n",
       "      <td>0.887045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.185590</td>\n",
       "      <td>0.871785</td>\n",
       "      <td>0.957436</td>\n",
       "      <td>0.801410</td>\n",
       "      <td>0.902207</td>\n",
       "      <td>0.988510</td>\n",
       "      <td>0.853786</td>\n",
       "      <td>0.900670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.160806</td>\n",
       "      <td>0.892009</td>\n",
       "      <td>0.961353</td>\n",
       "      <td>0.804674</td>\n",
       "      <td>0.903773</td>\n",
       "      <td>0.989816</td>\n",
       "      <td>0.853312</td>\n",
       "      <td>0.902586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.198395</td>\n",
       "      <td>0.869056</td>\n",
       "      <td>0.966445</td>\n",
       "      <td>0.798799</td>\n",
       "      <td>0.903643</td>\n",
       "      <td>0.991513</td>\n",
       "      <td>0.848120</td>\n",
       "      <td>0.901704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.163157</td>\n",
       "      <td>0.889933</td>\n",
       "      <td>0.964486</td>\n",
       "      <td>0.806633</td>\n",
       "      <td>0.907429</td>\n",
       "      <td>0.991774</td>\n",
       "      <td>0.854259</td>\n",
       "      <td>0.904916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.215816</td>\n",
       "      <td>0.853806</td>\n",
       "      <td>0.962267</td>\n",
       "      <td>0.813683</td>\n",
       "      <td>0.909127</td>\n",
       "      <td>0.991513</td>\n",
       "      <td>0.860843</td>\n",
       "      <td>0.907487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.195211</td>\n",
       "      <td>0.865687</td>\n",
       "      <td>0.965139</td>\n",
       "      <td>0.804021</td>\n",
       "      <td>0.906646</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.851208</td>\n",
       "      <td>0.903601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.245734</td>\n",
       "      <td>0.841063</td>\n",
       "      <td>0.962136</td>\n",
       "      <td>0.809636</td>\n",
       "      <td>0.904948</td>\n",
       "      <td>0.991905</td>\n",
       "      <td>0.856310</td>\n",
       "      <td>0.904987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.142843</td>\n",
       "      <td>0.908487</td>\n",
       "      <td>0.968142</td>\n",
       "      <td>0.805719</td>\n",
       "      <td>0.905471</td>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.852110</td>\n",
       "      <td>0.905139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.152100</td>\n",
       "      <td>0.194054</td>\n",
       "      <td>0.871001</td>\n",
       "      <td>0.968795</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.911868</td>\n",
       "      <td>0.993994</td>\n",
       "      <td>0.858810</td>\n",
       "      <td>0.909822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.163613</td>\n",
       "      <td>0.890560</td>\n",
       "      <td>0.968664</td>\n",
       "      <td>0.811986</td>\n",
       "      <td>0.910824</td>\n",
       "      <td>0.994125</td>\n",
       "      <td>0.857480</td>\n",
       "      <td>0.908616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.215696</td>\n",
       "      <td>0.860360</td>\n",
       "      <td>0.968534</td>\n",
       "      <td>0.822170</td>\n",
       "      <td>0.915263</td>\n",
       "      <td>0.994777</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.913309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.139629</td>\n",
       "      <td>0.907782</td>\n",
       "      <td>0.969317</td>\n",
       "      <td>0.809505</td>\n",
       "      <td>0.910432</td>\n",
       "      <td>0.994125</td>\n",
       "      <td>0.855453</td>\n",
       "      <td>0.907767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.149165</td>\n",
       "      <td>0.900196</td>\n",
       "      <td>0.968925</td>\n",
       "      <td>0.817470</td>\n",
       "      <td>0.916308</td>\n",
       "      <td>0.994255</td>\n",
       "      <td>0.861886</td>\n",
       "      <td>0.911769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>0.171049</td>\n",
       "      <td>0.886082</td>\n",
       "      <td>0.969578</td>\n",
       "      <td>0.820864</td>\n",
       "      <td>0.913696</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>0.862350</td>\n",
       "      <td>0.912358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-700\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-900\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr changed to: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1700\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-1900\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n",
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76590\n",
      "  Batch size = 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr changed to: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkpoint-1cycle/checkpoint-2100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [checkpoint-1cycle/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoint-1cycle/checkpoint-1800 (score: 0.9133088575117551).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2172, training_loss=0.19293519245326848, metrics={'train_runtime': 5430.4387, 'train_samples_per_second': 204.738, 'train_steps_per_second': 0.4, 'total_flos': 0.0, 'train_loss': 0.19293519245326848, 'epoch': 4.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38062c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "save_path = f\"{model_checkpoint}-{dataset_name}/checkpoint-best\"\n",
    "!mv $trainer.state.best_model_checkpoint $save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166b54d",
   "metadata": {},
   "source": [
    "**Testset Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506d6208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 74590\n",
      "  Batch size = 2048\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr changed to: 0.000125\n",
      "{'eval_loss': 0.22170042991638184, 'eval_accuracy': 0.8574875988738436, 'eval_r2@1': 0.9623273897305269, 'eval_r10@1': 0.8196809223756536, 'eval_r10@2': 0.9199624614559593, 'eval_r10@5': 0.9930285561067167, 'eval_mrr': 0.8645525699219171, 'eval_AvgScore': 0.9119103799181548, 'eval_runtime': 61.5337, 'eval_samples_per_second': 1212.182, 'eval_steps_per_second': 0.601, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=datasets[\"test\"])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f202c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint-1cycle/checkpoint-1800.\n"
     ]
    }
   ],
   "source": [
    "trainer._load_from_checkpoint('checkpoint-1cycle/checkpoint-1800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09a85a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "전체 실행 여기서 멈추기",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m전체 실행 여기서 멈추기\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: 전체 실행 여기서 멈추기"
     ]
    }
   ],
   "source": [
    "raise Exception('전체 실행 여기서 멈추기')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tensorflow + pytorch 1.10.1 + transformer install\n",
    "!pip install numpy==1.19.5\n",
    "!pip install pandas==1.4.0\n",
    "!pip install datasets==1.18.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6199f",
   "metadata": {},
   "source": [
    "**target should be float. Thus convert y_true to float**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc8f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss()\n",
    "input_ = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "target = torch.tensor([0, 1, 0]) # error\n",
    "output = loss(input_, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed1ed7",
   "metadata": {},
   "source": [
    "**Check for model with random inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00109cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 4\n",
    "device = torch.device(\"cuda\")\n",
    "response = torch.from_numpy(np.random.randint(1, 11400, size=(N, 50))).to(device)\n",
    "gt_response = torch.from_numpy(np.random.randint(1, 11400, size=(N, 50))).to(device)\n",
    "narrative = torch.from_numpy(np.random.randint(1, 11400, size=(N, 50))).to(device)\n",
    "utterance = torch.from_numpy(np.random.randint(1, 11400, size=(N, 11, 50))).to(device)\n",
    "labels = torch.FloatTensor(np.random.randint(0, 1, size=(N,))).to(device)\n",
    "\n",
    "model = ScriptWriter_cpre().to(device)\n",
    "\n",
    "_ = model(response=response, gt_response=gt_response, narrative=narrative, utterance=utterance, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding(torch.tensor([11883]).to(device))[:, 0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = torch.randn(3, requires_grad=True)\n",
    "#target = torch.empty(3).random_(2)\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "#y_true = [[0., 1., 0]]\n",
    "#y_pred = [[1., 0., 0]]\n",
    "y_true = [[0., 1., 0]]\n",
    "y_pred = [[0., 1., 0]]\n",
    "input = torch.tensor(y_pred)\n",
    "target = torch.tensor(y_true)\n",
    "loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss((input+1e-7).log(), target).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input.log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbf474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d96ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af539cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f5648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb816084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_normalization2(nn.Module):\n",
    "\n",
    "    def __init__(self, features, epsilon=1e-8):\n",
    "        '''Applies layer normalization.\n",
    "        Args:\n",
    "          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        print(x[0][1].mean())\n",
    "        print('mean.shape =', mean.shape)\n",
    "        print(x[0][1])\n",
    "        print('mean =', mean)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        print('std =', std)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
    "    \n",
    "class feedforward2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_units=[2048, 512]):\n",
    "        '''Point-wise feed forward net.\n",
    "        Args:\n",
    "          in_channels: a number of channels of inputs\n",
    "          num_units: A list of two integers.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_units = num_units\n",
    "        \n",
    "        # tensorflow compatible initializer\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "        # nn.Linear is faster than nn.Conv1d\n",
    "        self.conv = False\n",
    "        if self.conv:\n",
    "            params = {'in_channels': self.in_channels, 'out_channels': self.num_units[0],\n",
    "                      'kernel_size': 1, 'stride': 1, 'bias': True}\n",
    "            self.conv1 = nn.Sequential(nn.Conv1d(**params), nn.ReLU())\n",
    "            params = {'in_channels': self.num_units[0], 'out_channels': self.num_units[1],\n",
    "                      'kernel_size': 1, 'stride': 1, 'bias': True}\n",
    "            self.conv2 = nn.Conv1d(**params)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Linear(self.in_channels, self.num_units[0]), nn.ReLU())\n",
    "            self.conv2 = nn.Linear(self.num_units[0], self.num_units[1])\n",
    "        self.conv1.apply(init_weights)\n",
    "        self.conv2.apply(init_weights)\n",
    "        self.normalization = layer_normalization2(self.in_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.conv:\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "\n",
    "        # Layer normalization\n",
    "        if self.conv:\n",
    "            outputs = self.normalization(outputs.permute(0, 2, 1))\n",
    "            #outputs = outputs.permute(0, 2, 1)\n",
    "        else:\n",
    "            outputs = self.normalization(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "input_ = torch.FloatTensor(np.zeros((1,50,200))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = np.zeros((1,50))\n",
    "response[0][0] = 7\n",
    "response = torch.IntTensor(response).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caa963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScriptWriter_cpre().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_embeddings = model.embedding(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = response_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfdb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = multihead_attention(num_units=200).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mha(input_, input_, input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b032b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = feedforward2(200, [200,200]).to(device)\n",
    "output = ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51108744",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296600c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ac987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a936fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89fcc646",
   "metadata": {},
   "source": [
    "**l2_normalize tf compatible**  \n",
    "https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(\n",
    "    [[3.0, 3.0], [4.0, 4.0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed62406",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.normalize(x, dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d00d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4243**2 + 0.5657**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87567edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c34c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140dd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mat1 = torch.randn((10, 3, 4))\n",
    "mat2 = torch.randn((10, 4, 5))\n",
    "output = torch.matmul(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdca1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0eb37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/positive_ko.pkl', \"rb\") as f:\n",
    "    positive_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_utterances = []\n",
    "for utterances, narrative, _ in positive_data:\n",
    "    all_utterances += utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff264353",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = int(len(positive_data) * 0.9)\n",
    "dev_test_num = int(len(positive_data) * 0.05)\n",
    "train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.utterance, self.response, self.narrative, self.gt_response, self.label = x\n",
    "        self.utterance = torch.tensor(self.utterance)\n",
    "        self.response = torch.tensor(self.response)\n",
    "        self.narrative = torch.tensor(self.narrative)\n",
    "        self.gt_response = torch.tensor(self.gt_response)\n",
    "        self.label = torch.FloatTensor(self.label)\n",
    "        self.n = len(self.label)\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        return {\n",
    "            \"utterance\": self.utterance[index],\n",
    "            \"response\": self.response[index],\n",
    "            \"narrative\": self.narrative[index],\n",
    "            \"gt_response\": self.gt_response[index],\n",
    "            \"label\": self.label[index],\n",
    "        }\n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "def tensor_dataset(x):\n",
    "    utterance, response, narrative, gt_response, label = x\n",
    "    utterance = torch.tensor(utterance)\n",
    "    response = torch.tensor(response)\n",
    "    narrative = torch.tensor(narrative)\n",
    "    gt_response = torch.tensor(gt_response)\n",
    "    label = torch.FloatTensor(label)\n",
    "    return {\n",
    "        \"utterance\": TensorDataset(utterance),\n",
    "        \"response\": TensorDataset(response),\n",
    "        \"narrative\": TensorDataset(narrative),\n",
    "        \"gt_response\": TensorDataset(gt_response),\n",
    "        \"label\": TensorDataset(label),\n",
    "    }\n",
    "    return TensorDataset(utterance, response, narrative, gt_response, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list(context, narrative, all_utterances):\n",
    "    data_list = []\n",
    "    for utterance in all_utterances:\n",
    "        data_list.append([context, utterance, narrative, utterance, 0])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e47f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uterrances, narrative, _ in test[0:1]:\n",
    "    context = [utterances[0]]\n",
    "    for i in range(1, 10):\n",
    "        x_list = make_list(context, narrative, all_utterances)\n",
    "        #print('pad_processing...')\n",
    "        x = pad_process(x_list)    \n",
    "        predict_dataset = StoryDataset(x)\n",
    "        #predict_dataset = tensor_dataset(x)\n",
    "        y = trainer.predict(test_dataset=predict_dataset)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98212f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d300d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3bb0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f4dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd97fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_from_nonfixed_2d_array(aa, max_sentence_len=50, max_num_utterance=10, padding_value=0):\n",
    "    PAD_SEQUENCE = np.array([0] * max_sentence_len)\n",
    "    rows = np.empty([0, max_sentence_len], dtype='int')\n",
    "    aa = aa[:max_num_utterance]\n",
    "    for a in aa:\n",
    "        sentence_len = len(a)\n",
    "        if sentence_len < max_sentence_len:\n",
    "            rows  = np.append(rows, [np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)[:max_sentence_len]], axis=0)\n",
    "        else:\n",
    "            rows = np.append(rows, [a[:max_sentence_len]], axis=0)\n",
    "    num_utterance = len(aa)\n",
    "    if num_utterance < max_num_utterance:\n",
    "        rows = np.append(rows, [PAD_SEQUENCE]*(max_num_utterance-num_utterance), axis=0)\n",
    "    # add empty +1 sentence\n",
    "    rows = np.append(rows, [PAD_SEQUENCE], axis=0)\n",
    "    #return np.concatenate(rows, axis=0).reshape(-1, max_sentence_len)\n",
    "    return rows\n",
    "\n",
    "def get_numpy_from_nonfixed_1d_array(a, max_sentence_len=50, padding_value=0):\n",
    "    sentence_len = len(a)\n",
    "    if sentence_len < max_sentence_len:\n",
    "        return np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)\n",
    "    else:\n",
    "        return np.array(a[:max_sentence_len])\n",
    "\n",
    "from tqdm import tqdm\n",
    "def pad_process(data, max_sentence_len=50, max_num_utterance=10):\n",
    "    utterance = []\n",
    "    response = []\n",
    "    narrative = []\n",
    "    gt_response = []\n",
    "    y_true = []\n",
    "    #for unit in tqdm(data):\n",
    "    for unit in data:\n",
    "        utterance.append(get_numpy_from_nonfixed_2d_array(unit[0]))\n",
    "        response.append(get_numpy_from_nonfixed_1d_array(unit[1]))\n",
    "        narrative.append(get_numpy_from_nonfixed_1d_array(unit[2]))\n",
    "        gt_response.append(get_numpy_from_nonfixed_1d_array(unit[3]))\n",
    "        y_true.append(unit[4])\n",
    "        \n",
    "    utterance = np.stack(utterance)\n",
    "    response = np.stack(response)\n",
    "    narrative = np.stack(narrative)\n",
    "    gt_response = np.stack(gt_response)\n",
    "    y_true = np.stack(y_true)\n",
    "    return (utterance, response, narrative, gt_response, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.argsort([0.1, 0.2, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2589be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
