{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce678eb1",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18263a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda 11.1\n",
    "#!pip install -r requirements-torch-cu111.txt --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5aa70c-bcc0-451d-98a5-fe4565370fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list | grep -E \"gdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a96d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdb1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'final'\n",
    "# AI hub 업로드 용 변경\n",
    "#url = 'https://drive.google.com/uc?id=1qQQbjFd0c7unV-S18t-7x3KUw5OFVl3P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1b4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts_file = f'data/json_split.zip'\n",
    "#gdown.download(url, scripts_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450a0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zipu --extract --encoding cp949 'data/json_split.zip' 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40576bed-862b-48ed-a41d-2b0eaa6f20d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469M\tdata/train\n",
      "63M\tdata/validation\n"
     ]
    }
   ],
   "source": [
    "!du -sh data/train\n",
    "!du -sh data/validation\n",
    "#!du -sh data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1255e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df945932",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = sorted(glob('data/train/*/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6bcfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107e0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file) as f:\n",
    "        story_dict = json.load(f)\n",
    "    units = story_dict['units']\n",
    "    for unit in units:\n",
    "        unit_dict = {}\n",
    "        unit_dict['uid'] = unit['id']\n",
    "        unit_dict['storyline'] = unit['storyline']\n",
    "        unit_dict['script'] = []\n",
    "        for story_script in unit['story_scripts']:\n",
    "            unit_dict['script'].append(story_script['content'])\n",
    "        data_dict.append(unit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89c36ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train num = 79782\n"
     ]
    }
   ],
   "source": [
    "train_num = len(data_dict)\n",
    "print('train num =', train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f397b4aa-0757-4d10-b284-43ae87e8ca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n"
     ]
    }
   ],
   "source": [
    "json_files = sorted(glob('data/validation/*/*.json'))\n",
    "print(len(json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9bcaf02-fbc9-455f-9cb2-7376d26580a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_num = 10069\n"
     ]
    }
   ],
   "source": [
    "val_num = 0\n",
    "for json_file in json_files:\n",
    "    with open(json_file) as f:\n",
    "        story_dict = json.load(f)\n",
    "    units = story_dict['units']\n",
    "    for unit in units:\n",
    "        unit_dict = {}\n",
    "        unit_dict['uid'] = unit['id']\n",
    "        unit_dict['storyline'] = unit['storyline']\n",
    "        unit_dict['script'] = []\n",
    "        for story_script in unit['story_scripts']:\n",
    "            unit_dict['script'].append(story_script['content'])\n",
    "        data_dict.append(unit_dict)\n",
    "        val_num += 1\n",
    "print('val_num =', val_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aac235b-7bee-4eaa-8339-743f25cb2cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_num = 10069\n"
     ]
    }
   ],
   "source": [
    "print('val_num =', val_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c43c64b-0363-4677-9bb5-a3039dfc76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "json_files = sorted(glob('data/test/*/*.json'))\n",
    "print(len(json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec2831e-7004-4e94-9fc8-8527dee86238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test num = 0\n"
     ]
    }
   ],
   "source": [
    "test_num = 0\n",
    "for json_file in json_files:\n",
    "    with open(json_file) as f:\n",
    "        story_dict = json.load(f)\n",
    "    units = story_dict['units']\n",
    "    for unit in units:\n",
    "        unit_dict = {}\n",
    "        unit_dict['uid'] = unit['id']\n",
    "        unit_dict['storyline'] = unit['storyline']\n",
    "        unit_dict['script'] = []\n",
    "        for story_script in unit['story_scripts']:\n",
    "            unit_dict['script'].append(story_script['content'])\n",
    "        data_dict.append(unit_dict)\n",
    "        test_num += 1\n",
    "print('test num =', test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a67e5eea-1275-495b-97d5-ec42cbaea9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89851"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dc478",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1fe417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = 'final'\n",
    "#url = 'https://drive.google.com/uc?id=1Bts2h-QPQ5-m7sDIXgVRfumjl-8XHOst'\n",
    "#url = 'https://drive.google.com/uc?id=1x6HuyJTQcNydJ9P-fJl2LtxnnAu9Vp8N'\n",
    "#prefix = '1cycle'\n",
    "#url = 'https://drive.google.com/uc?id=1j46elyFZtkmnmCehlntMi0eX0Tp5nnav'\n",
    "#prefix = 'helper'\n",
    "#url = 'https://drive.google.com/uc?id=1iSP_YKFs56d5cRRTEMzfedwRxrx-nXWO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061a428",
   "metadata": {},
   "source": [
    "## 스토리헬퍼 샘플 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac09f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts_file = f'data/scripts_{prefix}.json'\n",
    "#zip_file = f'data/scripts_{prefix}.zip'\n",
    "#gdown.download(url, zip_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef689b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip $zip_file \n",
    "#!mv -f 'final.json' $scripts_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af38109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open(scripts_file) as f:\n",
    "#    data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c880d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid': '01_0017_01',\n",
       " 'storyline': 'C001은 지하철 안에서 졸다가 깨어 내리는데 두 여자아이를 발견하고 섬찟한다.',\n",
       " 'script': ['C001은 지하철 노약자석 한구석에서 졸고 있다.',\n",
       "  'C001은 졸다가 눈을 가늘게 떴을 때 어떤 엄마와 아이를 본다.',\n",
       "  'C001은 열차가 문을 열고 닫는 동안 계속 졸다가 종착역임을 알게 되고 빠져나온다.',\n",
       "  'C001은 열차 안에 두 여자아이가 잠든 채로 문이 닫히고 출발하는 것을 본다.',\n",
       "  'C001은 섬찟하지만, 잠에서 막 깨어 피곤해한다.',\n",
       "  'C001은 집으로 들어가 부엌으로 향한다.',\n",
       "  '뭐 해?',\n",
       "  '놀래라. 소리도 없이 들어오네? 귀신인 줄 알았어. 밖에 비 와?',\n",
       "  'C001은 수건으로 머리를 닦으며 말한다.',\n",
       "  '제대로 된 우산 하나 있었는데 지난번에 차에 두고 내렸네.',\n",
       "  '할 수 없네. 갈게. 피곤해 보이니까 잘 쉬고.']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플 데이터 출력\n",
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618293c6",
   "metadata": {},
   "source": [
    "**후처리**\n",
    "1. `\\n`을 제거한다. \"부엌에서 일하게 된 마리오\\n인부들 사이에서 인기만점인 베아트리체\"  \n",
    "   ==> 필요없는 것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d48b59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정상적 white character가 없는지 확인\n",
    "for idx, data in enumerate(data_dict):\n",
    "    #data['storyline'] = data['storyline'].replace('\\n', ' ')\n",
    "    for i, context in enumerate(data['script']):\n",
    "        #if '\\n' in context:\n",
    "        if '부엌에서 일하게' in context:\n",
    "            print(idx, i, context)\n",
    "            print('\"%s%s\"'%(context[9],context[10]))\n",
    "            print(context[10] == ' ')\n",
    "        #data['script'][i] = context.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58e2ce",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b08482",
   "metadata": {},
   "source": [
    "### kobigbird pretrained model을 이용한 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb68ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4cc6868ccf4e29a14f56279fd02426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96018d4acd144632860e1640b77efddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/241k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db50e2ba334a46cc8de141cb8650f281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/492k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e39ffb22fa4fcf8b1d06de0293279f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('monologg/kobigbird-bert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab61f27",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e81a5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all suitable sessions:  89851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EOS_ID = tokenizer.sep_token_id\n",
    "\n",
    "positive_sessions = []\n",
    "positive_str = []\n",
    "positive_ids = []\n",
    "for i, unit_data in enumerate(data_dict):\n",
    "    unit_contexts = [tokenizer.tokenize(text) for text in unit_data['script'] + ['[SEP]'] ]\n",
    "    while [] in unit_contexts:\n",
    "        print('empty string in the script. removing..., id=', i)\n",
    "        index = unit_contexts.index([])\n",
    "        #print(\"'{%s}'\"%unit_data['script'][index])\n",
    "        del unit_contexts[index]\n",
    "        #unit_contexts.remove([])\n",
    "        del unit_data['script'][index]\n",
    "    if len(unit_contexts) <= 1:\n",
    "        print('empty scripts. skipping..., id=', i)\n",
    "        continue\n",
    "    unit_narrative = tokenizer.tokenize(unit_data['storyline'])\n",
    "    if len(unit_narrative) == 0:\n",
    "        print('empty narrative. skipping, id=', i)\n",
    "        continue\n",
    "    positive_sessions.append([unit_contexts, unit_narrative, 1])\n",
    "    positive_str.append(unit_data)\n",
    "    positive_ids.append(unit_data['uid'])\n",
    "print(\"all suitable sessions: \", len(positive_sessions))\n",
    "\n",
    "# reproducibility를 위한 random seed 설정\n",
    "np.random.seed(42)\n",
    "# random shuffle data\n",
    "np.random.shuffle(positive_sessions)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(positive_str)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(positive_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a2bb72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01_1675_06',\n",
       " '04_3879_21',\n",
       " '02_1722_01',\n",
       " '01_2710_26',\n",
       " '04_3409_25',\n",
       " '02_2621_07',\n",
       " '01_0734_04',\n",
       " '02_2289_03',\n",
       " '02_3373_10',\n",
       " '02_3114_13']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc46677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train = 79782 , val = 10069 , test = 0\n"
     ]
    }
   ],
   "source": [
    "#train_num = int(len(positive_sessions) * 0.9)\n",
    "#dev_test_num = int(len(positive_sessions) * 0.05)\n",
    "train_sessions, dev_sessions, test_sessions = positive_sessions[:train_num], positive_sessions[train_num: train_num + val_num], positive_sessions[train_num + val_num:]\n",
    "print('number of train =', len(train_sessions), ', val =', len(dev_sessions), ', test =', len(test_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "132fe371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word2vec training sentences = 1253487\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "for train_session in train_sessions:\n",
    "    train_texts += train_session[0]\n",
    "    train_texts.append(train_session[1])\n",
    "print('number of word2vec training sentences =', len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7540667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# word2vec 학습\n",
    "model = Word2Vec(sentences = train_texts, vector_size = 200, window = 7, min_count = 5, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e2e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of words = 18874\n",
      "first word = \"C\"\n",
      "last word = \"창업자\"\n"
     ]
    }
   ],
   "source": [
    "print('total num of words =', len(model.wv.key_to_index))\n",
    "print('first word = \"%s\"'%model.wv.index_to_key[0])\n",
    "print('last word = \"%s\"'%model.wv.index_to_key[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4707535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친구', 0.6886770725250244), ('지인', 0.6636408567428589), ('식구', 0.631086528301239), ('자녀', 0.6270747780799866), ('백성', 0.6188133955001831), ('단원', 0.6184972524642944), ('하인', 0.6144576072692871), ('부대원', 0.6113783121109009), ('동료', 0.6105010509490967), ('하객', 0.6077830195426941)]\n"
     ]
    }
   ],
   "source": [
    "# word2vec이 잘 학습되었는지 여러가지 테스트를 수행하자.\n",
    "print(model.wv.most_similar(\"가족\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ad5d7",
   "metadata": {},
   "source": [
    "## 데이터 저장\n",
    "\n",
    "`embeddings.pkl`과 `vocab.txt`를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8453ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/vocab_{prefix}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, key in enumerate(model.wv.index_to_key):\n",
    "        file.write('%s\\t%i\\n'%(key, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67a3bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "new_embeddings = np.array([[0.]*200],dtype='float32') \n",
    "for i in range(len(model.wv.index_to_key)):\n",
    "    new_embeddings = np.append(new_embeddings, [model.wv.get_vector(i)], axis=0)\n",
    "\n",
    "with open(f'data/embeddings_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(new_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f435a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"data/word2vec_{prefix}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4ec99",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "066fc318",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = model.wv.key_to_index['[SEP]']+1\n",
    "UNK_ID = model.wv.key_to_index['[UNK]']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a86226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "with open(f\"data/vocab_{prefix}.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "    for idx, line in enumerate(fr):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        vocab[line[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0501f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample id 출력 확인\n",
    "vocab['가족']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611b33b",
   "metadata": {},
   "source": [
    "**positive data 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f165912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = []\n",
    "positive_str2 = []\n",
    "\n",
    "for unit, unit_str in zip(positive_sessions, positive_str):\n",
    "    narrative = unit[1]\n",
    "    #print(narrative)\n",
    "    context = unit[0]\n",
    "    narrative_id = [vocab.get(word, UNK_ID) for word in narrative]\n",
    "    context_id = [[vocab.get(word, UNK_ID) for word in sent] for sent in context]\n",
    "    if len(narrative_id) == 0 or len(context_id) == 0:\n",
    "        print('empty narrative found. skipping...')\n",
    "        #print(unit[0])\n",
    "        #print(unit[1])\n",
    "        print(unit_str)\n",
    "        continue\n",
    "    data = [context_id, narrative_id, 1]\n",
    "    positive_data.append(data)\n",
    "    positive_str2.append(unit_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a74d8151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89851, 89851, 89851)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_str), len(positive_str2), len(positive_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9944ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + val_num], positive_data[train_num + val_num:]\n",
    "train_ids, dev_ids, test_ids = positive_ids[:train_num], positive_ids[train_num: train_num + val_num], positive_ids[train_num + val_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22242ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 3, 36, 6, 1111, 10, 523, 34, 6743, 5, 46, 2]], [1, 3, 54, 9, 696, 1, 3, 36, 8, 16, 1010, 5, 337, 2], [7113, 8, 8096, 9162, 12998, 6, 1210, 164, 4538, 26, 14, 1, 3, 36, 346, 13, 646, 1111, 94, 26, 472, 16, 182, 2], [1, 3, 54, 9, 696, 1, 3, 36, 8, 16, 1010, 5, 337, 2], 1]\n",
      "[[[1, 3, 36, 6, 1111, 10, 523, 34, 6743, 5, 46, 2]], [1, 3, 49, 4, 10730, 5, 100, 14, 367, 104, 2], [7113, 8, 8096, 9162, 12998, 6, 1210, 164, 4538, 26, 14, 1, 3, 36, 346, 13, 646, 1111, 94, 26, 472, 16, 182, 2], [1, 3, 54, 9, 696, 1, 3, 36, 8, 16, 1010, 5, 337, 2], 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_all, dev_all, test_all = [], [], []\n",
    "for context_id, narrative_id, _ in train:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        train_all.append([context, response, narrative_id, response, 1])\n",
    "        flag = True\n",
    "        while flag:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if len(response) != len(random_response):\n",
    "                flag = False\n",
    "                train_all.append([context, random_response, narrative_id, response, 0])\n",
    "            else:\n",
    "                for idx, wid in enumerate(response):\n",
    "                    if wid != random_response[idx]:\n",
    "                        flag = False\n",
    "                        train_all.append([context, random_response, narrative_id, response, 0])\n",
    "                        break\n",
    "print(train_all[0]) \n",
    "print(train_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "287d8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 3, 7, 4, 8847, 10, 120, 14, 20, 14, 1, 3, 12, 4, 226, 87, 506, 2]], [1, 3, 12, 4, 8847, 120, 4, 1, 3, 7, 10, 2030, 22, 110, 172, 27, 77, 20, 11, 2], [1, 3, 12, 4, 1575, 17, 216, 2579, 8, 16, 1402, 22, 64, 14, 366, 22, 126, 1, 3, 7, 4, 166, 396, 127, 2], [1, 3, 12, 4, 8847, 120, 4, 1, 3, 7, 10, 2030, 22, 110, 172, 27, 77, 20, 11, 2], 1] [[[1, 3, 7, 4, 8847, 10, 120, 14, 20, 14, 1, 3, 12, 4, 226, 87, 506, 2]], [1, 3, 19, 6, 2607, 6, 707, 1648, 228, 40, 3072, 5, 77, 20, 11, 2], [1, 3, 12, 4, 1575, 17, 216, 2579, 8, 16, 1402, 22, 64, 14, 366, 22, 126, 1, 3, 7, 4, 166, 396, 127, 2], [1, 3, 12, 4, 8847, 120, 4, 1, 3, 7, 10, 2030, 22, 110, 172, 27, 77, 20, 11, 2], 0] [[[1, 3, 7, 4, 8847, 10, 120, 14, 20, 14, 1, 3, 12, 4, 226, 87, 506, 2]], [115, 263, 8, 1597, 1447, 1679, 8, 2], [1, 3, 12, 4, 1575, 17, 216, 2579, 8, 16, 1402, 22, 64, 14, 366, 22, 126, 1, 3, 7, 4, 166, 396, 127, 2], [1, 3, 12, 4, 8847, 120, 4, 1, 3, 7, 10, 2030, 22, 110, 172, 27, 77, 20, 11, 2], 0]\n"
     ]
    }
   ],
   "source": [
    "dev_all_ids = []\n",
    "for i_dev, (context_id, narrative_id, _) in enumerate(dev):\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        dev_all.append([context, response, narrative_id, response, 1])\n",
    "        dev_all_ids.append(dev_ids[i_dev])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        # fix count 버그\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    dev_all.append([context, random_response, narrative_id, response, 0])\n",
    "                    negative_samples.append(random_response)\n",
    "                    dev_all_ids.append(dev_ids[i_dev])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for idx, wid in enumerate(response):\n",
    "                        if wid != random_response[idx]:\n",
    "                            dev_all.append([context, random_response, narrative_id, response, 0])\n",
    "                            negative_samples.append(random_response)\n",
    "                            dev_all_ids.append(dev_ids[i_dev])\n",
    "                            count += 1\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 1])\n",
    "        else:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 0])\n",
    "        dev_all_ids.append(dev_ids[i_dev])\n",
    "print(dev_all[0], dev_all[1], dev_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d77f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = []\n",
    "test_all_ids = []\n",
    "test_num_context = []\n",
    "for i_test, (context_id, narrative_id, _) in enumerate(test):\n",
    "    num_context = len(context_id)\n",
    "    test_num_context.append(num_context-1)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        test_all.append([context, response, narrative_id, response, 1])\n",
    "        test_all_ids.append(test_ids[i_test])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        # fix count 버그\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    test_all.append([context, random_response, narrative_id, response, 0])\n",
    "                    negative_samples.append(random_response)\n",
    "                    test_all_ids.append(test_ids[i_test])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for idx, id in enumerate(response):\n",
    "                        if id != random_response[idx]:\n",
    "                            test_all.append([context, random_response, narrative_id, response, 0])\n",
    "                            negative_samples.append(random_response)\n",
    "                            test_all_ids.append(test_ids[i_test])\n",
    "                            count += 1\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 1])\n",
    "        else:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 0])\n",
    "        test_all_ids.append(test_ids[i_test])\n",
    "if test_num > 0:\n",
    "    print(test_all[0], test_all[1], test_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f3eaafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_all_ids), len(test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5c5b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train count = 2187846\n",
      "total val count = 1383250\n",
      "total test count = 0\n"
     ]
    }
   ],
   "source": [
    "print('total train count =', len(train_all))\n",
    "print('total val count =', len(dev_all))\n",
    "print('total test count =', len(test_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cb7931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(test_num_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47cd19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_from_nonfixed_2d_array(aa, max_sentence_len=50, max_num_utterance=10, padding_value=0):\n",
    "    PAD_SEQUENCE = np.array([0] * max_sentence_len)\n",
    "    rows = np.empty([0, max_sentence_len], dtype='int')\n",
    "    aa = aa[-max_num_utterance:]\n",
    "    for a in aa:\n",
    "        sentence_len = len(a)\n",
    "        if sentence_len < max_sentence_len:\n",
    "            rows  = np.append(rows, [np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)[:max_sentence_len]], axis=0)\n",
    "        else:\n",
    "            rows = np.append(rows, [a[:max_sentence_len]], axis=0)\n",
    "    num_utterance = len(aa)\n",
    "    if num_utterance < max_num_utterance:\n",
    "        rows = np.append(rows, [PAD_SEQUENCE]*(max_num_utterance-num_utterance), axis=0)\n",
    "    # add empty +1 sentence\n",
    "    rows = np.append(rows, [PAD_SEQUENCE], axis=0)\n",
    "    #return np.concatenate(rows, axis=0).reshape(-1, max_sentence_len)\n",
    "    return rows\n",
    "\n",
    "def get_numpy_from_nonfixed_1d_array(a, max_sentence_len=50, padding_value=0):\n",
    "    sentence_len = len(a)\n",
    "    if sentence_len < max_sentence_len:\n",
    "        return np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)\n",
    "    else:\n",
    "        return np.array(a[:max_sentence_len])\n",
    "\n",
    "cc_test_data = [\n",
    "        [1,2],\n",
    "        [4,5,6],\n",
    "        [7]\n",
    "     ]\n",
    "#get_numpy_from_nonfixed_2d_array(cc_test_data, max_sentence_len=5, max_num_utterance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "475ebe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    __IPYTHON__\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "def pad_process(data, max_sentence_len=50, max_num_utterance=10):\n",
    "    utterance = []\n",
    "    response = []\n",
    "    narrative = []\n",
    "    gt_response = []\n",
    "    y_true = []\n",
    "    for unit in tqdm(data):\n",
    "        utterance.append(get_numpy_from_nonfixed_2d_array(unit[0]))\n",
    "        response.append(get_numpy_from_nonfixed_1d_array(unit[1]))\n",
    "        narrative.append(get_numpy_from_nonfixed_1d_array(unit[2]))\n",
    "        gt_response.append(get_numpy_from_nonfixed_1d_array(unit[3]))\n",
    "        y_true.append(unit[4])\n",
    "    utterance = np.stack(utterance)\n",
    "    response = np.stack(response)\n",
    "    narrative = np.stack(narrative)\n",
    "    gt_response = np.stack(gt_response)\n",
    "    y_true = np.stack(y_true)\n",
    "    return (utterance, response, narrative, gt_response, y_true)\n",
    "\n",
    "train_pad = pad_process(train_all)\n",
    "dev_pad = pad_process(dev_all)\n",
    "if test_num > 0:    \n",
    "    test_pad = pad_process(test_all)\n",
    "else:\n",
    "    test_pad = ([], [], [], [], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003946db",
   "metadata": {},
   "source": [
    "**학습데이터셋 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4d8dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/train_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(train_pad, f)\n",
    "with open(f'data/dev_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_pad, f)\n",
    "with open(f'data/test_{prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3ac8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/positive_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_data, f)\n",
    "with open(f'data/positive_str_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_str2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "642fbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/test_all_ids_{prefix}.pkl', \"wb\") as f:\n",
    "    pickle.dump(test_all_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "449089fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit, unit_str in zip (positive_data, positive_str2):\n",
    "    len_unit = len(unit[0])\n",
    "    len_unit_str = len(unit_str['script'])\n",
    "    if len_unit != len_unit_str+1:\n",
    "        print(len_unit, len_unit_str)\n",
    "    #print(unit[0])\n",
    "    #print(unit_str['script'])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f477e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dat(index, data_pad, ids = None):\n",
    "    utterances = data_pad[0][index]\n",
    "    response = data_pad[1][index]\n",
    "    narrative = data_pad[2][index]\n",
    "    gt_response = data_pad[3][index]\n",
    "    y_true = data_pad[4][index]\n",
    "    narrative = narrative[narrative!=0]\n",
    "    response = response[response!=0]\n",
    "    gt_response = gt_response[gt_response!=0]\n",
    "    #print([model.wv.index_to_key[k-1] for k in narrative])\n",
    "    narrative_str = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in narrative])\n",
    "    response_str = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in response])\n",
    "    gt_response_str = tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in gt_response])\n",
    "    #print(y_true)\n",
    "    utterance_str = ['']*10\n",
    "    for i in range(10):\n",
    "        utterance = utterances[i]\n",
    "        utterance = utterance[utterance!=0]\n",
    "        if len(utterance) == 0:\n",
    "            break\n",
    "        utterance_str[i] =  tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in utterance])\n",
    "    #print()\n",
    "    if ids == None:\n",
    "        id_ = None\n",
    "    else:\n",
    "        id_ = ids[index]\n",
    "    return id_, narrative_str, response_str, gt_response_str, y_true, utterance_str\n",
    "    \n",
    "def browse_dat(index, data_pad):\n",
    "    utterances = data_pad[0][index]\n",
    "    response = data_pad[1][index]\n",
    "    narrative = data_pad[2][index]\n",
    "    gt_response = data_pad[3][index]\n",
    "    y_true = data_pad[4][index]\n",
    "    narrative = narrative[narrative!=0]\n",
    "    response = response[response!=0]\n",
    "    gt_response = gt_response[gt_response!=0]\n",
    "    #print([model.wv.index_to_key[k-1] for k in narrative])\n",
    "    print('N:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in narrative]))\n",
    "    print('R:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in response]))\n",
    "    print('T:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in gt_response]))\n",
    "    print(y_true)\n",
    "    for i in range(10):\n",
    "        utterance = utterances[i]\n",
    "        utterance = utterance[utterance!=0]\n",
    "        if len(utterance) == 0:\n",
    "            break\n",
    "        print('U:', tokenizer.convert_tokens_to_string([model.wv.index_to_key[k-1] for k in utterance]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "#browse_dat(0, train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd7b37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_num > 0:\n",
    "    for i in range(110,120): \n",
    "        browse_dat(i, test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82471e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b4d3606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'score', 'R2@1', 'R10@1', 'R10@2', 'R10@5', 'MRR', 'AVG', 'U01', 'U02', 'U03', 'U04', 'U05', 'U06', 'U07', 'U08', 'U09', 'U10']\n"
     ]
    }
   ],
   "source": [
    "column_names = ['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'score', 'R2@1', 'R10@1', 'R10@2', 'R10@5', 'MRR', 'AVG']\n",
    "for i in range(10):\n",
    "    column_names.append('U%02d'%(i+1))\n",
    "print(column_names)\n",
    "df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e54583f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48979c8479c54d2699a4bc7b96756fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(test_all_ids)\n",
    "data_dict_all = []\n",
    "for i in tqdm(range(n)):\n",
    "    id_, narrative_str, response_str, gt_response_str, y_true, utterance_str = get_dat(i, test_pad, test_all_ids)\n",
    "    data_dict = { }\n",
    "    data_dict['id'] = id_\n",
    "    data_dict['Narrative'] = narrative_str\n",
    "    data_dict['Response'] = response_str\n",
    "    data_dict['GT_Response'] = gt_response_str\n",
    "    data_dict['y_true'] = y_true\n",
    "    for i in range(10):\n",
    "        data_dict[f'U%02d'%(i+1)] = utterance_str[i]\n",
    "    #new_row = pd.Series(data_dict)\n",
    "    #df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "    data_dict_all.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f01f0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "354aa55f-1f0e-46f2-99f1-32764ba3ce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd97d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output rows are too large for excel\n",
    "# save to csv\n",
    "df.to_csv(f'test_output_{prefix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54a11abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'score', 'R2@1', 'R10@1', 'R10@2', 'R10@5', 'MRR', 'AVG', 'U01', 'U02', 'U03', 'U04', 'U05', 'U06', 'U07', 'U08', 'U09', 'U10']\n"
     ]
    }
   ],
   "source": [
    "column_names = ['id', 'Narrative', 'Response', 'GT_Response', 'y_true', 'score', 'R2@1', 'R10@1', 'R10@2', 'R10@5', 'MRR', 'AVG']\n",
    "for i in range(10):\n",
    "    column_names.append('U%02d'%(i+1))\n",
    "print(column_names)\n",
    "df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44b58053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a64dc8eed14b72a087f5ce2b0500c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1383250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(dev_all_ids)\n",
    "data_dict_all = []\n",
    "for i in tqdm(range(n)):\n",
    "    id_, narrative_str, response_str, gt_response_str, y_true, utterance_str = get_dat(i, dev_pad, dev_all_ids)\n",
    "    data_dict = { }\n",
    "    data_dict['id'] = id_\n",
    "    data_dict['Narrative'] = narrative_str\n",
    "    data_dict['Response'] = response_str\n",
    "    data_dict['GT_Response'] = gt_response_str\n",
    "    data_dict['y_true'] = y_true\n",
    "    for i in range(10):\n",
    "        data_dict[f'U%02d'%(i+1)] = utterance_str[i]\n",
    "    #new_row = pd.Series(data_dict)\n",
    "    #df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "    data_dict_all.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34a5e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict_all)\n",
    "df.to_csv(f'dev_output_{prefix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07d906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
