{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce678eb1",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18263a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda 11.1\n",
    "#!pip install -r requirements-torch-cu111.txt --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dc478",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ec5168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '1cycle'\n",
    "url = 'https://drive.google.com/uc?id=1j46elyFZtkmnmCehlntMi0eX0Tp5nnav'\n",
    "#prefix = 'helper'\n",
    "#url = 'https://drive.google.com/uc?id=1iSP_YKFs56d5cRRTEMzfedwRxrx-nXWO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061a428",
   "metadata": {},
   "source": [
    "## 스토리헬퍼 샘플 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ac09f2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1j46elyFZtkmnmCehlntMi0eX0Tp5nnav\n",
      "To: /home/kotech/workspace/ScriptWriter/data/scripts_1cycle.json\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12.6M/12.6M [00:03<00:00, 3.49MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/scripts_1cycle.json'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "scripts_file = f'data/scripts_{prefix}.json'\n",
    "gdown.download(url, scripts_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af38109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(scripts_file) as f:\n",
    "    data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c880d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'storyline': '우연히 옆집으로 이사 오게 된 차우와 수리첸',\n",
       " 'script': ['집을 구하는 차우.',\n",
       "  '그러나 수엔 부인은 방이 나갔다는 것을 알린다.',\n",
       "  '수엔 부인은 옆집에도 방이 있다며 한 번 볼 것을 권한다.',\n",
       "  '이사를 하는 차우와 수리첸.',\n",
       "  '그러나 동시에 이사를 해서 짐들이 계속 섞인다.',\n",
       "  '남편은 어디에 있냐고 묻는 수엔 부인.',\n",
       "  '수리첸은 남편이 출장 중이라고 말한다.',\n",
       "  '수리첸에게 책을 건네는 차우']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플 데이터 출력\n",
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618293c6",
   "metadata": {},
   "source": [
    "**후처리**\n",
    "1. `\\n`을 제거한다. \"부엌에서 일하게 된 마리오\\n인부들 사이에서 인기만점인 베아트리체\"  \n",
    "   ==> 필요없는 것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48b59ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378 7 부엌에서 일하게 된 마리오\n",
      "인부들 사이에서 인기만점인 베아트리체\n",
      "\"된 \"\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 비정상적 white character가 없는지 확인\n",
    "for idx, data in enumerate(data_dict):\n",
    "    #data['storyline'] = data['storyline'].replace('\\n', ' ')\n",
    "    for i, context in enumerate(data['script']):\n",
    "        #if '\\n' in context:\n",
    "        if '부엌에서 일하게' in context:\n",
    "            print(idx, i, context)\n",
    "            print('\"%s%s\"'%(context[9],context[10]))\n",
    "            print(context[10] == ' ')\n",
    "        #data['script'][i] = context.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58e2ce",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b08482",
   "metadata": {},
   "source": [
    "### kobigbird pretrained model을 이용한 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb68ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('monologg/kobigbird-bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f651a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n",
      "['<s>', '</s>', '[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "[5, 6, 1, 3, 0, 2, 4]\n",
      "sep_token_id = 3\n",
      "unk_token_id = 1\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 기본 설정 정보 출력\n",
    "print(tokenizer)\n",
    "print()\n",
    "# tokenizer special token정보 출력\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)\n",
    "print('sep_token_id =', tokenizer.sep_token_id)\n",
    "print('unk_token_id =', tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb68b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우연히 옆집으로 이사 오게 된 차우와 수리첸\n",
      "['우연히', '옆집', '##으로', '이사', '오', '##게', '된', '차우', '##와', '수리', '##첸']\n",
      "{'input_ids': [2, 12281, 23393, 9627, 7384, 3649, 4696, 2936, 27898, 4756, 9869, 5810, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "우연히 옆집으로 이사 오게 된 차우와 수리첸\n",
      "[CLS] 우연히 옆집으로 이사 오게 된 차우와 수리첸 [SEP]\n",
      "\n",
      "우연히 옆집으로 이사 오게 된 차우와 수리첸\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='monologg/kobigbird-bert-base', vocab_size=32500, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "['[SEP]']\n"
     ]
    }
   ],
   "source": [
    "s = data_dict[0]['storyline']\n",
    "print(s)\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(tokens)\n",
    "encoded = tokenizer(s)\n",
    "print(encoded)\n",
    "print()\n",
    "output = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(output)\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)\n",
    "print()\n",
    "\n",
    "# [CLS]와 [SEP]은 필요없으므로....\n",
    "encoded = tokenizer(s, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)\n",
    "print()\n",
    "\n",
    "# [SEP] token을 ScriptWriter모델의 EOS로 사용하자\n",
    "print(tokenizer)\n",
    "print(tokenizer.tokenize('[SEP]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016325f",
   "metadata": {},
   "source": [
    "# source code 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4b9a4",
   "metadata": {},
   "source": [
    "### 학습 및 검증, 테스트 데이터 셋 생성\n",
    "\n",
    "```py\n",
    "def generate_data_with_random_samples():\n",
    "    # generate negative samples randomly\n",
    "    # In training set, for each sample, we randomly sample a response as a negative candidate\n",
    "    # In development and test set, for each sample, we randomly sample 9 responses as negative candidates and we add a \"EOS\" response as a candidate to let model select when to stop\n",
    "```\n",
    "무작위로 negative sample을 생성한다.  \n",
    "학습set에서는 한개의 무작위 sample을 negative 후보로서 생성한다.\n",
    "```py\n",
    "        for context_id, narrative_id, _ in train:\n",
    "            num_context = len(context_id)\n",
    "            for i in range(1, num_context):\n",
    "                context = context_id[:i]\n",
    "                response = context_id[i]\n",
    "                train_all.append([context, response, narrative_id, 1])\n",
    "                flag = True\n",
    "                while flag:\n",
    "                    # 무작위로 스토리 중의 하나를 고르고, 그 중 무작위 script를 골라서 negative 샘플을 생성한다.\n",
    "                    random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                    random_context = positive_data[random_idx][0]\n",
    "                    random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                    random_response = random_context[random_idx_2]\n",
    "                    # 만일 정답 response와 추출한 response의 길이가 다르다면 negative 샘플로 추가한다.\n",
    "                    if len(response) != len(random_response):\n",
    "                        flag = False\n",
    "                        train_all.append([context, random_response, narrative_id, 0])\n",
    "                    # 길이가 같다면, 내용을 비교해서 다르다면 negative 샘플로 추가한다. \n",
    "                    else:\n",
    "                        for idx, wid in enumerate(response):\n",
    "                            if wid != random_response[idx]:\n",
    "                                flag = False\n",
    "                                train_all.append([context, random_response, narrative_id, 0])\n",
    "                                break\n",
    "```\n",
    "검증set에서는 9개의 negative sample을 추가한다.  \n",
    "response가 EOS인 경우(마지막인 경우), EOS를 True로 추가한다. (중복???)  \n",
    "그렇지 않을 경우, EOS response를 negative(False)로 추가한다.  \n",
    "테스트set도 동일하다.  \n",
    "```py\n",
    "        for context_id, narrative_id, _ in dev:\n",
    "            num_context = len(context_id)\n",
    "            for i in range(1, num_context):\n",
    "                context = context_id[:i]\n",
    "                response = context_id[i]\n",
    "                dev_all.append([context, response, narrative_id, 1])\n",
    "                count = 0\n",
    "                negative_samples = []\n",
    "                # total count = 1(positive) + 8(negative) + 1(EOS) 가 되어야함.\n",
    "                # 아래의 count는 count < 8로 수정 필요\n",
    "                while count < 9:\n",
    "                    random_idx = random.randint(0, len(positive_data) - 1)\n",
    "                    random_context = positive_data[random_idx][0]\n",
    "                    random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "                    random_response = random_context[random_idx_2]\n",
    "                    if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                        if len(response) != len(random_response):\n",
    "                            dev_all.append([context, random_response, narrative_id, 0])\n",
    "                            count += 1\n",
    "                            negative_samples.append(random_response)\n",
    "                        else:\n",
    "                            for idx, wid in enumerate(response):\n",
    "                                if wid != random_response[idx]:\n",
    "                                    dev_all.append([context, random_response, narrative_id, 0])\n",
    "                                    count += 1\n",
    "                                    negative_samples.append(random_response)\n",
    "                                    break\n",
    "                if response == [EOS_ID]:\n",
    "                    dev_all.append([context, [EOS_ID], narrative_id, 1])\n",
    "                else:\n",
    "                    dev_all.append([context, [EOS_ID], narrative_id, 0])\n",
    "```\n",
    "**To-Do**   \n",
    "Solr sample 추가가 필요한가?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0d332",
   "metadata": {},
   "source": [
    "### pickle data 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6669fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/train.gr.pkl', 'rb') as f:\n",
    "    train_gr = pickle.load(f)\n",
    "    \n",
    "with open('data/dev.gr.pkl', 'rb') as f:\n",
    "    dev_gr = pickle.load(f)\n",
    "    \n",
    "with open('data/test.gr.pkl', 'rb') as f:\n",
    "    test_gr = pickle.load(f)\n",
    "    \n",
    "with open('data/embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b765aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data tuple size = 5\n",
      "utterance shape = (136524, 11, 50)\n",
      "response shape = (136524, 50)\n",
      "narrative shape = (136524, 50)\n",
      "gt_response shape = (136524, 50)\n",
      "y_true shape = (136524,)\n"
     ]
    }
   ],
   "source": [
    "print('train data tuple size =', len(train_gr))\n",
    "print('utterance shape =', train_gr[0].shape)\n",
    "print('response shape =', train_gr[1].shape)\n",
    "print('narrative shape =', train_gr[2].shape)\n",
    "print('gt_response shape =', train_gr[3].shape)\n",
    "print('y_true shape =', train_gr[4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0dc8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number = len(train_gr[4])\n",
    "val_number = len(dev_gr[4])\n",
    "test_number = len(test_gr[4])\n",
    "#total_number_of_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560949af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136524, 37480, 38320)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original dataset numbers\n",
    "train_number, val_number, test_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8051cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance cnt = 1\n",
      "resonse = [   5  164   48    4 5334  123 2778    3    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "narrative = [10958 13504   137     8  2778     4     3    11    29   267     8     9\n",
      "    11 10915   867  3205     1    46   101   111  4745 23556   867     3\n",
      "   173    29    99  2373  2523    51     2    64     3     8    47    51\n",
      "    72  5339 16837     2     1   104  4531    27     2   363    15   117\n",
      "    71  5440]\n",
      "gt_response = [   5  164   48    4 5334  123 2778    3    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "y_true = 1\n"
     ]
    }
   ],
   "source": [
    "def get_data(data, idx):\n",
    "    return data[0][idx], data[1][idx], data[2][idx], data[3][idx], data[4][idx]\n",
    "\n",
    "def sample_data_print(utterance, response, narrative, gt_response, y_true):\n",
    "    #print('1st script =', utterance[0])\n",
    "    #print('2nd script =', utterance[1])\n",
    "    for i, utt in enumerate(utterance):\n",
    "        if (utt == [0]*50).all():\n",
    "            break\n",
    "    print('utterance cnt =', i)\n",
    "    print('resonse =', response)\n",
    "    print('narrative =', narrative)\n",
    "    print('gt_response =', gt_response)\n",
    "    print('y_true =', y_true)\n",
    "\n",
    "def browse_idx(data, idx):\n",
    "    utterance, response, narrative, gt_response, y_true = get_data(data, idx)\n",
    "    sample_data_print(utterance, response, narrative, gt_response, y_true)\n",
    "\n",
    "def get_utterance_len(data, idx):\n",
    "    utterance, response, narrative, gt_response, y_true = get_data(data, idx)\n",
    "    for i, utt in enumerate(utterance):\n",
    "        if (utt == [0]*50).all():\n",
    "            break\n",
    "    return i\n",
    "\n",
    "browse_idx(train_gr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd2e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance cnt = 1\n",
      "resonse = [   5   73   31   57  130    9 4446    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "narrative = [10958 13504   137     8  2778     4     3    11    29   267     8     9\n",
      "    11 10915   867  3205     1    46   101   111  4745 23556   867     3\n",
      "   173    29    99  2373  2523    51     2    64     3     8    47    51\n",
      "    72  5339 16837     2     1   104  4531    27     2   363    15   117\n",
      "    71  5440]\n",
      "gt_response = [   5  164   48    4 5334  123 2778    3    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "y_true = 0\n"
     ]
    }
   ],
   "source": [
    "# negative sample\n",
    "browse_idx(train_gr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c57cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance cnt = 2\n",
      "resonse = [267  77  10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "narrative = [10958 13504   137     8  2778     4     3    11    29   267     8     9\n",
      "    11 10915   867  3205     1    46   101   111  4745 23556   867     3\n",
      "   173    29    99  2373  2523    51     2    64     3     8    47    51\n",
      "    72  5339 16837     2     1   104  4531    27     2   363    15   117\n",
      "    71  5440]\n",
      "gt_response = [267  77  10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "y_true = 1\n"
     ]
    }
   ],
   "source": [
    "# 2nd sample with two utterance\n",
    "browse_idx(train_gr, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01ffeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance cnt = 8\n",
      "resonse = [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "narrative = [10958 13504   137     8  2778     4     3    11    29   267     8     9\n",
      "    11 10915   867  3205     1    46   101   111  4745 23556   867     3\n",
      "   173    29    99  2373  2523    51     2    64     3     8    47    51\n",
      "    72  5339 16837     2     1   104  4531    27     2   363    15   117\n",
      "    71  5440]\n",
      "gt_response = [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_true = 1\n"
     ]
    }
   ],
   "source": [
    "# last script\n",
    "browse_idx(train_gr, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987734e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136524"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gr[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79fa973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "2 2\n",
      "4 3\n",
      "6 4\n",
      "8 5\n",
      "10 6\n",
      "12 7\n",
      "14 8\n",
      "120 9\n",
      "314 10\n"
     ]
    }
   ],
   "source": [
    "# check utterance len\n",
    "max_utt_len = 0\n",
    "for i in range(train_gr[0].shape[0]):\n",
    "    utt_len = get_utterance_len(train_gr, i)\n",
    "    if utt_len > max_utt_len:\n",
    "        max_utt_len = utt_len\n",
    "        print(i, max_utt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beadbfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "10 2\n",
      "50 3\n",
      "60 4\n",
      "110 5\n",
      "470 6\n",
      "480 7\n",
      "760 8\n",
      "850 9\n",
      "1300 10\n"
     ]
    }
   ],
   "source": [
    "# check utterance len\n",
    "max_utt_len = 0\n",
    "for i in range(dev_gr[0].shape[0]):\n",
    "    utt_len = get_utterance_len(dev_gr, i)\n",
    "    if utt_len > max_utt_len:\n",
    "        max_utt_len = utt_len\n",
    "        print(i, max_utt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6771e8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  486,    32,    73,  9178,     5,     2, 11099,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  913, 15855,     2,   128,   105,   169,    37,  7022,     2,\n",
       "         4885,  9643,     2,  2914,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 10개의 utterance가 있다. 11번째는 항상 [0.]*50이고, [EOS_ID]는 없을 수도 있다.\n",
    "utterance, response, narrative, gt_response, y_true = get_data(dev_gr, 1300)\n",
    "utterance[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81633020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1758,  7785,   183,    15,    11,  1060,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [ 1856,     1,     6,    33,  2433, 11766,     2,  3945,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 10개의 utterance가 있다. 11번째는 항상 [0.]*50이고, [EOS_ID]는 없을 수도 있다.\n",
    "utterance, response, narrative, gt_response, y_true = get_data(train_gr, 314)\n",
    "utterance[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3524a7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1758, 7785,  183,   15,   11, 1060,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance, response, narrative, gt_response, y_true = get_data(train_gr, 312)\n",
    "utterance[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a8c1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance cnt = 2\n",
      "resonse = [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "narrative = [7234 1152   26  686   56    4    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "gt_response = [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_true = 1\n",
      "\n",
      "utterance cnt = 2\n",
      "resonse = [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "narrative = [7234 1152   26  686   56    4    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "gt_response = [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_true = 1\n"
     ]
    }
   ],
   "source": [
    "# 정답이 EOS인 경우 10번째 뒤에 중복적으로 데이터 들어 있음\n",
    "browse_idx(dev_gr, 10)\n",
    "print()\n",
    "browse_idx(dev_gr, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37dea0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#browse_idx(dev_pad, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3d2b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43514, 200)\n"
     ]
    }
   ],
   "source": [
    "# shape of embeddings (총단어수+1=43514, embedding_size=203)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "797f20b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings의 첫번째 data는 0.으로 채워져 있다. PADDING용 이다.\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab61f27",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e81a5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all suitable sessions:  11655\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EOS_ID = tokenizer.sep_token_id\n",
    "\n",
    "positive_sessions = []\n",
    "for unit_data in data_dict:\n",
    "    unit_contexts = [tokenizer.tokenize(text) for text in unit_data['script'] + ['[SEP]'] ]\n",
    "    unit_narrative = tokenizer.tokenize(unit_data['storyline'])\n",
    "    positive_sessions.append([unit_contexts, unit_narrative, 1])\n",
    "print(\"all suitable sessions: \", len(positive_sessions))\n",
    "\n",
    "# reproducibility를 위한 random seed 설정\n",
    "np.random.seed(42)\n",
    "# random shuffle data\n",
    "np.random.shuffle(positive_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc46677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train = 10489 , val = 582 , test = 584\n"
     ]
    }
   ],
   "source": [
    "train_num = int(len(positive_sessions) * 0.9)\n",
    "dev_test_num = int(len(positive_sessions) * 0.05)\n",
    "train_sessions, dev_sessions, test_sessions = positive_sessions[:train_num], positive_sessions[train_num: train_num + dev_test_num], positive_sessions[train_num + dev_test_num:]\n",
    "print('number of train =', len(train_sessions), ', val =', len(dev_sessions), ', test =', len(test_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "132fe371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word2vec training sentences = 144229\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "for train_session in train_sessions:\n",
    "    train_texts += train_session[0]\n",
    "    train_texts.append(train_session[1])\n",
    "print('number of word2vec training sentences =', len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c59077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['불안',\n",
       "   '##해',\n",
       "   '하',\n",
       "   '##는',\n",
       "   '미치',\n",
       "   '가족',\n",
       "   '##들',\n",
       "   ',',\n",
       "   '멜라',\n",
       "   '##니',\n",
       "   '##가',\n",
       "   '앨리스',\n",
       "   '##를',\n",
       "   '다독',\n",
       "   '##인다',\n",
       "   '.'],\n",
       "  ['창문',\n",
       "   '##을',\n",
       "   '확인',\n",
       "   '##하',\n",
       "   '##는',\n",
       "   '미치',\n",
       "   '멜라',\n",
       "   '##니',\n",
       "   '##가',\n",
       "   '사',\n",
       "   '##온',\n",
       "   '잉',\n",
       "   '##꼬',\n",
       "   '##를',\n",
       "   '보고',\n",
       "   '착',\n",
       "   '##잡',\n",
       "   '##해',\n",
       "   '##한다',\n",
       "   '.'],\n",
       "  ['불안', '##해', '하', '##는', '미치', '가족', '##과', '멜라', '##니', '.'],\n",
       "  ['드디어', '새', '##들', '##의', '공격', '##이', '시작', '##된', '##다', '.'],\n",
       "  ['팔',\n",
       "   '##의',\n",
       "   '상처',\n",
       "   '##를',\n",
       "   '입',\n",
       "   '##은',\n",
       "   '미치',\n",
       "   '##가',\n",
       "   '새',\n",
       "   '##들',\n",
       "   '##의',\n",
       "   '공격',\n",
       "   '##을',\n",
       "   '맨',\n",
       "   '##손',\n",
       "   '##으로',\n",
       "   '막아내',\n",
       "   '##지만',\n",
       "   '역부족',\n",
       "   '##이다',\n",
       "   '.'],\n",
       "  ['새',\n",
       "   '##들이',\n",
       "   '떠나',\n",
       "   '##고',\n",
       "   '조용',\n",
       "   '##해',\n",
       "   '##지',\n",
       "   '##자',\n",
       "   '한',\n",
       "   '숨',\n",
       "   '돌리',\n",
       "   '##는',\n",
       "   '사람',\n",
       "   '##들',\n",
       "   '.'],\n",
       "  ['[SEP]']],\n",
       " ['불안', '##해하', '##는', '멜라', '##니', '##와', '미치', '가족', '.'],\n",
       " 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41b03ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['새',\n",
       "  '##들이',\n",
       "  '떠나',\n",
       "  '##고',\n",
       "  '조용',\n",
       "  '##해',\n",
       "  '##지',\n",
       "  '##자',\n",
       "  '한',\n",
       "  '숨',\n",
       "  '돌리',\n",
       "  '##는',\n",
       "  '사람',\n",
       "  '##들',\n",
       "  '.'],\n",
       " ['[SEP]'],\n",
       " ['불안', '##해하', '##는', '멜라', '##니', '##와', '미치', '가족', '.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[5:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7540667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# word2vec 학습\n",
    "model = Word2Vec(sentences = train_texts, vector_size = 200, window = 7, min_count = 5, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ecb91",
   "metadata": {},
   "source": [
    "**참조 링크**  \n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/  \n",
    "https://wikidocs.net/50739  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52e2e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of words = 11882\n",
      "first word = \".\"\n",
      "last word = \"디지털\"\n"
     ]
    }
   ],
   "source": [
    "print('total num of words =', len(model.wv.key_to_index))\n",
    "print('first word = \"%s\"'%model.wv.index_to_key[0])\n",
    "print('last word = \"%s\"'%model.wv.index_to_key[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4707535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친척', 0.7296834588050842), ('친구', 0.7136985063552856), ('동생', 0.6769063472747803), ('딸', 0.6370465755462646), ('팀원', 0.6250258088111877), ('아들', 0.6217833161354065), ('식구', 0.6177724599838257), ('동료', 0.6113835573196411), ('동물', 0.6099558472633362), ('어른', 0.6072829365730286)]\n"
     ]
    }
   ],
   "source": [
    "# word2vec이 잘 학습되었는지 여러가지 테스트를 수행하자.\n",
    "print(model.wv.most_similar(\"가족\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d73cfc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "[-1.0027357   0.48399073  0.8626121   1.2091957   0.7915889 ]\n",
      "753\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "key = '가족'\n",
    "print(model.wv.key_to_index[key])\n",
    "print(model.wv.get_vector(key)[:5])\n",
    "key = '[UNK]'\n",
    "print(model.wv.key_to_index[key])\n",
    "print(model.wv.key_to_index.get('최민수', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "076381d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17658584 -0.305841  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.17658584, -0.305841  ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.wv.get_vector(0)[:2])\n",
    "model.wv.get_vector('.')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ad5d7",
   "metadata": {},
   "source": [
    "## 데이터 저장\n",
    "\n",
    "`embeddings.pkl`과 `vocab.txt`를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8453ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/vocab.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for i, key in enumerate(model.wv.index_to_key):\n",
    "        file.write('%s\\t%i\\n'%(key, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67a3bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "new_embeddings = np.array([[0.]*200],dtype='float32') \n",
    "for i in range(len(model.wv.index_to_key)):\n",
    "    new_embeddings = np.append(new_embeddings, [model.wv.get_vector(i)], axis=0)\n",
    "\n",
    "with open('data/embeddings_ko.pkl', 'wb') as f:\n",
    "    pickle.dump(new_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4ec99",
   "metadata": {},
   "source": [
    "# 학습 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "066fc318",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = model.wv.key_to_index['[SEP]']+1\n",
    "UNK_ID = model.wv.key_to_index['[UNK]']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a86226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vocab = {}\n",
    "positive_data = []\n",
    "\n",
    "with open(\"./data/vocab.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "    for idx, line in enumerate(fr):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        vocab[line[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0501f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample id 출력 확인\n",
    "vocab['가족']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611b33b",
   "metadata": {},
   "source": [
    "**positive data 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f165912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = []\n",
    "\n",
    "for unit in positive_sessions:\n",
    "    narrative = unit[1]\n",
    "    #print(narrative)\n",
    "    context = unit[0]\n",
    "    narrative_id = [vocab.get(word, UNK_ID) for word in narrative]\n",
    "    context_id = [[vocab.get(word, UNK_ID) for word in sent] for sent in context]\n",
    "    if len(narrative_id) == 0 or len(context_id) == 0:\n",
    "        continue\n",
    "    data = [context_id, narrative_id, 1]\n",
    "    positive_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9944ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_num = int(len(positive_data) * 0.05)\n",
    "train, dev, test = positive_data[:train_num], positive_data[train_num: train_num + dev_test_num], positive_data[train_num + dev_test_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22242ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1154, 36, 26, 2, 1227, 134, 15, 24, 2119, 60, 10, 462, 4, 5485, 831, 1]], [945, 3, 342, 12, 2, 1227, 2119, 60, 10, 230, 294, 1699, 5439, 4, 77, 3416, 2321, 36, 14, 1], [1154, 979, 2, 2119, 60, 17, 1227, 134, 1], [945, 3, 342, 12, 2, 1227, 2119, 60, 10, 230, 294, 1699, 5439, 4, 77, 3416, 2321, 36, 14, 1], 1]\n",
      "[[[1154, 36, 26, 2, 1227, 134, 15, 24, 2119, 60, 10, 462, 4, 5485, 831, 1]], [25], [1154, 979, 2, 2119, 60, 17, 1227, 134, 1], [945, 3, 342, 12, 2, 1227, 2119, 60, 10, 230, 294, 1699, 5439, 4, 77, 3416, 2321, 36, 14, 1], 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_all, dev_all, test_all = [], [], []\n",
    "for context_id, narrative_id, _ in train:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        train_all.append([context, response, narrative_id, response, 1])\n",
    "        flag = True\n",
    "        while flag:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if len(response) != len(random_response):\n",
    "                flag = False\n",
    "                train_all.append([context, random_response, narrative_id, response, 0])\n",
    "            else:\n",
    "                for idx, wid in enumerate(response):\n",
    "                    if wid != random_response[idx]:\n",
    "                        flag = False\n",
    "                        train_all.append([context, random_response, narrative_id, response, 0])\n",
    "                        break\n",
    "print(train_all[0]) \n",
    "print(train_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "287d8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[742, 6671, 29, 591, 274, 212, 6, 6655, 8, 147, 1]], [28, 2, 942, 368, 3, 217, 4, 35, 1], [126, 13, 212, 19, 536, 3, 754, 123, 1073, 893, 3, 364, 9, 20, 11, 1, 7838, 599, 676, 38, 28, 2, 1476, 521, 3, 3133, 47, 20, 11, 1, 526, 2, 210, 126, 6, 1162, 23, 72, 89, 103, 11, 1], [28, 2, 942, 368, 3, 217, 4, 35, 1], 1] [[[742, 6671, 29, 591, 274, 212, 6, 6655, 8, 147, 1]], [1792, 4, 848, 54, 639, 10, 268, 4, 3640, 47, 343, 1003, 13, 2156, 5866, 4, 2075, 1], [126, 13, 212, 19, 536, 3, 754, 123, 1073, 893, 3, 364, 9, 20, 11, 1, 7838, 599, 676, 38, 28, 2, 1476, 521, 3, 3133, 47, 20, 11, 1, 526, 2, 210, 126, 6, 1162, 23, 72, 89, 103, 11, 1], [28, 2, 942, 368, 3, 217, 4, 35, 1], 0] [[[742, 6671, 29, 591, 274, 212, 6, 6655, 8, 147, 1]], [11600, 423, 5, 239, 734, 15, 3, 230, 2, 363, 688, 23, 230, 90, 11, 1], [126, 13, 212, 19, 536, 3, 754, 123, 1073, 893, 3, 364, 9, 20, 11, 1, 7838, 599, 676, 38, 28, 2, 1476, 521, 3, 3133, 47, 20, 11, 1, 526, 2, 210, 126, 6, 1162, 23, 72, 89, 103, 11, 1], [28, 2, 942, 368, 3, 217, 4, 35, 1], 0]\n"
     ]
    }
   ],
   "source": [
    "for context_id, narrative_id, _ in dev:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        dev_all.append([context, response, narrative_id, response, 1])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        # fix count 버그\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    dev_all.append([context, random_response, narrative_id, response, 0])\n",
    "                    count += 1\n",
    "                    negative_samples.append(random_response)\n",
    "                else:\n",
    "                    for idx, wid in enumerate(response):\n",
    "                        if wid != random_response[idx]:\n",
    "                            dev_all.append([context, random_response, narrative_id, response, 0])\n",
    "                            count += 1\n",
    "                            negative_samples.append(random_response)\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 1])\n",
    "        else:\n",
    "            dev_all.append([context, [EOS_ID], narrative_id, response, 0])\n",
    "print(dev_all[0], dev_all[1], dev_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d77f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[261, 2, 1410, 5, 7, 32, 6, 286, 3, 727, 14, 1]], [28, 2, 1410, 4, 373, 9, 20, 11, 1], [261, 2, 709, 2266, 23, 961, 8, 1360, 1, 96, 509, 95, 7, 191, 416, 404, 1042, 2, 261, 4, 1528, 36, 250, 1], [28, 2, 1410, 4, 373, 9, 20, 11, 1], 1] [[[261, 2, 1410, 5, 7, 32, 6, 286, 3, 727, 14, 1]], [79, 41, 57, 192, 19, 6449, 101, 2, 64, 1], [261, 2, 709, 2266, 23, 961, 8, 1360, 1, 96, 509, 95, 7, 191, 416, 404, 1042, 2, 261, 4, 1528, 36, 250, 1], [28, 2, 1410, 4, 373, 9, 20, 11, 1], 0] [[[261, 2, 1410, 5, 7, 32, 6, 286, 3, 727, 14, 1]], [408, 2, 180, 5, 1258, 564, 418, 16, 5556, 7, 2736, 389, 11, 1], [261, 2, 709, 2266, 23, 961, 8, 1360, 1, 96, 509, 95, 7, 191, 416, 404, 1042, 2, 261, 4, 1528, 36, 250, 1], [28, 2, 1410, 4, 373, 9, 20, 11, 1], 0]\n"
     ]
    }
   ],
   "source": [
    "for context_id, narrative_id, _ in test:\n",
    "    num_context = len(context_id)\n",
    "    for i in range(1, num_context):\n",
    "        context = context_id[:i]\n",
    "        response = context_id[i]\n",
    "        test_all.append([context, response, narrative_id, response, 1])\n",
    "        count = 0\n",
    "        negative_samples = []\n",
    "        # fix count 버그\n",
    "        while count < 8:\n",
    "            random_idx = random.randint(0, len(positive_data) - 1)\n",
    "            random_context = positive_data[random_idx][0]\n",
    "            random_idx_2 = random.randint(0, len(random_context) - 1)\n",
    "            random_response = random_context[random_idx_2]\n",
    "            if random_response not in negative_samples and random_response != [EOS_ID]:\n",
    "                if len(response) != len(random_response):\n",
    "                    test_all.append([context, random_response, narrative_id, response, 0])\n",
    "                    negative_samples.append(random_response)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for idx, id in enumerate(response):\n",
    "                        if id != random_response[idx]:\n",
    "                            test_all.append([context, random_response, narrative_id, response, 0])\n",
    "                            negative_samples.append(random_response)\n",
    "                            count += 1\n",
    "                            break\n",
    "        if response == [EOS_ID]:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 1])\n",
    "        else:\n",
    "            test_all.append([context, [EOS_ID], narrative_id, response, 0])\n",
    "print(test_all[0], test_all[1], test_all[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5c5b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train count = 246578\n",
      "total val count = 66430\n",
      "total test count = 66830\n"
     ]
    }
   ],
   "source": [
    "print('total train count =', len(train_all))\n",
    "print('total val count =', len(dev_all))\n",
    "print('total test count =', len(test_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47cd19f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, 0, 0],\n",
       "       [4, 5, 6, 0, 0],\n",
       "       [7, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numpy_from_nonfixed_2d_array(aa, max_sentence_len=50, max_num_utterance=10, padding_value=0):\n",
    "    PAD_SEQUENCE = np.array([0] * max_sentence_len)\n",
    "    rows = np.empty([0, max_sentence_len], dtype='int')\n",
    "    aa = aa[:max_num_utterance]\n",
    "    for a in aa:\n",
    "        sentence_len = len(a)\n",
    "        if sentence_len < max_sentence_len:\n",
    "            rows  = np.append(rows, [np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)[:max_sentence_len]], axis=0)\n",
    "        else:\n",
    "            rows = np.append(rows, [a[:max_sentence_len]], axis=0)\n",
    "    num_utterance = len(aa)\n",
    "    if num_utterance < max_num_utterance:\n",
    "        rows = np.append(rows, [PAD_SEQUENCE]*(max_num_utterance-num_utterance), axis=0)\n",
    "    # add empty +1 sentence\n",
    "    rows = np.append(rows, [PAD_SEQUENCE], axis=0)\n",
    "    #return np.concatenate(rows, axis=0).reshape(-1, max_sentence_len)\n",
    "    return rows\n",
    "\n",
    "def get_numpy_from_nonfixed_1d_array(a, max_sentence_len=50, padding_value=0):\n",
    "    sentence_len = len(a)\n",
    "    if sentence_len < max_sentence_len:\n",
    "        return np.pad(a, (0, max_sentence_len-sentence_len), 'constant', constant_values=padding_value)\n",
    "    else:\n",
    "        return np.array(a[:max_sentence_len])\n",
    "\n",
    "cc_test_data = [\n",
    "        [1,2],\n",
    "        [4,5,6],\n",
    "        [7]\n",
    "     ]\n",
    "get_numpy_from_nonfixed_2d_array(cc_test_data, max_sentence_len=5, max_num_utterance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51302d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_numpy_from_nonfixed_1d_array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "475ebe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 246578/246578 [00:48<00:00, 5061.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66430/66430 [00:12<00:00, 5112.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66830/66830 [00:12<00:00, 5213.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def pad_process(data, max_sentence_len=50, max_num_utterance=10):\n",
    "    utterance = []\n",
    "    response = []\n",
    "    narrative = []\n",
    "    gt_response = []\n",
    "    y_true = []\n",
    "    for unit in tqdm(data):\n",
    "        utterance.append(get_numpy_from_nonfixed_2d_array(unit[0]))\n",
    "        response.append(get_numpy_from_nonfixed_1d_array(unit[1]))\n",
    "        narrative.append(get_numpy_from_nonfixed_1d_array(unit[2]))\n",
    "        gt_response.append(get_numpy_from_nonfixed_1d_array(unit[3]))\n",
    "        y_true.append(unit[4])\n",
    "        \n",
    "    utterance = np.stack(utterance)\n",
    "    response = np.stack(response)\n",
    "    narrative = np.stack(narrative)\n",
    "    gt_response = np.stack(gt_response)\n",
    "    y_true = np.stack(y_true)\n",
    "    return (utterance, response, narrative, gt_response, y_true)\n",
    "\n",
    "train_pad = pad_process(train_all)\n",
    "dev_pad = pad_process(dev_all)\n",
    "test_pad = pad_process(test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003946db",
   "metadata": {},
   "source": [
    "**학습데이터셋 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4d8dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_ko.pkl', 'wb') as f:\n",
    "    pickle.dump(train_pad, f)\n",
    "with open('data/dev_ko.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_pad, f)\n",
    "with open('data/test_ko.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214841c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3ac8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/positive_ko.pkl', \"wb\") as f:\n",
    "    pickle.dump(positive_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f435a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2067c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3052a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec.load(\"data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79ad76ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음으로 베르메르의 집에 발을 들여놓는다.\n"
     ]
    }
   ],
   "source": [
    "#print(model.wv.index_to_key[2])\n",
    "for data in train[1:2]:\n",
    "    utterances = data[0]\n",
    "    narrative = data[1]\n",
    "    tokens = [model2.wv.index_to_key[i-1] for i in narrative]\n",
    "    output = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f0663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
